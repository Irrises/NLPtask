{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8203e5d-d561-4e82-8872-f1982f3085ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前使用的设备: cuda\n",
      "GPU名称: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import difflib\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM, # 注意：使用 AutoModelForCausalLM\n",
    "    TrainingArguments,    # 注意：使用 TrainingArguments\n",
    "    Trainer,              # 注意：使用 Trainer\n",
    "    DataCollatorForSeq2Seq, \n",
    "    EarlyStoppingCallback,\n",
    "    BitsAndBytesConfig    # 用于量化加载\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "# import evaluate # Hugging Face evaluate 库，如果需要标准指标如BLEU/ROUGE\n",
    "\n",
    "# 检查是否有可用的GPU\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"当前使用的设备: {DEVICE}\")\n",
    "if DEVICE.type == 'cuda':\n",
    "    print(f\"GPU名称: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab5f276a-d2b1-41a0-a09f-ee56c6381ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 配置参数 ---\n",
    "MODEL_NAME = \"/root/autodl-tmp/models/Qwen3-8B\" \n",
    "\n",
    "\n",
    "# LoRA 配置 (如果启用)\n",
    "USE_LORA = True # 是否启用LoRA进行参数高效微调\n",
    "LORA_R = 16 # LoRA的秩 (rank)\n",
    "LORA_ALPHA = 32 # LoRA的alpha参数 (缩放因子)\n",
    "LORA_DROPOUT = 0.05 # LoRA层的dropout率\n",
    "# LoRA作用的目标模块，对于Qwen1.5/Qwen2模型，常见的模块包括 'q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'\n",
    "# 您可以通过 print(model) 查看模型结构来确定正确的模块名称。\n",
    "LORA_TARGET_MODULES = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "\n",
    "\n",
    "# 量化配置 (可选, 如果显存不足)\n",
    "USE_QUANTIZATION = True # 是否使用4-bit/8-bit量化加载模型以节省显存\n",
    "QUANTIZATION_TYPE = \"nf4\" # \"nf4\" (4-bit NormalFloat), \"fp4\" 或 \"int8\"\n",
    "\n",
    "# 训练相关参数\n",
    "OUTPUT_DIR = \"/root/autodl-tmp/qwen_hate_speech_finetuned-1.7B\" # 微调后模型的输出和保存目录\n",
    "# 定义训练文件路径，请确保该文件存在于您的目录中\n",
    "TRAIN_FILE_PATH = \"./train_formatted_for_llm.jsonl\" \n",
    "\n",
    "TRAIN_BATCH_SIZE = 5 # Causal LM通常需要更小的批次大小，根据您的GPU显存进行调整\n",
    "EVAL_BATCH_SIZE = 2  # 评估时的批次大小\n",
    "NUM_TRAIN_EPOCHS = 1 # 训练的总轮数，可根据收敛情况调整 (对于大模型和LoRA，较少轮数可能就够了)\n",
    "LEARNING_RATE = 2e-4 # LoRA微调时学习率通常可以稍大一些\n",
    "WEIGHT_DECAY = 0.01  # 权重衰减参数\n",
    "MAX_INPUT_LENGTH = 1024 # 输入序列（包括提示和输出）的最大token长度\n",
    "MAX_TARGET_LENGTH = 256 # 目标输出（四元组字符串）的最大token长度  \n",
    "# MAX_TARGET_LENGTH 在Causal LM中不太直接使用，因为输入和输出合并了\n",
    "GRADIENT_ACCUMULATION_STEPS = 4 # 梯度累积步骤，有效扩大批次大小\n",
    "WARMUP_RATIO = 0.03 # 学习率预热比例\n",
    "LR_SCHEDULER_TYPE = \"cosine\" # 学习率调度器类型\n",
    "\n",
    "SEED = 42 # 随机种子，用于保证实验的可复现性\n",
    "\n",
    "# 特殊标记定义\n",
    "END_TOKEN = \"[END]\" # 单个四元组的结束标记\n",
    "SEP_TOKEN = \"[SEP]\" # 多个四元_组之间的分隔标记\n",
    "# 目标群体的所有类别，确保与您的任务描述和数据标注一致\n",
    "TARGET_GROUPS = [\"Region\", \"Racism\", \"Sexism\", \"LGBTQ\", \"others\", \"non-hate\"] \n",
    "# 是否构成仇恨言论的两种状态\n",
    "HATEFUL_STATUS = [\"hate\", \"non-hate\"]\n",
    "\n",
    "# 定义用于Causal LM的提示模板结构\n",
    "PROMPT_TEMPLATE = \"\"\"<s>[INST] <<SYS>>\n",
    "你是一个专业的中文社交媒体内容分析助手，专门用于细粒度片段级仇恨言论识别。请根据用户提供的文本，识别其中存在的仇恨言论或非仇恨的评论性言论，并按照以下格式输出一个或多个四元组：\n",
    "评论对象 (Target) | 论点 (Argument) | 目标群体 (Targeted Group) | 是否仇恨 (Hateful) [END]\n",
    "详细说明：\n",
    "1.  **评论对象 (Target)：** 帖子中被评论或提及的具体人物、群体、事物或概念。如果是针对文本中隐含的、没有明确指出的对象，或者评论是泛指，则设为 \"NULL\"。\n",
    "2.  **论点 (Argument)：** 针对“评论对象”所发表的核心观点、描述或行为，应为文本中的一个关键信息片段。\n",
    "3.  **目标群体 (Targeted Group)：** 指该“评论对象-论点”所涉及或指向的社会群体。重点注意：目标仇恨群体有可能不止一种，可能同时存在多种仇恨\n",
    "    如：她们 | 支持她们找黑哥哥 | Sexism, Racism | hate [END]\n",
    "    但总的群体必须从以下预设类别中选择：\n",
    "    * `Region`：针对特定地域（国家、省份、城市等）人群的评论。\n",
    "    * `Racism`：针对特定种族或民族人群的评论。\n",
    "    * `Sexism`：针对特定性别人群（男性、女性）的评论，或性别歧视、刻板印象。\n",
    "    * `LGBTQ`：针对性少数群体的评论（如同性恋、跨性别等）。\n",
    "    * `others`：针对上述四类之外的特定群体（如特定职业、疾病群体、政治立场群体等）或不构成对特定社会群体的攻击，而是个人攻击、观点评论等。\n",
    "    * `non-hate`：不存在攻击群体。\n",
    "4.  **是否仇恨 (Hateful)：** 判断该“评论对象-论点”是否构成了对“目标群体”的仇恨言论。\n",
    "    * `hate`：构成仇恨。\n",
    "    * `non-hate`：不构成仇恨（包括中性、积极、或一般性负面评论但未达到仇恨程度）。\n",
    "格式要求：\n",
    "* 四元组内各元素之间用 \" | \"（空格竖杠空格）分隔。\n",
    "* 每个四元组必须以 \" [END]\"（空格[END]）结尾。\n",
    "* 如果一条评论中识别出多个独立的评论对象和论点，应输出多个四元组，不同四元组之间用 \" [SEP] \"（空格[SEP]空格）分隔。\n",
    "\n",
    "现在，请处理以下新的输入内容：\n",
    "<</SYS>>\n",
    "\n",
    "用户提供的文本如下：\n",
    "{input_text} [/INST]\n",
    "模型输出：\n",
    "\"\"\"\n",
    "# 模型应该在此之后接上四元组字符串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff479fe1-dd92-4a33-ba4c-ccc7fc72fe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data(file_path, test_size=0.01, random_state=SEED):\n",
    "    \"\"\"\n",
    "    加载 .jsonl 格式的数据文件，并将其划分为训练集和验证集。\n",
    "    此函数专门适配包含 \"messages\" 列表的数据格式，从中提取 \"user\" 和 \"assistant\" 的内容。\n",
    "    \n",
    "    参数:\n",
    "    - file_path (str): .jsonl 数据文件的路径。\n",
    "    - test_size (float): 分配给验证集的比例。\n",
    "    - random_state (int): 随机种子，用于可复现的划分。\n",
    "    \"\"\"\n",
    "    input_texts_from_user = []      # 用于存储从 \"user\" 角色提取的输入文本\n",
    "    target_quadruples_from_assistant = [] # 用于存储从 \"assistant\" 角色提取的目标四元组字符串\n",
    "    system_prompts_from_data = []   # 可选：存储数据中提供的系统提示，供后续分析或使用\n",
    "    \n",
    "    # 检查文件是否存在，以避免后续错误\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"错误: 训练文件 '{file_path}' 未找到。请检查路径是否正确。\")\n",
    "        \n",
    "    print(f\"开始从 '{file_path}' 加载数据 (适配 'messages' 格式)...\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f, 1): # 从1开始计数行号，便于调试\n",
    "            try:\n",
    "                data_item = json.loads(line) # 解析当前行JSON数据\n",
    "                \n",
    "                if \"messages\" not in data_item or not isinstance(data_item[\"messages\"], list):\n",
    "                    print(f\"警告: 跳过行 (行号 {line_num})，因为缺少 'messages' 键或其值不是列表: {line.strip()}\")\n",
    "                    continue\n",
    "\n",
    "                messages_list = data_item[\"messages\"]\n",
    "                user_content = None\n",
    "                assistant_content = None\n",
    "                system_content_in_item = None # 当前条目中的系统提示\n",
    "\n",
    "                for message_dict in messages_list:\n",
    "                    if \"role\" in message_dict and \"content\" in message_dict:\n",
    "                        if message_dict[\"role\"] == \"user\":\n",
    "                            user_content = message_dict[\"content\"]\n",
    "                        elif message_dict[\"role\"] == \"assistant\":\n",
    "                            assistant_content = message_dict[\"content\"]\n",
    "                        elif message_dict[\"role\"] == \"system\":\n",
    "                            system_content_in_item = message_dict[\"content\"]\n",
    "                    else:\n",
    "                        print(f\"警告: 跳过 'messages' 列表中的无效条目 (行号 {line_num})，缺少 'role' 或 'content': {message_dict}\")\n",
    "                \n",
    "                # 确保成功提取了user和assistant的内容\n",
    "                if user_content is not None and assistant_content is not None:\n",
    "                    input_texts_from_user.append(user_content)\n",
    "                    target_quadruples_from_assistant.append(assistant_content)\n",
    "                    if system_content_in_item: # 如果当前数据条目中找到了system prompt\n",
    "                        system_prompts_from_data.append(system_content_in_item)\n",
    "                else:\n",
    "                    print(f\"警告: 跳过行 (行号 {line_num})，未能从 'messages' 中同时找到 'user' 和 'assistant' 的有效内容。\")\n",
    "                    if user_content is None:\n",
    "                        print(f\"  - 缺失 'user' 内容。\")\n",
    "                    if assistant_content is None:\n",
    "                        print(f\"  - 缺失 'assistant' 内容。\")\n",
    "\n",
    "            except json.JSONDecodeError:\n",
    "                # 如果某行JSON格式错误，打印警告并跳过该行\n",
    "                print(f\"警告: 跳过无效的JSON行 (行号 {line_num}): {line.strip()}\")\n",
    "            except Exception as e: # 捕获其他潜在错误\n",
    "                print(f\"警告: 处理行 (行号 {line_num}) 时发生未知错误 '{e}': {line.strip()}\")\n",
    "                continue \n",
    "    \n",
    "    # 检查是否成功加载了数据\n",
    "    if not input_texts_from_user or not target_quadruples_from_assistant:\n",
    "        raise ValueError(f\"错误: 未能从 '{file_path}' 加载任何有效的 'user'/'assistant' 对话数据。请检查文件格式、内容和角色标签是否正确。\")\n",
    "    \n",
    "    print(f\"成功从 '{file_path}' 加载了 {len(input_texts_from_user)} 条有效的对话记录。\")\n",
    "    if system_prompts_from_data:\n",
    "        print(f\"（其中 {len(system_prompts_from_data)} 条记录包含系统提示）\")\n",
    "        # print(f\"  数据中发现的第一个系统提示示例: '{system_prompts_from_data[0]}'\") # 可选打印\n",
    "\n",
    "    # 使用 sklearn 的 train_test_split 函数划分训练集和验证集\n",
    "    print(f\"正在将数据划分为训练集和验证集 (验证集比例: {test_size})...\")\n",
    "    # 注意：这里传递给Dataset的键名仍然是 \"text\" 和 \"quadruples_str\" 以便后续单元格代码兼容\n",
    "    # input_texts_from_user 对应 \"text\"\n",
    "    # target_quadruples_from_assistant 对应 \"quadruples_str\"\n",
    "    train_texts, val_texts, train_quads, val_quads = train_test_split(\n",
    "        input_texts_from_user, target_quadruples_from_assistant, \n",
    "        test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    print(f\"划分完成: 训练集 {len(train_texts)} 条, 验证集 {len(val_texts)} 条。\")\n",
    "\n",
    "    # 将划分后的数据转换为 Hugging Face Dataset 对象\n",
    "    # 使用与之前代码兼容的键名 \"text\" 和 \"quadruples_str\"\n",
    "    train_dataset = Dataset.from_dict({\"text\": train_texts, \"quadruples_str\": train_quads})\n",
    "    val_dataset = Dataset.from_dict({\"text\": val_texts, \"quadruples_str\": val_quads})\n",
    "    \n",
    "    # 将训练集和验证集包装在 DatasetDict 中返回\n",
    "    return DatasetDict({\"train\": train_dataset, \"validation\": val_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df902d2c-8fca-4d53-98e3-8d9bb305dce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准备从文件 './train_formatted_for_llm.jsonl' 加载数据...\n",
      "开始从 './train_formatted_for_llm.jsonl' 加载数据 (适配 'messages' 格式)...\n",
      "成功从 './train_formatted_for_llm.jsonl' 加载了 4000 条有效的对话记录。\n",
      "（其中 4000 条记录包含系统提示）\n",
      "正在将数据划分为训练集和验证集 (验证集比例: 0.01)...\n",
      "划分完成: 训练集 3960 条, 验证集 40 条。\n",
      "\n",
      "数据加载和初步划分成功:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'quadruples_str'],\n",
      "        num_rows: 3960\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'quadruples_str'],\n",
      "        num_rows: 40\n",
      "    })\n",
      "})\n",
      "\n",
      "训练集中的第一个样本示例:\n",
      "  输入文本 (text): 是的，反黑不要一上来就骂，会引起反感的\n",
      "  目标标签 (quadruples_str): 反黑 | 不要一上来就骂 | Racism | hate [END]\n"
     ]
    }
   ],
   "source": [
    "print(f\"准备从文件 '{TRAIN_FILE_PATH}' 加载数据...\")\n",
    "try:\n",
    "    raw_datasets = load_and_prepare_data(TRAIN_FILE_PATH)\n",
    "    print(\"\\n数据加载和初步划分成功:\")\n",
    "    print(raw_datasets) \n",
    "    \n",
    "    if raw_datasets and 'train' in raw_datasets and len(raw_datasets['train']) > 0:\n",
    "        print(f\"\\n训练集中的第一个样本示例:\")\n",
    "        print(f\"  输入文本 (text): {raw_datasets['train'][0]['text']}\")\n",
    "        print(f\"  目标标签 (quadruples_str): {raw_datasets['train'][0]['quadruples_str']}\")\n",
    "    else:\n",
    "        print(\"\\n警告: 加载后的 'raw_datasets' 为空或 'train' 部分不完整。请检查数据加载过程。\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n数据加载或准备过程中发生严重错误: {e}\")\n",
    "    # raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b95d771-8103-4648-ac5f-82aad18ead5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在从 '/root/autodl-tmp/models/Qwen3-8B' 加载Tokenizer...\n",
      "Tokenizer 加载完成。\n"
     ]
    }
   ],
   "source": [
    "print(f\"正在从 '{MODEL_NAME}' 加载Tokenizer...\")\n",
    "# trust_remote_code=True 对于某些模型（如Qwen）是必要的\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "print(\"Tokenizer 加载完成。\")\n",
    "\n",
    "# Qwen tokenizer 可能没有默认的 pad_token。如果需要填充，通常将其设置为 eos_token。\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token \n",
    "    print(f\"Tokenizer的pad_token未设置，已将其设置为eos_token: '{tokenizer.eos_token}'\")\n",
    "\n",
    "# 对于Causal LM, 我们需要将输入和目标合并，并正确处理标签以仅计算目标部分的损失\n",
    "def preprocess_function_causal(examples):\n",
    "    \"\"\"\n",
    "    对批量数据进行tokenize和预处理，适配Causal LM。\n",
    "    输入和输出将被合并，并创建标签以仅对输出部分计算损失。\n",
    "    \"\"\"\n",
    "    full_prompts = []\n",
    "    input_texts_for_prompt = examples[\"text\"]\n",
    "    target_outputs = examples[\"quadruples_str\"]\n",
    "\n",
    "    for input_text, target_output in zip(input_texts_for_prompt, target_outputs):\n",
    "        # 构建包含指令、输入和预期输出的完整文本\n",
    "        # 模型在推理时只会看到 PROMPT_TEMPLATE.format(input_text=input_text) 这部分\n",
    "        # 训练时，我们将完整输出也加进去，并添加eos_token表示序列结束\n",
    "        full_text = PROMPT_TEMPLATE.format(input_text=input_text) + target_output + tokenizer.eos_token\n",
    "        full_prompts.append(full_text)\n",
    "\n",
    "    # Tokenize 完整文本\n",
    "    model_inputs = tokenizer(\n",
    "        full_prompts,\n",
    "        max_length=MAX_INPUT_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=False, # 先不填充，DataCollator会处理，或者可以设为 \"max_length\"\n",
    "        return_attention_mask=True # 需要attention_mask\n",
    "    )\n",
    "\n",
    "    # 创建标签，初始时与input_ids相同\n",
    "    labels = [list(ids) for ids in model_inputs[\"input_ids\"]] # 深拷贝\n",
    "\n",
    "    # 关键步骤：屏蔽掉提示部分的标签，使其在损失计算中被忽略 (设为-100)\n",
    "    # 我们只希望模型学习预测 \"模型输出：\"之后的内容\n",
    "    for i in range(len(examples[\"text\"])):\n",
    "        prompt_only_text = PROMPT_TEMPLATE.format(input_text=examples[\"text\"][i])\n",
    "        # token_response_keyword = \"模型输出：\" # 定位输出开始的关键词\n",
    "        # prompt_only_text_until_response = prompt_only_text.split(token_response_keyword)[0] + token_response_keyword\n",
    "        \n",
    "        # Tokenize不包含答案的提示部分，以确定需要屏蔽的长度\n",
    "        # Qwen的tokenizer在tokenize提示时可能会有所不同，这里需要精确匹配\n",
    "        # 更稳妥的方法是找到 \"模型输出：\" 之后token的起始位置\n",
    "        \n",
    "        # 找到 \"模型输出：\" 在完整提示中的位置，并获取其tokenize后的长度\n",
    "        # 这里用一个近似方法：tokenize不包含答案的提示部分\n",
    "        # 注意：Qwen tokenizer 对于 chat template 有特定处理，直接 format 可能不完全等同于 chat template 的tokenize结果\n",
    "        # 但对于我们自定义的 PROMPT_TEMPLATE，这种方式是可行的。\n",
    "        \n",
    "        # 找到 \"模型输出：\" 在完整tokenize序列中的位置\n",
    "        # 这是一个复杂点，因为 \"模型输出：\" 本身会被tokenize\n",
    "        # 一个更简单的方法是，我们知道答案是从 PROMPT_TEMPLATE.format(...) 之后开始的\n",
    "        \n",
    "        temp_inputs_for_prompt_only = tokenizer(\n",
    "            PROMPT_TEMPLATE.format(input_text=examples[\"text\"][i]),\n",
    "            max_length=MAX_INPUT_LENGTH,\n",
    "            truncation=True,\n",
    "            padding=False,\n",
    "            add_special_tokens=False # 通常在构建完整序列时，首尾的特殊token由整体控制\n",
    "                                     # 但Qwen tokenizer可能在内部添加，需要实验\n",
    "        )\n",
    "        prompt_length = len(temp_inputs_for_prompt_only[\"input_ids\"])\n",
    "        \n",
    "        # 屏蔽提示部分的标签\n",
    "        for j in range(prompt_length):\n",
    "            if j < len(labels[i]): # 确保不越界\n",
    "                 labels[i][j] = -100\n",
    "            else: # 如果prompt_length超过了当前样本的总长度（可能因为截断），则停止\n",
    "                 break\n",
    "        \n",
    "        # 确保 eos_token 不被屏蔽（如果它在答案的末尾）\n",
    "        # 因为我们添加了 eos_token 到 target_output 之后，它应该在计算损失的范围内\n",
    "        # 如果 tokenizer 自动在末尾添加 eos_token，且未包含在 target_output + eos_token 中，\n",
    "        # 那么 labels[i] 的最后一个非-100元素之后直到序列末尾都应是-100（除了真正的eos_token）\n",
    "\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ee6e7b7-29c3-4510-af89-e7cb3567fc90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始对数据集进行tokenize和预处理 (适配Causal LM)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbeb859f580e4ed9a12a251ea30a98fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3960 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a651245f3d04f269c0ba2a51d6c5744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "数据tokenize和预处理完成:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 3960\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 40\n",
      "    })\n",
      "})\n",
      "\n",
      "Tokenize后的训练集样本 (检查input_ids和labels的屏蔽情况):\n",
      "  原始输入文本 (text): 是的，反黑不要一上来就骂，会引起反感的\n",
      "  原始目标输出 (quadruples_str): 反黑 | 不要一上来就骂 | Racism | hate [END]\n",
      "\n",
      "  Tokenized input_ids (前60): [44047, 30768, 64462, 60, 1115, 37931, 39071, 56568, 101909, 104715, 104811, 116253, 43815, 101042, 110498, 3837, 102093, 100751, 99338, 101425, 26381, 115076, 52334, 117828, 109445, 102450, 1773, 14880, 100345, 20002, 103008, 108704, 3837, 102450, 90919, 102670, 117828, 109445, 57191, 65676, 117828, 9370, 85641, 33071, 109445, 90395, 101892, 87752, 68805, 66017, 46944, 57191, 101213, 63703, 23305, 40027, 28311, 85641, 64429, 320]\n",
      "  Decoded input_ids (前60): <s>[INST] <<SYS>>\n",
      "你是一个专业的中文社交媒体内容分析助手，专门用于细粒度片段级仇恨言论识别。请根据用户提供的文本，识别其中存在的仇恨言论或非仇恨的评论性言论，并按照以下格式输出一个或多个四元组：\n",
      "评论对象 (\n",
      "\n",
      "  Tokenized labels (前60, -100表示已屏蔽): [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "  Decoded labels from first non-masked token (部分): 反黑 | 不要一上来就骂 | Racism | hate [END]<|im_end|>\n",
      "\n",
      "  Attention_mask (前60): [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"开始对数据集进行tokenize和预处理 (适配Causal LM)...\")\n",
    "if 'raw_datasets' not in locals() or not raw_datasets['train']: \n",
    "    print(\"错误: 'raw_datasets' 未定义或为空，无法进行tokenize。请先成功执行数据加载单元格。\")\n",
    "else:\n",
    "    # 使用之前为Causal LM定义的预处理函数\n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "        preprocess_function_causal, \n",
    "        batched=True, # 批处理以提高效率\n",
    "        remove_columns=raw_datasets[\"train\"].column_names \n",
    "    )\n",
    "    print(\"\\n数据tokenize和预处理完成:\")\n",
    "    print(tokenized_datasets) \n",
    "\n",
    "    if tokenized_datasets and 'train' in tokenized_datasets and len(tokenized_datasets['train']) > 0:\n",
    "        print(f\"\\nTokenize后的训练集样本 (检查input_ids和labels的屏蔽情况):\")\n",
    "        sample_idx = 0\n",
    "        print(f\"  原始输入文本 (text): {raw_datasets['train'][sample_idx]['text']}\")\n",
    "        print(f\"  原始目标输出 (quadruples_str): {raw_datasets['train'][sample_idx]['quadruples_str']}\")\n",
    "        \n",
    "        print(f\"\\n  Tokenized input_ids (前60): {tokenized_datasets['train'][sample_idx]['input_ids'][:60]}\")\n",
    "        print(f\"  Decoded input_ids (前60): {tokenizer.decode(tokenized_datasets['train'][sample_idx]['input_ids'][:60])}\")\n",
    "        \n",
    "        print(f\"\\n  Tokenized labels (前60, -100表示已屏蔽): {tokenized_datasets['train'][sample_idx]['labels'][:60]}\")\n",
    "        # 找到第一个非-100的标签，解码该部分以验证\n",
    "        first_label_idx = -1\n",
    "        for idx, lbl_id in enumerate(tokenized_datasets['train'][sample_idx]['labels']):\n",
    "            if lbl_id != -100:\n",
    "                first_label_idx = idx\n",
    "                break\n",
    "        if first_label_idx != -1:\n",
    "            print(f\"  Decoded labels from first non-masked token (部分): {tokenizer.decode([l for l in tokenized_datasets['train'][sample_idx]['labels'][first_label_idx:first_label_idx+30] if l != -100])}\")\n",
    "        else:\n",
    "            print(\"  注意：该样本的所有标签都被屏蔽了，可能存在问题或该样本答案部分被截断。\")\n",
    "            \n",
    "        if 'attention_mask' in tokenized_datasets['train'][sample_idx]:\n",
    "             print(f\"\\n  Attention_mask (前60): {tokenized_datasets['train'][sample_idx]['attention_mask'][:60]}\")\n",
    "    else:\n",
    "        print(\"\\n警告: Tokenize后的数据集为空或不完整。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "541b1012-e5bf-4b74-9590-23230b8988d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准备从 '/root/autodl-tmp/models/Qwen3-8B' 加载预训练的Causal LM...\n",
      "使用4-bit量化 (nf4) 加载模型。\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5697388e63842029159866698c4c59a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型 '/root/autodl-tmp/models/Qwen3-8B' 加载完成。\n",
      "\n",
      "启用LoRA进行参数高效微调。\n",
      "模型已为k-bit训练准备就绪 (LoRA适配)。\n",
      "LoRA配置已创建:\n",
      "LoraConfig(task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=16, target_modules={'down_proj', 'o_proj', 'gate_proj', 'q_proj', 'k_proj', 'up_proj', 'v_proj'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)\n",
      "\n",
      "LoRA适配器已应用到模型。\n",
      "trainable params: 43,646,976 || all params: 8,234,382,336 || trainable%: 0.5301\n",
      "\n",
      "模型已手动移动到设备: cuda:0\n",
      "当前模型所在设备（通过model.device）: cuda:0\n",
      "模型层设备分布 (hf_device_map): {'': 0}\n"
     ]
    }
   ],
   "source": [
    "print(f\"准备从 '{MODEL_NAME}' 加载预训练的Causal LM...\")\n",
    "\n",
    "# 量化配置 (如果启用)\n",
    "bnb_config = None\n",
    "if USE_QUANTIZATION:\n",
    "    if QUANTIZATION_TYPE == \"nf4\" or QUANTIZATION_TYPE == \"fp4\":\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=QUANTIZATION_TYPE, # \"nf4\" 或 \"fp4\"\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16, # 计算时使用的类型，bfloat16 更稳定\n",
    "            bnb_4bit_use_double_quant=True, # 双量化\n",
    "        )\n",
    "        print(f\"使用4-bit量化 ({QUANTIZATION_TYPE}) 加载模型。\")\n",
    "    elif QUANTIZATION_TYPE == \"int8\":\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "        )\n",
    "        print(\"使用8-bit量化加载模型。\")\n",
    "    else:\n",
    "        print(f\"警告：不支持的量化类型 '{QUANTIZATION_TYPE}'，将不使用量化加载。\")\n",
    "\n",
    "\n",
    "# 加载预训练的Causal LM (如Qwen)\n",
    "# trust_remote_code=True 对很多HF上的模型是必要的\n",
    "# device_map=\"auto\" 可用于多GPU或显存不足时自动分配模型层\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config if USE_QUANTIZATION else None,\n",
    "    trust_remote_code=True,\n",
    "    #device_map=\"auto\" # 自动将模型分布到可用设备，对大模型友好\n",
    "    # torch_dtype=torch.bfloat16 # 如果不量化，可以尝试用bfloat16加载以节省显存并加速 (在支持的GPU上)\n",
    ")\n",
    "print(f\"模型 '{MODEL_NAME}' 加载完成。\")\n",
    "\n",
    "# 如果tokenizer的pad_token被设置为eos_token，模型的config中也最好同步\n",
    "if tokenizer.pad_token_id == tokenizer.eos_token_id:\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "    print(f\"模型配置的pad_token_id已设置为eos_token_id: {model.config.eos_token_id}\")\n",
    "\n",
    "\n",
    "if USE_LORA:\n",
    "    print(\"\\n启用LoRA进行参数高效微调。\")\n",
    "    # 如果使用了k-bit量化(4-bit/8-bit)，需要先准备模型\n",
    "    if USE_QUANTIZATION:\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "        print(\"模型已为k-bit训练准备就绪 (LoRA适配)。\")\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=LORA_R, \n",
    "        lora_alpha=LORA_ALPHA, \n",
    "        target_modules=LORA_TARGET_MODULES, # 确保这些模块在您的Qwen模型中存在\n",
    "        lora_dropout=LORA_DROPOUT, \n",
    "        bias=\"none\", \n",
    "        task_type=TaskType.CAUSAL_LM # 任务类型设置为CAUSAL_LM\n",
    "    )\n",
    "    print(\"LoRA配置已创建:\")\n",
    "    print(lora_config)\n",
    "    \n",
    "    model = get_peft_model(model, lora_config) \n",
    "    print(\"\\nLoRA适配器已应用到模型。\")\n",
    "    model.print_trainable_parameters() \n",
    "else:\n",
    "    print(\"\\n未启用LoRA，将进行全参数微调 (如果资源允许且未量化)。\")\n",
    "\n",
    "# 注意：如果使用了 device_map=\"auto\"，模型可能已部分或全部在GPU上，无需再手动 .to(DEVICE)\n",
    "# 但如果未使用 device_map 或希望确保在特定主设备，可以取消下面行的注释（但要小心与device_map冲突）\n",
    "target_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # 或者直接使用之前定义的 DEVICE\n",
    "model.to(target_device)\n",
    "print(f\"\\n模型已手动移动到设备: {model.device}\") # 现在应该显示 cuda:0\n",
    "\n",
    "print(f\"当前模型所在设备（通过model.device）: {model.device}\")\n",
    "if hasattr(model, 'hf_device_map'):\n",
    "    print(f\"模型层设备分布 (hf_device_map): {model.hf_device_map}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b0bbf42-0061-4b97-ab47-ad65e21f3942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "评估指标相关函数 (parse_quadruples, calculate_f1_metrics, compute_metrics_causal) 已定义。\n"
     ]
    }
   ],
   "source": [
    "# 解析和F1计算函数与之前Seq2Seq版本类似，因为它们处理的是文本字符串\n",
    "def parse_quadruples(text_str):\n",
    "    \"\"\"\n",
    "    将模型生成的单个目标字符串解析回结构化的四元组列表。\n",
    "    \"\"\"\n",
    "    quadruples = [] \n",
    "    if not isinstance(text_str, str) or not text_str.strip(): \n",
    "        return [] \n",
    "        \n",
    "    parts = text_str.split(SEP_TOKEN) \n",
    "    for part_idx, part in enumerate(parts):\n",
    "        part_cleaned = part.strip() \n",
    "        \n",
    "        if part_cleaned.endswith(END_TOKEN):\n",
    "            part_cleaned = part_cleaned[:-len(END_TOKEN)].strip() \n",
    "        elif not part_cleaned and part_idx == len(parts) -1 : \n",
    "             continue\n",
    "\n",
    "        if not part_cleaned: \n",
    "            continue\n",
    "            \n",
    "        elements = [e.strip() for e in part_cleaned.split(\"|\")] \n",
    "        \n",
    "        if len(elements) == 4:\n",
    "            quadruples.append(elements)\n",
    "        # else:\n",
    "            # print(f\"解析警告: 无法将部分 '{part_cleaned}' 解析为4个元素。实际得到 {len(elements)} 个元素: {elements}\")\n",
    "    return quadruples\n",
    "\n",
    "\n",
    "def calculate_f1_metrics(preds_quads_list, labels_quads_list):\n",
    "    \"\"\"\n",
    "    根据预测的四元组列表和真实的四元组列表，计算硬匹配和软匹配的F1分数。\n",
    "    \"\"\"\n",
    "    true_positives_hard = 0\n",
    "    predicted_positives_hard = 0 \n",
    "    actual_positives_hard = 0    \n",
    "\n",
    "    true_positives_soft = 0\n",
    "    predicted_positives_soft = 0\n",
    "    actual_positives_soft = 0\n",
    "\n",
    "    for pred_quads_for_sample, gold_quads_for_sample in zip(preds_quads_list, labels_quads_list):\n",
    "        predicted_positives_hard += len(pred_quads_for_sample)\n",
    "        actual_positives_hard += len(gold_quads_for_sample)\n",
    "        predicted_positives_soft += len(pred_quads_for_sample)\n",
    "        actual_positives_soft += len(gold_quads_for_sample)\n",
    "\n",
    "        matched_gold_indices_hard = set()\n",
    "        for p_quad in pred_quads_for_sample:\n",
    "            for i, g_quad in enumerate(gold_quads_for_sample):\n",
    "                if i in matched_gold_indices_hard: \n",
    "                    continue\n",
    "                if p_quad == g_quad: \n",
    "                    true_positives_hard += 1\n",
    "                    matched_gold_indices_hard.add(i)\n",
    "                    break \n",
    "        \n",
    "        matched_gold_indices_soft = set()\n",
    "        for p_quad in pred_quads_for_sample:\n",
    "            if len(p_quad) != 4: continue\n",
    "            for i, g_quad in enumerate(gold_quads_for_sample):\n",
    "                if len(g_quad) != 4: continue \n",
    "                if i in matched_gold_indices_soft:\n",
    "                    continue\n",
    "                if p_quad[2] == g_quad[2] and p_quad[3] == g_quad[3]:\n",
    "                    sim_target = difflib.SequenceMatcher(None, p_quad[0], g_quad[0]).ratio()\n",
    "                    sim_argument = difflib.SequenceMatcher(None, p_quad[1], g_quad[1]).ratio()\n",
    "                    if sim_target > 0.5 and sim_argument > 0.5: \n",
    "                        true_positives_soft += 1\n",
    "                        matched_gold_indices_soft.add(i)\n",
    "                        break \n",
    "\n",
    "    precision_hard = true_positives_hard / predicted_positives_hard if predicted_positives_hard > 0 else 0\n",
    "    recall_hard = true_positives_hard / actual_positives_hard if actual_positives_hard > 0 else 0\n",
    "    f1_hard = 2 * (precision_hard * recall_hard) / (precision_hard + recall_hard) if (precision_hard + recall_hard) > 0 else 0\n",
    "\n",
    "    precision_soft = true_positives_soft / predicted_positives_soft if predicted_positives_soft > 0 else 0\n",
    "    recall_soft = true_positives_soft / actual_positives_soft if actual_positives_soft > 0 else 0\n",
    "    f1_soft = 2 * (precision_soft * recall_soft) / (precision_soft + recall_soft) if (precision_soft + recall_soft) > 0 else 0\n",
    "    \n",
    "    avg_f1 = (f1_hard + f1_soft) / 2\n",
    "\n",
    "    return {\n",
    "        \"f1_hard\": f1_hard, \"precision_hard\": precision_hard, \"recall_hard\": recall_hard,\n",
    "        \"f1_soft\": f1_soft, \"precision_soft\": precision_soft, \"recall_soft\": recall_soft,\n",
    "        \"avg_f1\": avg_f1\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_metrics_causal(eval_preds):\n",
    "    \"\"\"\n",
    "    Trainer在评估时调用的函数，用于计算Causal LM的自定义指标。\n",
    "    eval_preds: 一个包含 predictions 和 label_ids 的元组。\n",
    "                predictions 是模型生成（或logits），label_ids 是真实标签。\n",
    "    \"\"\"\n",
    "    # predictions 是模型生成（或logits），label_ids 是真实标签。\n",
    "    # 对于CausalLM，如果 Trainer 中没有特殊设置，predictions 可能是 logits。\n",
    "    # 但如果使用了 generation_config 或类似设置，或者在 SFTTrainer 中，它可能是生成的 token ID。\n",
    "    # 这里假设 predictions 是生成的 token ID 序列 (因为我们会在 TrainingArguments 中启用 generation)\n",
    "    generated_token_ids, label_ids_from_input = eval_preds \n",
    "    \n",
    "    # 将 label_ids 中的 -100 (用于在损失计算中忽略的填充token) 替换为 tokenizer 的 pad_token_id，以便正确解码\n",
    "    processed_label_ids = np.where(label_ids_from_input != -100, label_ids_from_input, tokenizer.pad_token_id)\n",
    "    \n",
    "    # 解码生成的 token ID\n",
    "    # skip_special_tokens=True 会移除解码结果中的特殊token\n",
    "    # clean_up_tokenization_spaces=True 会清理tokenization过程中可能产生的额外空格\n",
    "    decoded_preds_str = tokenizer.batch_decode(generated_token_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    \n",
    "    # 解码真实的标签 ID (只解码答案部分)\n",
    "    # 注意：对于Causal LM，label_ids_from_input 包含了整个输入序列（提示+答案）\n",
    "    # 我们只关心答案部分的真实标签。\n",
    "    # 而 decoded_preds_str 应该是模型仅生成的答案部分（不含提示）。\n",
    "    # 所以，我们需要从原始数据中获取真实的“答案”字符串进行比较。\n",
    "    \n",
    "    # 这里有一个不匹配：decoded_preds_str 是模型生成的纯答案。\n",
    "    # 但 processed_label_ids 解码后会包含提示+答案。\n",
    "    # 我们需要真实的“答案”字符串，这在原始数据中是 quadruples_str。\n",
    "    # Trainer 的 eval_dataset 通常不直接传递原始字符串。\n",
    "    # 解决方案：在评估时，我们主要关心模型生成的质量。\n",
    "    # 真实的 quadruples_str 需要从原始验证集中获取，这在 compute_metrics 中有点麻烦。\n",
    "    \n",
    "    # 简化的方法：假设 label_ids_from_input 只包含答案部分（如果数据加载时已处理）\n",
    "    # 或者，更标准的方法是，模型生成时，我们只给它提示，它生成答案。\n",
    "    # 然后，我们将生成的答案与原始数据中的 quadruples_str 比较。\n",
    "\n",
    "    # 当前的 preprocess_function_causal 使 label_ids_from_input 对应完整序列，但提示部分是-100\n",
    "    # 所以，解码 processed_label_ids 会得到 提示+答案。我们需要从中提取答案。\n",
    "    \n",
    "    decoded_labels_full_str = tokenizer.batch_decode(processed_label_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    \n",
    "    # 从解码的完整标签中提取真实的答案部分\n",
    "    # 这需要知道提示的结构，或者找到 \"模型输出：\" 之后的文本\n",
    "    actual_target_strs = []\n",
    "    for full_label_text in decoded_labels_full_str:\n",
    "        # 找到 \"模型输出：\" 之后的内容\n",
    "        # 注意：解码后的文本可能与原始提示不完全一致（由于tokenize和decode过程）\n",
    "        # 最可靠的方式是使用原始未tokenize的 quadruples_str，但这不易在 compute_metrics 中直接获得\n",
    "        # 这里我们尝试从解码后的完整标签中提取\n",
    "        if \"模型输出：\" in full_label_text:\n",
    "            actual_target_strs.append(full_label_text.split(\"模型输出：\", 1)[-1].strip())\n",
    "        else: # 如果关键词未找到，可能意味着这个样本的标签部分为空或被完全截断\n",
    "            actual_target_strs.append(\"\") \n",
    "\n",
    "\n",
    "    # 解析四元组\n",
    "    pred_quads_list = [parse_quadruples(p_str) for p_str in decoded_preds_str]\n",
    "    label_quads_list = [parse_quadruples(l_str) for l_str in actual_target_strs] # 使用提取的或原始的真实答案\n",
    "    \n",
    "    results = calculate_f1_metrics(pred_quads_list, label_quads_list)\n",
    "    return results\n",
    "\n",
    "print(\"评估指标相关函数 (parse_quadruples, calculate_f1_metrics, compute_metrics_causal) 已定义。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3707a36f-190b-47f4-b2ab-1ae6eb2c6ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: 当前Jupyter内核实际使用的 Hugging Face Transformers 版本: 4.52.4\n",
      "DEBUG: 当前Jupyter内核实际使用的 Torch 版本: 2.5.1+cu124\n",
      "DEBUG: 根据计算，每个epoch大约有 198 个更新步骤。\n",
      "DEBUG: 尝试初始化 TrainingArguments (使用最终确认的参数组合)...\n",
      "训练参数 (TrainingArguments) 配置完成。评估和保存策略均设置为 'steps'，每 198 步执行一次。\n",
      "数据整理器 (DataCollatorForSeq2Seq) 初始化完成。\n"
     ]
    }
   ],
   "source": [
    "import transformers # 导入 transformers 主模块以检查版本\n",
    "import torch # 导入 torch 检查版本\n",
    "# from transformers.trainer_utils import IntervalStrategy # 如果直接用 \"steps\" 字符串，则不需要导入这个\n",
    "\n",
    "# 再次确认 Transformers 和 Torch 版本 (在 Notebook 单元格内执行，确保是内核使用的版本)\n",
    "print(f\"DEBUG: 当前Jupyter内核实际使用的 Hugging Face Transformers 版本: {transformers.__version__}\")\n",
    "print(f\"DEBUG: 当前Jupyter内核实际使用的 Torch 版本: {torch.__version__}\")\n",
    "\n",
    "# --- 计算每个epoch的步数 ---\n",
    "# 这个计算需要在 tokenized_datasets 加载完成之后 (通常在之前的单元格完成)\n",
    "# 并在这里再次确认或计算，以确保 TrainingArguments 获得正确的值。\n",
    "\n",
    "# 设置一个默认值，以防之前的计算步骤因某种原因未正确执行或变量丢失\n",
    "CALCULATED_STEPS_PER_EPOCH = 500 \n",
    "\n",
    "if 'tokenized_datasets' in locals() and \\\n",
    "   'train' in tokenized_datasets and \\\n",
    "   tokenized_datasets['train'] is not None and \\\n",
    "   len(tokenized_datasets['train']) > 0:\n",
    "    \n",
    "    # 确保 TRAIN_BATCH_SIZE 和 GRADIENT_ACCUMULATION_STEPS 是正整数\n",
    "    if 'TRAIN_BATCH_SIZE' in globals() and TRAIN_BATCH_SIZE > 0 and \\\n",
    "       'GRADIENT_ACCUMULATION_STEPS' in globals() and GRADIENT_ACCUMULATION_STEPS > 0:\n",
    "        \n",
    "        # 在单GPU或未使用显式分布式训练（如DDP）时，world_size为1\n",
    "        # effective_train_batch_size_per_step = TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS * (1 if not torch.distributed.is_initialized() else torch.distributed.get_world_size())\n",
    "        # 为简化，并考虑到 device_map=\"auto\" 的常见用法，我们假设并行处理由Trainer内部管理，此处基于单进程计算\n",
    "        effective_train_batch_size_per_step = TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS\n",
    "        \n",
    "        CALCULATED_STEPS_PER_EPOCH = len(tokenized_datasets[\"train\"]) // effective_train_batch_size_per_step\n",
    "        if CALCULATED_STEPS_PER_EPOCH == 0: \n",
    "            CALCULATED_STEPS_PER_EPOCH = 1 # 至少为1步，防止除零或无效值\n",
    "        print(f\"DEBUG: 根据计算，每个epoch大约有 {CALCULATED_STEPS_PER_EPOCH} 个更新步骤。\")\n",
    "    else:\n",
    "        print(f\"DEBUG: TRAIN_BATCH_SIZE 或 GRADIENT_ACCUMULATION_STEPS 未定义或无效，steps_per_epoch 使用默认值 {CALCULATED_STEPS_PER_EPOCH}。\")\n",
    "else:\n",
    "    print(f\"DEBUG: 'tokenized_datasets' 信息不足或训练集为空，steps_per_epoch 使用默认值 {CALCULATED_STEPS_PER_EPOCH}。\")\n",
    "\n",
    "\n",
    "print(f\"DEBUG: 尝试初始化 TrainingArguments (使用最终确认的参数组合)...\")\n",
    "try:\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "        per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "        per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "\n",
    "        # --- 核心配置：使用被证明在您环境中有效的参数名和策略 ---\n",
    "        do_eval=True,                 # 启用评估\n",
    "        eval_strategy=\"steps\",        # 使用 \"steps\" 字符串作为策略名\n",
    "        eval_steps=CALCULATED_STEPS_PER_EPOCH, # 设置评估步数\n",
    "        \n",
    "        save_strategy=\"steps\",        # 使用 \"steps\" 字符串作为策略名\n",
    "        save_steps=CALCULATED_STEPS_PER_EPOCH, # 设置保存步数，与评估步数一致\n",
    "        # ----------------------------------------------------\n",
    "        \n",
    "        save_total_limit=2, # 最多保存的检查点数量\n",
    "        \n",
    "        logging_dir=f\"{OUTPUT_DIR}/logs\", # TensorBoard等日志的输出目录\n",
    "        logging_strategy=\"steps\", # 日志记录策略\n",
    "        # 日志记录步数，例如每epoch记录10次，或至少每50步（如果epoch太短）\n",
    "        logging_steps=max(1, CALCULATED_STEPS_PER_EPOCH // 10 if CALCULATED_STEPS_PER_EPOCH > 10 else 50),\n",
    "        \n",
    "        load_best_model_at_end=True, # 训练结束后加载在验证集上性能最佳的模型\n",
    "        metric_for_best_model=\"avg_f1\", # 用于选择最佳模型的指标名称 (应与compute_metrics返回的键匹配)\n",
    "        greater_is_better=True,      # 上述指标是否越大越好\n",
    "        \n",
    "        # 混合精度训练配置 (根据您的 USE_QUANTIZATION 设置)\n",
    "        fp16=(torch.cuda.is_available() and not USE_QUANTIZATION),\n",
    "        bf16=(torch.cuda.is_bf16_supported() and not USE_QUANTIZATION),\n",
    "\n",
    "        lr_scheduler_type=LR_SCHEDULER_TYPE, # 学习率调度器类型\n",
    "        warmup_ratio=WARMUP_RATIO,           # 学习率预热的比例 (相对于总训练步数)\n",
    "        \n",
    "        report_to=[\"tensorboard\"], # 将训练指标报告给哪些平台 (例如 \"tensorboard\", \"wandb\")\n",
    "        seed=SEED,                 # 全局随机种子，保证可复现性\n",
    "        \n",
    "        optim=\"adamw_torch\", # 使用的优化器 (\"adamw_torch\", \"adamw_hf\", \"adafactor\" 等)\n",
    "        remove_unused_columns=True, # 是否自动移除数据集中模型forward方法不使用的列 (通常推荐True)\n",
    "    )\n",
    "    print(f\"训练参数 (TrainingArguments) 配置完成。评估和保存策略均设置为 'steps'，每 {CALCULATED_STEPS_PER_EPOCH} 步执行一次。\")\n",
    "\n",
    "except Exception as e: \n",
    "    print(f\"DEBUG: TrainingArguments 初始化时捕获到错误: {e}\")\n",
    "    # 如果这里仍然出错，请仔细检查错误信息和所有传入的参数值是否合理\n",
    "    raise e\n",
    "\n",
    "\n",
    "# 初始化数据整理器 (Data Collator)\n",
    "# 这部分代码通常在 TrainingArguments 成功初始化后执行\n",
    "if 'training_args' in locals() and training_args is not None:\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer,\n",
    "        model=model, \n",
    "        label_pad_token_id=-100, # 使用-100填充标签，以便在损失计算中被忽略\n",
    "        pad_to_multiple_of=8 if (training_args.fp16 or training_args.bf16) else None # 对于fp16/bf16训练，填充到8的倍数可能提高效率\n",
    "    )\n",
    "    print(\"数据整理器 (DataCollatorForSeq2Seq) 初始化完成。\")\n",
    "else:\n",
    "    print(\"DEBUG: training_args 未能成功初始化，跳过 DataCollator 初始化。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d51d461-bae8-4d2d-bcd0-93543375733b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14807/199047802.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer 初始化完成。\n"
     ]
    }
   ],
   "source": [
    "# 初始化 Trainer\n",
    "# 注意：这里使用的是 Trainer，而不是 Seq2SeqTrainer\n",
    "trainer = Trainer(\n",
    "    model=model,                         \n",
    "    args=training_args,                  \n",
    "    train_dataset=tokenized_datasets[\"train\"] if tokenized_datasets else None, \n",
    "    eval_dataset=tokenized_datasets[\"validation\"] if tokenized_datasets else None, \n",
    "    tokenizer=tokenizer,                 \n",
    "    data_collator=data_collator,         \n",
    "    compute_metrics=compute_metrics_causal, # 使用为Causal LM调整的评估函数\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.001)] \n",
    ")\n",
    "print(\"Trainer 初始化完成。\")\n",
    "\n",
    "if not tokenized_datasets or not tokenized_datasets[\"train\"]:\n",
    "    print(\"警告: 由于tokenized_datasets为空或不完整，Trainer可能没有正确设置训练集。请检查之前的步骤。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdae80d-b065-4a04-811a-8009b420b2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "即将开始模型训练...\n",
      "模型评估时将使用以下生成配置: num_beams=3, max_new_tokens=256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='198' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  3/198 00:38 < 2:03:30, 0.03 it/s, Epoch 0.01/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"即将开始模型训练...\")\n",
    "if trainer.train_dataset is None:\n",
    "    print(\"错误: 训练数据集未设置，无法开始训练。请检查数据加载和预处理步骤。\")\n",
    "else:\n",
    "    try:\n",
    "        # 为了在评估时让 Trainer 使用 model.generate()，我们需要确保它知道这是一个生成任务。\n",
    "        # 这通常通过 SFTTrainer 或在 TrainingArguments 中设置相关参数（如 generation_config）来完成。\n",
    "        # 对于普通的 Trainer，compute_metrics 将接收模型的 logits 输出（或如果模型本身在forward中生成，则为生成结果）。\n",
    "        # 为了确保 compute_metrics_causal 接收到生成的 token ID 而不是 logits，\n",
    "        # 我们需要在 TrainingArguments 中设置一些与生成相关的参数，\n",
    "        # 或者在 compute_metrics_causal 内部进行 model.generate() 调用（但这更复杂）。\n",
    "        # 一个简单的方法是，如果模型本身是 PeftModel，它通常会正确传递调用给基础模型的 generate。\n",
    "        # TrainingArguments 没有直接的 predict_with_generate，但 Trainer.evaluate 会尝试生成。\n",
    "        # 我们需要在 compute_metrics_causal 内部确认 eval_preds[0] 是生成的 token ID。\n",
    "        # Trainer 在调用 compute_metrics 前会进行 prediction_step，如果模型是生成式的，\n",
    "        # 且 eval_dataset 提供了 input_ids（不含labels），它应该会调用 generate。\n",
    "        # 我们的 tokenized_datasets[\"validation\"] 包含 labels，Trainer 会用它来计算损失，\n",
    "        # 并且如果模型是生成模型，也会生成预测。\n",
    "\n",
    "        # 查看一下 Trainer 的 prediction_loop 逻辑，确保它为 Causal LM 生成文本。\n",
    "        # Trainer 会在 evaluation_loop 中调用 prediction_loop。\n",
    "        # prediction_loop 会调用 model(**inputs) 或 model.generate(**inputs, generation_config)\n",
    "        # 如果 labels is not None，它会计算损失。\n",
    "        # 它也会返回 logits 或生成的序列。\n",
    "        # 如果是 AutoModelForCausalLM，并且有 generation_config，它会生成。\n",
    "        \n",
    "        # 确保模型有 generation_config\n",
    "        if model.generation_config is None:\n",
    "            from transformers import GenerationConfig\n",
    "            model.generation_config = GenerationConfig.from_model_config(model.config)\n",
    "            print(\"已为模型设置默认的GenerationConfig。\")\n",
    "        \n",
    "        # Trainer 会使用 model.generation_config.max_length 等参数\n",
    "        # 我们可以覆盖这些，例如在 TrainingArguments 中使用 generation_max_length\n",
    "        # (但 TrainingArguments 没有这个参数，它是在 Seq2SeqTrainingArguments 中)\n",
    "        # 所以，依赖于 model.generation_config，或者在 predict 时手动传入\n",
    "        model.generation_config.max_new_tokens = MAX_TARGET_LENGTH # 控制生成答案的最大长度\n",
    "        model.generation_config.num_beams = 3\n",
    "        model.generation_config.early_stopping = True\n",
    "        model.generation_config.pad_token_id = tokenizer.pad_token_id # 确保pad_token_id正确\n",
    "        model.generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "        print(f\"模型评估时将使用以下生成配置: num_beams={model.generation_config.num_beams}, max_new_tokens={model.generation_config.max_new_tokens}\")\n",
    "\n",
    "        train_result = trainer.train()\n",
    "        print(\"\\n模型训练完成!\")\n",
    "\n",
    "        print(\"正在保存模型 (LoRA adapter)...\")\n",
    "        trainer.save_model(OUTPUT_DIR) # 对于PEFT，这通常只保存adapter\n",
    "        tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "        print(f\"模型适配器和tokenizer已保存到 '{OUTPUT_DIR}'。\")\n",
    "\n",
    "        metrics = train_result.metrics\n",
    "        trainer.log_metrics(\"train\", metrics) \n",
    "        trainer.save_metrics(\"train\", metrics) \n",
    "        trainer.save_state() \n",
    "        print(\"\\n训练指标已记录和保存。\")\n",
    "        print(f\"训练统计指标: {metrics}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n模型训练过程中发生严重错误: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74c9ace7-b7b5-4b1f-b507-01bf7a21333d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用于预测的模型已准备好，当前设备: cuda:0\n",
      "预测/推理相关函数 (predict_quadruples_causal) 已定义。\n"
     ]
    }
   ],
   "source": [
    "# 加载训练好的模型进行预测/推理\n",
    "# Trainer.model 应该是训练结束后性能最佳的模型 (如果 load_best_model_at_end=True)\n",
    "\n",
    "# 如果需要手动加载 PEFT 模型：\n",
    "# from peft import PeftModel\n",
    "# print(f\"正在从 '{MODEL_NAME}' 加载基础模型 (用于推理)...\")\n",
    "# base_model_for_inference = AutoModelForCausalLM.from_pretrained(\n",
    "# MODEL_NAME,\n",
    "# quantization_config=bnb_config if USE_QUANTIZATION else None, # 与训练时一致的量化\n",
    "# trust_remote_code=True,\n",
    "# device_map=\"auto\" # 或者将其移至特定设备\n",
    "# )\n",
    "# if tokenizer.pad_token_id == tokenizer.eos_token_id: # 确保pad_token_id一致\n",
    "# base_model_for_inference.config.pad_token_id = base_model_for_inference.config.eos_token_id\n",
    "\n",
    "# print(f\"正在从 '{OUTPUT_DIR}' 加载LoRA适配器...\")\n",
    "# model_to_predict = PeftModel.from_pretrained(base_model_for_inference, OUTPUT_DIR)\n",
    "# print(\"LoRA适配器加载完成。\")\n",
    "# model_to_predict = model_to_predict.merge_and_unload() # 可选: 合并权重并卸载LoRA层，得到一个标准模型\n",
    "# print(\"LoRA权重已合并 (如果执行了merge_and_unload)。\")\n",
    "\n",
    "# 这里我们直接使用 trainer.model (因为它应该是最好的，并且已经是PeftModel)\n",
    "model_to_predict = trainer.model \n",
    "model_to_predict.eval() # 设置为评估模式\n",
    "# 如果未使用 device_map=\"auto\" 或者模型不在GPU上，需要手动移动\n",
    "# if DEVICE.type == 'cuda' and not hasattr(model_to_predict, 'hf_device_map'):\n",
    "# model_to_predict.to(DEVICE)\n",
    "print(f\"用于预测的模型已准备好，当前设备: {model_to_predict.device}\")\n",
    "\n",
    "\n",
    "def predict_quadruples_causal(text_list, model, tokenizer_pred):\n",
    "    \"\"\"\n",
    "    使用微调后的Causal LM模型对一批文本进行预测。\n",
    "    \"\"\"\n",
    "    generated_quadruples_str = []\n",
    "    parsed_results_list = []\n",
    "\n",
    "    for text_input in text_list:\n",
    "        # 1. 构建不包含答案的提示\n",
    "        prompt_for_inference = PROMPT_TEMPLATE.format(input_text=text_input)\n",
    "        \n",
    "        # 2. Tokenize提示\n",
    "        # 对于Qwen等模型，tokenizer(..., add_special_tokens=True) 通常是推荐的\n",
    "        # 但如果PROMPT_TEMPLATE已包含所有必要的特殊token (如<s>, </s>, [INST]), 则可能设为False\n",
    "        # Qwen的Chat模型通常期望特定的对话格式，可以通过tokenizer.apply_chat_template处理\n",
    "        # 但我们这里用的是自定义的PROMPT_TEMPLATE，所以直接tokenize\n",
    "        inputs = tokenizer_pred(\n",
    "            prompt_for_inference, \n",
    "            return_tensors=\"pt\", \n",
    "            truncation=True, \n",
    "            max_length=MAX_INPUT_LENGTH - MAX_TARGET_LENGTH, # 给答案留出空间\n",
    "            padding=False # 单个样本推理不需要padding\n",
    "        ).to(model.device) # 将输入移动到模型所在设备\n",
    "\n",
    "        # 3. 使用模型生成输出\n",
    "        with torch.no_grad():\n",
    "            # 设置生成参数\n",
    "            generation_config = model.generation_config\n",
    "            generation_config.max_new_tokens = 1024 # 控制生成答案的最大长度\n",
    "            generation_config.num_beams = 3\n",
    "            generation_config.early_stopping = True\n",
    "            #generation_config.do_sample = True # 如果想要采样而不是beam search\n",
    "            #generation_config.temperature = 0.1\n",
    "            # generation_config.top_k = 50\n",
    "            # generation_config.pad_token_id = tokenizer_pred.eos_token_id # 重要：用于beam search\n",
    "\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                generation_config=generation_config\n",
    "            )\n",
    "        \n",
    "        # 4. 解码生成的token ID\n",
    "        # outputs包含完整的序列 (提示+答案)，我们需要提取答案部分\n",
    "        # generated_ids = outputs[0] # 对于batch_size=1\n",
    "        # prompt_tokens_count = inputs.input_ids.shape[1]\n",
    "        # answer_tokens = generated_ids[prompt_tokens_count:]\n",
    "        \n",
    "        # 更简单的方式：直接解码整个输出，然后通过字符串处理移除提示部分\n",
    "        # 或者，如果tokenizer.decode能正确处理，它可能只解码新生成的部分\n",
    "        # (取决于模型的generate实现和skip_special_tokens)\n",
    "        full_generated_text = tokenizer_pred.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        \n",
    "        # 从完整生成文本中提取答案部分\n",
    "        # 假设答案在 \"模型输出：\" 之后\n",
    "        answer_part_str = \"\"\n",
    "        if \"模型输出：\" in full_generated_text:\n",
    "            answer_part_str = full_generated_text.split(\"模型输出：\", 1)[-1].strip()\n",
    "        else: # 如果模型没有生成 \"模型输出：\"，则取最后一部分作为尝试\n",
    "            # 这部分可能需要根据模型实际输出进行调整\n",
    "            answer_part_str = full_generated_text.replace(prompt_for_inference.replace(\"{input_text}\", text_input).split(\"模型输出：\")[0]+\"模型输出：\", \"\").strip()\n",
    "\n",
    "\n",
    "        generated_quadruples_str.append(answer_part_str)\n",
    "        \n",
    "        # 5. 解析生成的四元组字符串\n",
    "        parsed_quads = parse_quadruples(answer_part_str)\n",
    "        parsed_results_list.append({\n",
    "            \"original_text\": text_input,\n",
    "            \"full_generated_text\": full_generated_text, # 包含提示的完整输出，用于调试\n",
    "            \"extracted_answer_string\": answer_part_str,\n",
    "            \"parsed_quadruples\": parsed_quads\n",
    "        })\n",
    "        \n",
    "    return parsed_results_list\n",
    "\n",
    "print(\"预测/推理相关函数 (predict_quadruples_causal) 已定义。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8969a78c-0a5f-45f0-bf11-48d1e6ff5edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "开始运行示例预测...\n",
      "\n",
      "示例预测结果:\n",
      "原始文本 (Original Text): 那些同性恋真恶心，败坏社会风气。\n",
      "提取的答案字符串 (Extracted Answer): 同性恋 | 真恶心，败坏社会风气 | LGBTQ | hate [END]\n",
      "解析后的四元组 (Parsed Quadruples): [['同性恋', '真恶心，败坏社会风气', 'LGBTQ', 'hate']]\n",
      "------------------------------\n",
      "原始文本 (Original Text): 这道菜味道不错，下次还来。\n",
      "提取的答案字符串 (Extracted Answer): 这道菜 | 味道不错 | non-hate | non-hate [END]\n",
      "解析后的四元组 (Parsed Quadruples): [['这道菜', '味道不错', 'non-hate', 'non-hate']]\n",
      "------------------------------\n",
      "原始文本 (Original Text): 上海人就是排外，看不起外地人。\n",
      "提取的答案字符串 (Extracted Answer): 上海人 | 排外，看不起外地人 | Region | hate [END]\n",
      "解析后的四元组 (Parsed Quadruples): [['上海人', '排外，看不起外地人', 'Region', 'hate']]\n",
      "------------------------------\n",
      "原始文本 (Original Text): 黑人都是罪犯，应该被赶走。\n",
      "提取的答案字符串 (Extracted Answer): 黑人 | 罪犯 | Racism | hate [END]\n",
      "解析后的四元组 (Parsed Quadruples): [['黑人', '罪犯', 'Racism', 'hate']]\n",
      "------------------------------\n",
      "原始文本 (Original Text): 你可真是头蠢驴，这都做不好。我是支持的理中客和鉴权♂太多早该砸砸了还有那种乱开黄腔然后后面个狗头的低能\n",
      "提取的答案字符串 (Extracted Answer): 你 | 头蠢驴 | non-hate | non-hate [END]\n",
      "解析后的四元组 (Parsed Quadruples): [['你', '头蠢驴', 'non-hate', 'non-hate']]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 示例预测\n",
    "sample_test_texts_for_prediction = [\n",
    "    \"那些同性恋真恶心，败坏社会风气。\",\n",
    "    \"这道菜味道不错，下次还来。\",\n",
    "    \"上海人就是排外，看不起外地人。\",\n",
    "    \"黑人都是罪犯，应该被赶走。\",\n",
    "    \"你可真是头蠢驴，这都做不好。\",\n",
    "    \"我是支持的理中客和鉴权♂太多早该砸砸了还有那种乱开黄腔然后后面个狗头的低能\"\n",
    "]\n",
    "\n",
    "print(\"\\n开始运行示例预测...\")\n",
    "if 'model_to_predict' not in locals() or model_to_predict is None:\n",
    "    print(\"错误: 'model_to_predict' 未定义。请确保模型已成功训练并加载。\")\n",
    "else:\n",
    "    predictions = predict_quadruples_causal(sample_test_texts_for_prediction, model_to_predict, tokenizer)\n",
    "    print(\"\\n示例预测结果:\")\n",
    "    for item in predictions:\n",
    "        print(f\"原始文本 (Original Text): {item['original_text']}\")\n",
    "        # print(f\"模型完整输出 (Full Generated Text): {item['full_generated_text']}\") # 用于调试\n",
    "        print(f\"提取的答案字符串 (Extracted Answer): {item['extracted_answer_string']}\")\n",
    "        print(f\"解析后的四元组 (Parsed Quadruples): {item['parsed_quadruples']}\")\n",
    "        print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c04032e-1e8f-4a0a-932f-a98521f16317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "开始处理官方测试文件: ./test1.json\n",
      "正在从 './test1.json' 加载官方测试数据...\n",
      "成功从 './test1.json' 加载了 2000 条测试数据。\n",
      "开始对 2000 条测试数据进行预测 (批次大小: 3)...\n",
      "  正在预测批次 1 / 667...\n",
      "  正在预测批次 2 / 667...\n",
      "  正在预测批次 3 / 667...\n",
      "  正在预测批次 4 / 667...\n",
      "  正在预测批次 5 / 667...\n",
      "  正在预测批次 6 / 667...\n",
      "  正在预测批次 7 / 667...\n",
      "  正在预测批次 8 / 667...\n",
      "  正在预测批次 9 / 667...\n",
      "  正在预测批次 10 / 667...\n",
      "  正在预测批次 11 / 667...\n",
      "  正在预测批次 12 / 667...\n",
      "  正在预测批次 13 / 667...\n",
      "  正在预测批次 14 / 667...\n",
      "  正在预测批次 15 / 667...\n",
      "  正在预测批次 16 / 667...\n",
      "  正在预测批次 17 / 667...\n",
      "  正在预测批次 18 / 667...\n",
      "  正在预测批次 19 / 667...\n",
      "  正在预测批次 20 / 667...\n",
      "  正在预测批次 21 / 667...\n",
      "  正在预测批次 22 / 667...\n",
      "  正在预测批次 23 / 667...\n",
      "  正在预测批次 24 / 667...\n",
      "  正在预测批次 25 / 667...\n",
      "  正在预测批次 26 / 667...\n",
      "  正在预测批次 27 / 667...\n",
      "  正在预测批次 28 / 667...\n",
      "  正在预测批次 29 / 667...\n",
      "  正在预测批次 30 / 667...\n",
      "  正在预测批次 31 / 667...\n",
      "  正在预测批次 32 / 667...\n",
      "  正在预测批次 33 / 667...\n",
      "  正在预测批次 34 / 667...\n",
      "  正在预测批次 35 / 667...\n",
      "  正在预测批次 36 / 667...\n",
      "  正在预测批次 37 / 667...\n",
      "  正在预测批次 38 / 667...\n",
      "  正在预测批次 39 / 667...\n",
      "  正在预测批次 40 / 667...\n",
      "  正在预测批次 41 / 667...\n",
      "  正在预测批次 42 / 667...\n",
      "  正在预测批次 43 / 667...\n",
      "  正在预测批次 44 / 667...\n",
      "  正在预测批次 45 / 667...\n",
      "  正在预测批次 46 / 667...\n",
      "  正在预测批次 47 / 667...\n",
      "  正在预测批次 48 / 667...\n",
      "  正在预测批次 49 / 667...\n",
      "  正在预测批次 50 / 667...\n",
      "  正在预测批次 51 / 667...\n",
      "  正在预测批次 52 / 667...\n",
      "  正在预测批次 53 / 667...\n",
      "  正在预测批次 54 / 667...\n",
      "  正在预测批次 55 / 667...\n",
      "  正在预测批次 56 / 667...\n",
      "  正在预测批次 57 / 667...\n",
      "  正在预测批次 58 / 667...\n",
      "  正在预测批次 59 / 667...\n",
      "  正在预测批次 60 / 667...\n",
      "  正在预测批次 61 / 667...\n",
      "  正在预测批次 62 / 667...\n",
      "  正在预测批次 63 / 667...\n",
      "  正在预测批次 64 / 667...\n",
      "  正在预测批次 65 / 667...\n",
      "  正在预测批次 66 / 667...\n",
      "  正在预测批次 67 / 667...\n",
      "  正在预测批次 68 / 667...\n",
      "  正在预测批次 69 / 667...\n",
      "  正在预测批次 70 / 667...\n",
      "  正在预测批次 71 / 667...\n",
      "  正在预测批次 72 / 667...\n",
      "  正在预测批次 73 / 667...\n",
      "  正在预测批次 74 / 667...\n",
      "  正在预测批次 75 / 667...\n",
      "  正在预测批次 76 / 667...\n",
      "  正在预测批次 77 / 667...\n",
      "  正在预测批次 78 / 667...\n",
      "  正在预测批次 79 / 667...\n",
      "  正在预测批次 80 / 667...\n",
      "  正在预测批次 81 / 667...\n",
      "  正在预测批次 82 / 667...\n",
      "  正在预测批次 83 / 667...\n",
      "  正在预测批次 84 / 667...\n",
      "  正在预测批次 85 / 667...\n",
      "  正在预测批次 86 / 667...\n",
      "  正在预测批次 87 / 667...\n",
      "  正在预测批次 88 / 667...\n",
      "  正在预测批次 89 / 667...\n",
      "  正在预测批次 90 / 667...\n",
      "  正在预测批次 91 / 667...\n",
      "  正在预测批次 92 / 667...\n",
      "  正在预测批次 93 / 667...\n",
      "  正在预测批次 94 / 667...\n",
      "  正在预测批次 95 / 667...\n",
      "  正在预测批次 96 / 667...\n",
      "  正在预测批次 97 / 667...\n",
      "  正在预测批次 98 / 667...\n",
      "  正在预测批次 99 / 667...\n",
      "  正在预测批次 100 / 667...\n",
      "  正在预测批次 101 / 667...\n",
      "  正在预测批次 102 / 667...\n",
      "  正在预测批次 103 / 667...\n",
      "  正在预测批次 104 / 667...\n",
      "  正在预测批次 105 / 667...\n",
      "  正在预测批次 106 / 667...\n",
      "  正在预测批次 107 / 667...\n",
      "  正在预测批次 108 / 667...\n",
      "  正在预测批次 109 / 667...\n",
      "  正在预测批次 110 / 667...\n",
      "  正在预测批次 111 / 667...\n",
      "  正在预测批次 112 / 667...\n",
      "  正在预测批次 113 / 667...\n",
      "  正在预测批次 114 / 667...\n",
      "  正在预测批次 115 / 667...\n",
      "  正在预测批次 116 / 667...\n",
      "  正在预测批次 117 / 667...\n",
      "  正在预测批次 118 / 667...\n",
      "  正在预测批次 119 / 667...\n",
      "  正在预测批次 120 / 667...\n",
      "  正在预测批次 121 / 667...\n",
      "  正在预测批次 122 / 667...\n",
      "  正在预测批次 123 / 667...\n",
      "  正在预测批次 124 / 667...\n",
      "  正在预测批次 125 / 667...\n",
      "  正在预测批次 126 / 667...\n",
      "  正在预测批次 127 / 667...\n",
      "  正在预测批次 128 / 667...\n",
      "  正在预测批次 129 / 667...\n",
      "  正在预测批次 130 / 667...\n",
      "  正在预测批次 131 / 667...\n",
      "  正在预测批次 132 / 667...\n",
      "  正在预测批次 133 / 667...\n",
      "  正在预测批次 134 / 667...\n",
      "  正在预测批次 135 / 667...\n",
      "  正在预测批次 136 / 667...\n",
      "  正在预测批次 137 / 667...\n",
      "  正在预测批次 138 / 667...\n",
      "  正在预测批次 139 / 667...\n",
      "  正在预测批次 140 / 667...\n",
      "  正在预测批次 141 / 667...\n",
      "  正在预测批次 142 / 667...\n",
      "  正在预测批次 143 / 667...\n",
      "  正在预测批次 144 / 667...\n",
      "  正在预测批次 145 / 667...\n",
      "  正在预测批次 146 / 667...\n",
      "  正在预测批次 147 / 667...\n",
      "  正在预测批次 148 / 667...\n",
      "  正在预测批次 149 / 667...\n",
      "  正在预测批次 150 / 667...\n",
      "  正在预测批次 151 / 667...\n",
      "  正在预测批次 152 / 667...\n",
      "  正在预测批次 153 / 667...\n",
      "  正在预测批次 154 / 667...\n",
      "  正在预测批次 155 / 667...\n",
      "  正在预测批次 156 / 667...\n",
      "  正在预测批次 157 / 667...\n",
      "  正在预测批次 158 / 667...\n",
      "  正在预测批次 159 / 667...\n",
      "  正在预测批次 160 / 667...\n",
      "  正在预测批次 161 / 667...\n",
      "  正在预测批次 162 / 667...\n",
      "  正在预测批次 163 / 667...\n",
      "  正在预测批次 164 / 667...\n",
      "  正在预测批次 165 / 667...\n",
      "  正在预测批次 166 / 667...\n",
      "  正在预测批次 167 / 667...\n",
      "  正在预测批次 168 / 667...\n",
      "  正在预测批次 169 / 667...\n",
      "  正在预测批次 170 / 667...\n",
      "  正在预测批次 171 / 667...\n",
      "  正在预测批次 172 / 667...\n",
      "  正在预测批次 173 / 667...\n",
      "  正在预测批次 174 / 667...\n",
      "  正在预测批次 175 / 667...\n",
      "  正在预测批次 176 / 667...\n",
      "  正在预测批次 177 / 667...\n",
      "  正在预测批次 178 / 667...\n",
      "  正在预测批次 179 / 667...\n",
      "  正在预测批次 180 / 667...\n",
      "  正在预测批次 181 / 667...\n",
      "  正在预测批次 182 / 667...\n",
      "  正在预测批次 183 / 667...\n",
      "  正在预测批次 184 / 667...\n",
      "  正在预测批次 185 / 667...\n",
      "  正在预测批次 186 / 667...\n",
      "  正在预测批次 187 / 667...\n",
      "  正在预测批次 188 / 667...\n",
      "  正在预测批次 189 / 667...\n",
      "  正在预测批次 190 / 667...\n",
      "  正在预测批次 191 / 667...\n",
      "  正在预测批次 192 / 667...\n",
      "  正在预测批次 193 / 667...\n",
      "  正在预测批次 194 / 667...\n",
      "  正在预测批次 195 / 667...\n",
      "  正在预测批次 196 / 667...\n",
      "  正在预测批次 197 / 667...\n",
      "  正在预测批次 198 / 667...\n",
      "  正在预测批次 199 / 667...\n",
      "  正在预测批次 200 / 667...\n",
      "  正在预测批次 201 / 667...\n",
      "  正在预测批次 202 / 667...\n",
      "  正在预测批次 203 / 667...\n",
      "  正在预测批次 204 / 667...\n",
      "  正在预测批次 205 / 667...\n",
      "  正在预测批次 206 / 667...\n",
      "  正在预测批次 207 / 667...\n",
      "  正在预测批次 208 / 667...\n",
      "  正在预测批次 209 / 667...\n",
      "  正在预测批次 210 / 667...\n",
      "  正在预测批次 211 / 667...\n",
      "  正在预测批次 212 / 667...\n",
      "  正在预测批次 213 / 667...\n",
      "  正在预测批次 214 / 667...\n",
      "  正在预测批次 215 / 667...\n",
      "  正在预测批次 216 / 667...\n",
      "  正在预测批次 217 / 667...\n",
      "  正在预测批次 218 / 667...\n",
      "  正在预测批次 219 / 667...\n",
      "  正在预测批次 220 / 667...\n",
      "  正在预测批次 221 / 667...\n",
      "  正在预测批次 222 / 667...\n",
      "  正在预测批次 223 / 667...\n",
      "  正在预测批次 224 / 667...\n",
      "  正在预测批次 225 / 667...\n",
      "  正在预测批次 226 / 667...\n",
      "  正在预测批次 227 / 667...\n",
      "  正在预测批次 228 / 667...\n",
      "  正在预测批次 229 / 667...\n",
      "  正在预测批次 230 / 667...\n",
      "  正在预测批次 231 / 667...\n",
      "  正在预测批次 232 / 667...\n",
      "  正在预测批次 233 / 667...\n",
      "  正在预测批次 234 / 667...\n",
      "  正在预测批次 235 / 667...\n",
      "  正在预测批次 236 / 667...\n",
      "  正在预测批次 237 / 667...\n",
      "  正在预测批次 238 / 667...\n",
      "  正在预测批次 239 / 667...\n",
      "  正在预测批次 240 / 667...\n",
      "  正在预测批次 241 / 667...\n",
      "  正在预测批次 242 / 667...\n",
      "  正在预测批次 243 / 667...\n",
      "  正在预测批次 244 / 667...\n",
      "  正在预测批次 245 / 667...\n",
      "  正在预测批次 246 / 667...\n",
      "  正在预测批次 247 / 667...\n",
      "  正在预测批次 248 / 667...\n",
      "  正在预测批次 249 / 667...\n",
      "  正在预测批次 250 / 667...\n",
      "  正在预测批次 251 / 667...\n",
      "  正在预测批次 252 / 667...\n",
      "  正在预测批次 253 / 667...\n",
      "  正在预测批次 254 / 667...\n",
      "  正在预测批次 255 / 667...\n",
      "  正在预测批次 256 / 667...\n",
      "  正在预测批次 257 / 667...\n",
      "  正在预测批次 258 / 667...\n",
      "  正在预测批次 259 / 667...\n",
      "  正在预测批次 260 / 667...\n",
      "  正在预测批次 261 / 667...\n",
      "  正在预测批次 262 / 667...\n",
      "  正在预测批次 263 / 667...\n",
      "  正在预测批次 264 / 667...\n",
      "  正在预测批次 265 / 667...\n",
      "  正在预测批次 266 / 667...\n",
      "  正在预测批次 267 / 667...\n",
      "  正在预测批次 268 / 667...\n",
      "  正在预测批次 269 / 667...\n",
      "  正在预测批次 270 / 667...\n",
      "  正在预测批次 271 / 667...\n",
      "  正在预测批次 272 / 667...\n",
      "  正在预测批次 273 / 667...\n",
      "  正在预测批次 274 / 667...\n",
      "  正在预测批次 275 / 667...\n",
      "  正在预测批次 276 / 667...\n",
      "  正在预测批次 277 / 667...\n",
      "  正在预测批次 278 / 667...\n",
      "  正在预测批次 279 / 667...\n",
      "  正在预测批次 280 / 667...\n",
      "  正在预测批次 281 / 667...\n",
      "  正在预测批次 282 / 667...\n",
      "  正在预测批次 283 / 667...\n",
      "  正在预测批次 284 / 667...\n",
      "  正在预测批次 285 / 667...\n",
      "  正在预测批次 286 / 667...\n",
      "  正在预测批次 287 / 667...\n",
      "  正在预测批次 288 / 667...\n",
      "  正在预测批次 289 / 667...\n",
      "  正在预测批次 290 / 667...\n",
      "  正在预测批次 291 / 667...\n",
      "  正在预测批次 292 / 667...\n",
      "  正在预测批次 293 / 667...\n",
      "  正在预测批次 294 / 667...\n",
      "  正在预测批次 295 / 667...\n",
      "  正在预测批次 296 / 667...\n",
      "  正在预测批次 297 / 667...\n",
      "  正在预测批次 298 / 667...\n",
      "  正在预测批次 299 / 667...\n",
      "  正在预测批次 300 / 667...\n",
      "  正在预测批次 301 / 667...\n",
      "  正在预测批次 302 / 667...\n",
      "  正在预测批次 303 / 667...\n",
      "  正在预测批次 304 / 667...\n",
      "  正在预测批次 305 / 667...\n",
      "  正在预测批次 306 / 667...\n",
      "  正在预测批次 307 / 667...\n",
      "  正在预测批次 308 / 667...\n",
      "  正在预测批次 309 / 667...\n",
      "  正在预测批次 310 / 667...\n",
      "  正在预测批次 311 / 667...\n",
      "  正在预测批次 312 / 667...\n",
      "  正在预测批次 313 / 667...\n",
      "  正在预测批次 314 / 667...\n",
      "  正在预测批次 315 / 667...\n",
      "  正在预测批次 316 / 667...\n",
      "  正在预测批次 317 / 667...\n",
      "  正在预测批次 318 / 667...\n",
      "  正在预测批次 319 / 667...\n",
      "  正在预测批次 320 / 667...\n",
      "  正在预测批次 321 / 667...\n",
      "  正在预测批次 322 / 667...\n",
      "  正在预测批次 323 / 667...\n",
      "  正在预测批次 324 / 667...\n",
      "  正在预测批次 325 / 667...\n",
      "  正在预测批次 326 / 667...\n",
      "  正在预测批次 327 / 667...\n",
      "  正在预测批次 328 / 667...\n",
      "  正在预测批次 329 / 667...\n",
      "  正在预测批次 330 / 667...\n",
      "  正在预测批次 331 / 667...\n",
      "  正在预测批次 332 / 667...\n",
      "  正在预测批次 333 / 667...\n",
      "  正在预测批次 334 / 667...\n",
      "  正在预测批次 335 / 667...\n",
      "  正在预测批次 336 / 667...\n",
      "  正在预测批次 337 / 667...\n",
      "  正在预测批次 338 / 667...\n",
      "  正在预测批次 339 / 667...\n",
      "  正在预测批次 340 / 667...\n",
      "  正在预测批次 341 / 667...\n",
      "  正在预测批次 342 / 667...\n",
      "  正在预测批次 343 / 667...\n",
      "  正在预测批次 344 / 667...\n",
      "  正在预测批次 345 / 667...\n",
      "  正在预测批次 346 / 667...\n",
      "  正在预测批次 347 / 667...\n",
      "  正在预测批次 348 / 667...\n",
      "  正在预测批次 349 / 667...\n",
      "  正在预测批次 350 / 667...\n",
      "  正在预测批次 351 / 667...\n",
      "  正在预测批次 352 / 667...\n",
      "  正在预测批次 353 / 667...\n",
      "  正在预测批次 354 / 667...\n",
      "  正在预测批次 355 / 667...\n",
      "  正在预测批次 356 / 667...\n",
      "  正在预测批次 357 / 667...\n",
      "  正在预测批次 358 / 667...\n",
      "  正在预测批次 359 / 667...\n",
      "  正在预测批次 360 / 667...\n",
      "  正在预测批次 361 / 667...\n",
      "  正在预测批次 362 / 667...\n",
      "  正在预测批次 363 / 667...\n",
      "  正在预测批次 364 / 667...\n",
      "  正在预测批次 365 / 667...\n",
      "  正在预测批次 366 / 667...\n",
      "  正在预测批次 367 / 667...\n",
      "  正在预测批次 368 / 667...\n",
      "  正在预测批次 369 / 667...\n",
      "  正在预测批次 370 / 667...\n",
      "  正在预测批次 371 / 667...\n",
      "  正在预测批次 372 / 667...\n",
      "  正在预测批次 373 / 667...\n",
      "  正在预测批次 374 / 667...\n",
      "  正在预测批次 375 / 667...\n",
      "  正在预测批次 376 / 667...\n",
      "  正在预测批次 377 / 667...\n",
      "  正在预测批次 378 / 667...\n",
      "  正在预测批次 379 / 667...\n",
      "  正在预测批次 380 / 667...\n",
      "  正在预测批次 381 / 667...\n",
      "  正在预测批次 382 / 667...\n",
      "  正在预测批次 383 / 667...\n",
      "  正在预测批次 384 / 667...\n",
      "  正在预测批次 385 / 667...\n",
      "  正在预测批次 386 / 667...\n",
      "  正在预测批次 387 / 667...\n",
      "  正在预测批次 388 / 667...\n",
      "  正在预测批次 389 / 667...\n",
      "  正在预测批次 390 / 667...\n",
      "  正在预测批次 391 / 667...\n",
      "  正在预测批次 392 / 667...\n",
      "  正在预测批次 393 / 667...\n",
      "  正在预测批次 394 / 667...\n",
      "  正在预测批次 395 / 667...\n",
      "  正在预测批次 396 / 667...\n",
      "  正在预测批次 397 / 667...\n",
      "  正在预测批次 398 / 667...\n",
      "  正在预测批次 399 / 667...\n",
      "  正在预测批次 400 / 667...\n",
      "  正在预测批次 401 / 667...\n",
      "  正在预测批次 402 / 667...\n",
      "  正在预测批次 403 / 667...\n",
      "  正在预测批次 404 / 667...\n",
      "  正在预测批次 405 / 667...\n",
      "  正在预测批次 406 / 667...\n",
      "  正在预测批次 407 / 667...\n",
      "  正在预测批次 408 / 667...\n",
      "  正在预测批次 409 / 667...\n",
      "  正在预测批次 410 / 667...\n",
      "  正在预测批次 411 / 667...\n",
      "  正在预测批次 412 / 667...\n",
      "  正在预测批次 413 / 667...\n",
      "  正在预测批次 414 / 667...\n",
      "  正在预测批次 415 / 667...\n",
      "  正在预测批次 416 / 667...\n",
      "  正在预测批次 417 / 667...\n",
      "  正在预测批次 418 / 667...\n",
      "  正在预测批次 419 / 667...\n",
      "  正在预测批次 420 / 667...\n",
      "  正在预测批次 421 / 667...\n",
      "  正在预测批次 422 / 667...\n",
      "  正在预测批次 423 / 667...\n",
      "  正在预测批次 424 / 667...\n",
      "  正在预测批次 425 / 667...\n",
      "  正在预测批次 426 / 667...\n",
      "  正在预测批次 427 / 667...\n",
      "  正在预测批次 428 / 667...\n",
      "  正在预测批次 429 / 667...\n",
      "  正在预测批次 430 / 667...\n",
      "  正在预测批次 431 / 667...\n",
      "  正在预测批次 432 / 667...\n",
      "  正在预测批次 433 / 667...\n",
      "  正在预测批次 434 / 667...\n",
      "  正在预测批次 435 / 667...\n",
      "  正在预测批次 436 / 667...\n",
      "  正在预测批次 437 / 667...\n",
      "  正在预测批次 438 / 667...\n",
      "  正在预测批次 439 / 667...\n",
      "  正在预测批次 440 / 667...\n",
      "  正在预测批次 441 / 667...\n",
      "  正在预测批次 442 / 667...\n",
      "  正在预测批次 443 / 667...\n",
      "  正在预测批次 444 / 667...\n",
      "  正在预测批次 445 / 667...\n",
      "  正在预测批次 446 / 667...\n",
      "  正在预测批次 447 / 667...\n",
      "  正在预测批次 448 / 667...\n",
      "  正在预测批次 449 / 667...\n",
      "  正在预测批次 450 / 667...\n",
      "  正在预测批次 451 / 667...\n",
      "  正在预测批次 452 / 667...\n",
      "  正在预测批次 453 / 667...\n",
      "  正在预测批次 454 / 667...\n",
      "  正在预测批次 455 / 667...\n",
      "  正在预测批次 456 / 667...\n",
      "  正在预测批次 457 / 667...\n",
      "  正在预测批次 458 / 667...\n",
      "  正在预测批次 459 / 667...\n",
      "  正在预测批次 460 / 667...\n",
      "  正在预测批次 461 / 667...\n",
      "  正在预测批次 462 / 667...\n",
      "  正在预测批次 463 / 667...\n",
      "  正在预测批次 464 / 667...\n",
      "  正在预测批次 465 / 667...\n",
      "  正在预测批次 466 / 667...\n",
      "  正在预测批次 467 / 667...\n",
      "  正在预测批次 468 / 667...\n",
      "  正在预测批次 469 / 667...\n",
      "  正在预测批次 470 / 667...\n",
      "  正在预测批次 471 / 667...\n",
      "  正在预测批次 472 / 667...\n",
      "  正在预测批次 473 / 667...\n",
      "  正在预测批次 474 / 667...\n",
      "  正在预测批次 475 / 667...\n",
      "  正在预测批次 476 / 667...\n",
      "  正在预测批次 477 / 667...\n",
      "  正在预测批次 478 / 667...\n",
      "  正在预测批次 479 / 667...\n",
      "  正在预测批次 480 / 667...\n",
      "  正在预测批次 481 / 667...\n",
      "  正在预测批次 482 / 667...\n",
      "  正在预测批次 483 / 667...\n",
      "  正在预测批次 484 / 667...\n",
      "  正在预测批次 485 / 667...\n",
      "  正在预测批次 486 / 667...\n",
      "  正在预测批次 487 / 667...\n",
      "  正在预测批次 488 / 667...\n",
      "  正在预测批次 489 / 667...\n",
      "  正在预测批次 490 / 667...\n",
      "  正在预测批次 491 / 667...\n",
      "  正在预测批次 492 / 667...\n",
      "  正在预测批次 493 / 667...\n",
      "  正在预测批次 494 / 667...\n",
      "  正在预测批次 495 / 667...\n",
      "  正在预测批次 496 / 667...\n",
      "  正在预测批次 497 / 667...\n",
      "  正在预测批次 498 / 667...\n",
      "  正在预测批次 499 / 667...\n",
      "  正在预测批次 500 / 667...\n",
      "  正在预测批次 501 / 667...\n",
      "  正在预测批次 502 / 667...\n",
      "  正在预测批次 503 / 667...\n",
      "  正在预测批次 504 / 667...\n",
      "  正在预测批次 505 / 667...\n",
      "  正在预测批次 506 / 667...\n",
      "  正在预测批次 507 / 667...\n",
      "  正在预测批次 508 / 667...\n",
      "  正在预测批次 509 / 667...\n",
      "  正在预测批次 510 / 667...\n",
      "  正在预测批次 511 / 667...\n",
      "  正在预测批次 512 / 667...\n",
      "  正在预测批次 513 / 667...\n",
      "  正在预测批次 514 / 667...\n",
      "  正在预测批次 515 / 667...\n",
      "  正在预测批次 516 / 667...\n",
      "  正在预测批次 517 / 667...\n",
      "  正在预测批次 518 / 667...\n",
      "  正在预测批次 519 / 667...\n",
      "  正在预测批次 520 / 667...\n",
      "  正在预测批次 521 / 667...\n",
      "  正在预测批次 522 / 667...\n",
      "  正在预测批次 523 / 667...\n",
      "  正在预测批次 524 / 667...\n",
      "  正在预测批次 525 / 667...\n",
      "  正在预测批次 526 / 667...\n",
      "  正在预测批次 527 / 667...\n",
      "  正在预测批次 528 / 667...\n",
      "  正在预测批次 529 / 667...\n",
      "  正在预测批次 530 / 667...\n",
      "  正在预测批次 531 / 667...\n",
      "  正在预测批次 532 / 667...\n",
      "  正在预测批次 533 / 667...\n",
      "  正在预测批次 534 / 667...\n",
      "  正在预测批次 535 / 667...\n",
      "  正在预测批次 536 / 667...\n",
      "  正在预测批次 537 / 667...\n",
      "  正在预测批次 538 / 667...\n",
      "  正在预测批次 539 / 667...\n",
      "  正在预测批次 540 / 667...\n",
      "  正在预测批次 541 / 667...\n",
      "  正在预测批次 542 / 667...\n",
      "  正在预测批次 543 / 667...\n",
      "  正在预测批次 544 / 667...\n",
      "  正在预测批次 545 / 667...\n",
      "  正在预测批次 546 / 667...\n",
      "  正在预测批次 547 / 667...\n",
      "  正在预测批次 548 / 667...\n",
      "  正在预测批次 549 / 667...\n",
      "  正在预测批次 550 / 667...\n",
      "  正在预测批次 551 / 667...\n",
      "  正在预测批次 552 / 667...\n",
      "  正在预测批次 553 / 667...\n",
      "  正在预测批次 554 / 667...\n",
      "  正在预测批次 555 / 667...\n",
      "  正在预测批次 556 / 667...\n",
      "  正在预测批次 557 / 667...\n",
      "  正在预测批次 558 / 667...\n",
      "  正在预测批次 559 / 667...\n",
      "  正在预测批次 560 / 667...\n",
      "  正在预测批次 561 / 667...\n",
      "  正在预测批次 562 / 667...\n",
      "  正在预测批次 563 / 667...\n",
      "  正在预测批次 564 / 667...\n",
      "  正在预测批次 565 / 667...\n",
      "  正在预测批次 566 / 667...\n",
      "  正在预测批次 567 / 667...\n",
      "  正在预测批次 568 / 667...\n",
      "  正在预测批次 569 / 667...\n",
      "  正在预测批次 570 / 667...\n",
      "  正在预测批次 571 / 667...\n",
      "  正在预测批次 572 / 667...\n",
      "  正在预测批次 573 / 667...\n",
      "  正在预测批次 574 / 667...\n",
      "  正在预测批次 575 / 667...\n",
      "  正在预测批次 576 / 667...\n",
      "  正在预测批次 577 / 667...\n",
      "  正在预测批次 578 / 667...\n",
      "  正在预测批次 579 / 667...\n",
      "  正在预测批次 580 / 667...\n",
      "  正在预测批次 581 / 667...\n",
      "  正在预测批次 582 / 667...\n",
      "  正在预测批次 583 / 667...\n",
      "  正在预测批次 584 / 667...\n",
      "  正在预测批次 585 / 667...\n",
      "  正在预测批次 586 / 667...\n",
      "  正在预测批次 587 / 667...\n",
      "  正在预测批次 588 / 667...\n",
      "  正在预测批次 589 / 667...\n",
      "  正在预测批次 590 / 667...\n",
      "  正在预测批次 591 / 667...\n",
      "  正在预测批次 592 / 667...\n",
      "  正在预测批次 593 / 667...\n",
      "  正在预测批次 594 / 667...\n",
      "  正在预测批次 595 / 667...\n",
      "  正在预测批次 596 / 667...\n",
      "  正在预测批次 597 / 667...\n",
      "  正在预测批次 598 / 667...\n",
      "  正在预测批次 599 / 667...\n",
      "  正在预测批次 600 / 667...\n",
      "  正在预测批次 601 / 667...\n",
      "  正在预测批次 602 / 667...\n",
      "  正在预测批次 603 / 667...\n",
      "  正在预测批次 604 / 667...\n",
      "  正在预测批次 605 / 667...\n",
      "  正在预测批次 606 / 667...\n",
      "  正在预测批次 607 / 667...\n",
      "  正在预测批次 608 / 667...\n",
      "  正在预测批次 609 / 667...\n",
      "  正在预测批次 610 / 667...\n",
      "  正在预测批次 611 / 667...\n",
      "  正在预测批次 612 / 667...\n",
      "  正在预测批次 613 / 667...\n",
      "  正在预测批次 614 / 667...\n",
      "  正在预测批次 615 / 667...\n",
      "  正在预测批次 616 / 667...\n",
      "  正在预测批次 617 / 667...\n",
      "  正在预测批次 618 / 667...\n",
      "  正在预测批次 619 / 667...\n",
      "  正在预测批次 620 / 667...\n",
      "  正在预测批次 621 / 667...\n",
      "  正在预测批次 622 / 667...\n",
      "  正在预测批次 623 / 667...\n",
      "  正在预测批次 624 / 667...\n",
      "  正在预测批次 625 / 667...\n",
      "  正在预测批次 626 / 667...\n",
      "  正在预测批次 627 / 667...\n",
      "  正在预测批次 628 / 667...\n",
      "  正在预测批次 629 / 667...\n",
      "  正在预测批次 630 / 667...\n",
      "  正在预测批次 631 / 667...\n",
      "  正在预测批次 632 / 667...\n",
      "  正在预测批次 633 / 667...\n",
      "  正在预测批次 634 / 667...\n",
      "  正在预测批次 635 / 667...\n",
      "  正在预测批次 636 / 667...\n",
      "  正在预测批次 637 / 667...\n",
      "  正在预测批次 638 / 667...\n",
      "  正在预测批次 639 / 667...\n",
      "  正在预测批次 640 / 667...\n",
      "  正在预测批次 641 / 667...\n",
      "  正在预测批次 642 / 667...\n",
      "  正在预测批次 643 / 667...\n",
      "  正在预测批次 644 / 667...\n",
      "  正在预测批次 645 / 667...\n",
      "  正在预测批次 646 / 667...\n",
      "  正在预测批次 647 / 667...\n",
      "  正在预测批次 648 / 667...\n",
      "  正在预测批次 649 / 667...\n",
      "  正在预测批次 650 / 667...\n",
      "  正在预测批次 651 / 667...\n",
      "  正在预测批次 652 / 667...\n",
      "  正在预测批次 653 / 667...\n",
      "  正在预测批次 654 / 667...\n",
      "  正在预测批次 655 / 667...\n",
      "  正在预测批次 656 / 667...\n",
      "  正在预测批次 657 / 667...\n",
      "  正在预测批次 658 / 667...\n",
      "  正在预测批次 659 / 667...\n",
      "  正在预测批次 660 / 667...\n",
      "  正在预测批次 661 / 667...\n",
      "  正在预测批次 662 / 667...\n",
      "  正在预测批次 663 / 667...\n",
      "  正在预测批次 664 / 667...\n",
      "  正在预测批次 665 / 667...\n",
      "  正在预测批次 666 / 667...\n",
      "  正在预测批次 667 / 667...\n",
      "\n",
      "提交文件已成功生成: ./submission2.txt\n",
      "该文件包含 2000 行预测。\n",
      "请检查文件内容是否符合demo.txt的格式。\n"
     ]
    }
   ],
   "source": [
    "import json # 确保导入json库\n",
    "import os   # 确保导入os库\n",
    "\n",
    "# --- 如何加载官方测试数据并生成提交文件的示例 ---\n",
    "\n",
    "def load_official_test_data(file_path):\n",
    "    \"\"\"\n",
    "    加载官方测试数据。\n",
    "    假设文件是一个JSON，其顶级结构是一个列表，列表中的每个元素是一个包含 \"id\" 和 \"content\" 键的字典。\n",
    "    \n",
    "    参数:\n",
    "    - file_path (str): 测试数据JSON文件的路径。\n",
    "    \n",
    "    返回:\n",
    "    - list: 包含所有 \"content\" 字符串的列表。\n",
    "    - list: 包含所有对应 \"id\" 的列表 (可选, 如果需要id进行映射或调试)。\n",
    "    \"\"\"\n",
    "    texts_to_predict = []\n",
    "    ids_from_test_data = [] # 可选，用于追踪ID\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"错误: 测试文件 '{file_path}' 未找到。\")\n",
    "        return texts_to_predict, ids_from_test_data # 返回空列表\n",
    "\n",
    "    print(f\"正在从 '{file_path}' 加载官方测试数据...\")\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f) # 整个文件是一个JSON列表\n",
    "            if not isinstance(data, list):\n",
    "                print(f\"错误: 测试文件 '{file_path}' 的顶级结构不是一个列表。请检查文件格式。\")\n",
    "                return texts_to_predict, ids_from_test_data\n",
    "\n",
    "            for item_num, item in enumerate(data, 1):\n",
    "                if isinstance(item, dict) and \"content\" in item and \"id\" in item:\n",
    "                    texts_to_predict.append(item[\"content\"])\n",
    "                    ids_from_test_data.append(item[\"id\"])\n",
    "                else:\n",
    "                    print(f\"警告: 测试文件 '{file_path}' 中的第 {item_num} 项格式不正确或缺少 'id'/'content' 键，已跳过: {item}\")\n",
    "        \n",
    "        print(f\"成功从 '{file_path}' 加载了 {len(texts_to_predict)} 条测试数据。\")\n",
    "\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"错误: 解析测试文件 '{file_path}' 时发生JSON解码错误。请检查文件是否为有效的JSON格式。\")\n",
    "    except Exception as e:\n",
    "        print(f\"加载测试文件 '{file_path}' 时发生其他错误: {e}\")\n",
    "        \n",
    "    return texts_to_predict, ids_from_test_data\n",
    "\n",
    "# --- 开始处理测试数据并生成提交文件 ---\n",
    "\n",
    "# 确保 'model_to_predict' 和 'tokenizer' 已经从之前的单元格成功加载和设置\n",
    "if 'model_to_predict' not in locals() or model_to_predict is None:\n",
    "    print(\"错误: 'model_to_predict' 未定义。请确保模型已成功训练并赋值给此变量。\")\n",
    "elif 'tokenizer' not in locals() or tokenizer is None:\n",
    "    print(\"错误: 'tokenizer' 未定义。请确保Tokenizer已成功加载。\")\n",
    "else:\n",
    "    # 选择要处理的测试文件 (例如 test1.json 或 test2.json)\n",
    "    # official_test_file_path = \"/kaggle/input/nlptrain/test1.json\" \n",
    "    official_test_file_path = \"./test1.json\" # 或者选择 test2.json\n",
    "\n",
    "    if os.path.exists(official_test_file_path):\n",
    "        print(f\"\\n开始处理官方测试文件: {official_test_file_path}\")\n",
    "        \n",
    "        # 加载测试数据\n",
    "        official_test_texts, official_test_ids = load_official_test_data(official_test_file_path)\n",
    "        \n",
    "        if official_test_texts:\n",
    "            submission_outputs_strings = []\n",
    "            # 为了提高效率，可以分批处理官方测试数据\n",
    "            # 推理时的批次大小，根据您的显存和模型大小调整\n",
    "            # (应与单元格2中的 EVAL_BATCH_SIZE 或一个适合推理的值一致)\n",
    "            inference_batch_size = EVAL_BATCH_SIZE \n",
    "\n",
    "            print(f\"开始对 {len(official_test_texts)} 条测试数据进行预测 (批次大小: {inference_batch_size})...\")\n",
    "            for i in range(0, len(official_test_texts), inference_batch_size):\n",
    "                batch_texts = official_test_texts[i : i + inference_batch_size]\n",
    "                \n",
    "                current_batch_num = (i // inference_batch_size) + 1\n",
    "                total_batches = (len(official_test_texts) + inference_batch_size - 1) // inference_batch_size\n",
    "                print(f\"  正在预测批次 {current_batch_num} / {total_batches}...\")\n",
    "                \n",
    "                # 调用您在单元格12中定义的预测函数\n",
    "                # predict_quadruples_causal 函数返回一个字典列表，\n",
    "                # 每个字典包含 'original_text', 'full_generated_text', 'extracted_answer_string', 'parsed_quadruples'\n",
    "                batch_predictions = predict_quadruples_causal(batch_texts, model_to_predict, tokenizer)\n",
    "                \n",
    "                for item_prediction in batch_predictions:\n",
    "                    # 我们需要的是模型生成的、仅包含四元组格式的字符串\n",
    "                    submission_outputs_strings.append(item_prediction['extracted_answer_string'])\n",
    "            \n",
    "            # 将预测结果按demo.txt的格式保存到 submission.txt 文件\n",
    "            submission_file_path = \"./submission2.txt\" # Kaggle工作目录\n",
    "            try:\n",
    "                with open(submission_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    for line_num, line_content in enumerate(submission_outputs_strings):\n",
    "                        f.write(line_content + \"\\n\")\n",
    "                print(f\"\\n提交文件已成功生成: {submission_file_path}\")\n",
    "                print(f\"该文件包含 {len(submission_outputs_strings)} 行预测。\")\n",
    "                print(\"请检查文件内容是否符合demo.txt的格式。\")\n",
    "            except Exception as e:\n",
    "                print(f\"写入提交文件 '{submission_file_path}' 时发生错误: {e}\")\n",
    "        else:\n",
    "            print(f\"未能从 '{official_test_file_path}' 加载任何测试数据进行预测。\")\n",
    "    else:\n",
    "        print(f\"测试文件路径 '{official_test_file_path}' 不存在。跳过提交文件生成。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb97cf79-8697-40df-bc10-d88f305ca7a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
