{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aef1d2476f82cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bitsandbytes 版本: 0.46.0 已成功导入。\n",
      "当前使用的设备: cuda\n",
      "GPU名称: NVIDIA GeForce RTX 3090\n",
      "已设置 PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n"
     ]
    }
   ],
   "source": [
    "# --- 单元格 1: 环境设置和库导入 🛠️ ---\n",
    "\n",
    "\n",
    "# 在Notebook内部验证和提示bitsandbytes的安装\n",
    "try:\n",
    "    import bitsandbytes as bnb\n",
    "    print(f\"bitsandbytes 版本: {bnb.__version__} 已成功导入。\")\n",
    "except ImportError:\n",
    "    print(\"错误: bitsandbytes 未安装或导入失败。\")\n",
    "    print(\"请尝试在新的单元格中运行: !pip install -U bitsandbytes\")\n",
    "    print(\"或者，如果您使用的是特定CUDA版本，可能需要查找特定的bitsandbytes安装命令。\")\n",
    "    print(\"安装后务必重启Jupyter Kernel！\")\n",
    "    raise\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import difflib\n",
    "import gc \n",
    "from tqdm import tqdm \n",
    "\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq, \n",
    "    EarlyStoppingCallback,\n",
    "    BitsAndBytesConfig,\n",
    "    GenerationConfig\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# 检查可用GPU\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"当前使用的设备: {DEVICE}\")\n",
    "if DEVICE.type == 'cuda':\n",
    "    print(f\"GPU名称: {torch.cuda.get_device_name(0)}\")\n",
    "    # 设置 PYTORCH_CUDA_ALLOC_CONF 来减少显存碎片\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "    print(\"已设置 PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e6f45414641998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 单元格 2: 配置参数 ⚙️ ---\n",
    "MODEL_NAME = \"/root/autodl-tmp/models/Qwen3-1.7B\" \n",
    "# LoRA 配置\n",
    "USE_LORA = True \n",
    "LORA_R = 16 \n",
    "LORA_ALPHA = 32 \n",
    "LORA_DROPOUT = 0.05 \n",
    "LORA_TARGET_MODULES = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "\n",
    "# 量化配置 \n",
    "USE_QUANTIZATION = True \n",
    "QUANTIZATION_TYPE = \"nf4\" \n",
    "\n",
    "# 训练相关参数 (因为经常OOM所以进行保守设置)\n",
    "OUTPUT_DIR = \"/root/autodl-tmp/qwen_hate_speech_finetuned_llm_aug\" # 输出目录名\n",
    "TRAIN_FILE_PATH = \"./train_formatted_for_llm.jsonl\" \n",
    "\n",
    "TRAIN_BATCH_SIZE = 2 # 非常小的批次大小以避免OOM\n",
    "EVAL_BATCH_SIZE = 3  \n",
    "NUM_TRAIN_EPOCHS = 3 \n",
    "LEARNING_RATE = 2e-4 \n",
    "WEIGHT_DECAY = 0.01  \n",
    "MAX_INPUT_LENGTH = 1024 # \n",
    "MAX_TARGET_LENGTH = 256 # 生成目标（四元组字符串）的最大token长度\n",
    "GRADIENT_ACCUMULATION_STEPS = 8 # 增大梯度累积以补偿小批次大小\n",
    "WARMUP_RATIO = 0.03 \n",
    "LR_SCHEDULER_TYPE = \"cosine\" \n",
    "\n",
    "SEED = 42 \n",
    "\n",
    "# 特殊标记定义\n",
    "END_TOKEN = \"[END]\" \n",
    "SEP_TOKEN = \"[SEP]\" \n",
    "TARGET_GROUPS = [\"Region\", \"Racism\", \"Sexism\", \"LGBTQ\", \"others\", \"non-hate\"] \n",
    "HATEFUL_STATUS = [\"hate\", \"non-hate\"]\n",
    "\n",
    "# 定义提示模板结构 \n",
    "PROMPT_TEMPLATE = \"\"\"<s>[INST] <<SYS>>\n",
    "你是一个专业的中文社交媒体内容分析助手，专门用于细粒度片段级仇恨言论识别。请根据用户提供的文本，识别其中存在的仇恨言论或非仇恨的评论性言论，并按照以下格式输出一个或多个四元组：\n",
    "评论对象 (Target) | 论点 (Argument) | 目标群体 (Targeted Group) | 是否仇恨 (Hateful) [END]\n",
    "详细说明：\n",
    "1.  **评论对象 (Target)：** 帖子中被评论或提及的具体人物、群体、事物或概念。如果是针对文本中隐含的、没有明确指出的对象，或者评论是泛指，则设为 \"NULL\"。\n",
    "2.  **论点 (Argument)：** 针对“评论对象”所发表的核心观点、描述或行为，应为文本中的一个关键信息片段。\n",
    "3.  **目标群体 (Targeted Group)：** 指该“评论对象-论点”所涉及或指向的社会群体。其中，目标群体可以有多项，但必须从以下预设类别中选择：\n",
    "    * `Region`：针对特定地域（国家、省份、城市等）人群的评论。\n",
    "    * `Racism`：针对特定种族或民族人群的评论。\n",
    "    * `Sexism`：针对特定性别人群（男性、女性）的评论，或性别歧视、刻板印象。\n",
    "    * `LGBTQ`：针对性少数群体的评论（如同性恋、跨性别等）。\n",
    "    * `others`：针对上述四类之外的特定群体（如特定职业、疾病群体、政治立场群体等）或不构成对特定社会群体的攻击，而是个人攻击、观点评论等。\n",
    "    * `non-hate`：不存在攻击群体。\n",
    "4.  **是否仇恨 (Hateful)：** 判断该“评论对象-论点”是否构成了对“目标群体”的仇恨言论。\n",
    "    * `hate`：构成仇恨。\n",
    "    * `non-hate`：不构成仇恨（包括中性、积极、或一般性负面评论但未达到仇恨程度）。\n",
    "格式要求：\n",
    "* 四元组内各元素之间用 \" | \"（空格竖杠空格）分隔。\n",
    "* 每个四元组必须以 \" [END]\"（空格[END]）结尾。\n",
    "* 如果一条评论中识别出多个独立的评论对象和论点，应输出多个四元组，不同四元组之间用 \" [SEP] \"（空格[SEP]空格）分隔。\n",
    "\n",
    "现在，请处理以下新的输入内容：\n",
    "<</SYS>>\n",
    "\n",
    "用户提供的文本如下：\n",
    "{input_text} [/INST]\n",
    "模型输出：\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285c2d5fea04c6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 单元格 3: 数据加载与准备函数  📄 ---\n",
    "def load_and_prepare_data(file_path, test_size=0.1, random_state=SEED):\n",
    "    input_texts_from_user = []      \n",
    "    target_quadruples_from_assistant = [] \n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"错误: 训练文件 '{file_path}' 未找到。请检查路径是否正确。\")\n",
    "        \n",
    "    print(f\"开始从 '{file_path}' 加载数据 (适配 'messages' 格式)...\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f, 1): \n",
    "            try:\n",
    "                data_item = json.loads(line) \n",
    "                \n",
    "                if \"messages\" not in data_item or not isinstance(data_item[\"messages\"], list):\n",
    "                    # print(f\"警告: 跳过行 (行号 {line_num})，因为缺少 'messages' 键或其值不是列表: {line.strip()}\")\n",
    "                    continue\n",
    "\n",
    "                messages_list = data_item[\"messages\"]\n",
    "                user_content = None\n",
    "                assistant_content = None\n",
    "                \n",
    "                for message_dict in messages_list:\n",
    "                    if \"role\" in message_dict and \"content\" in message_dict:\n",
    "                        if message_dict[\"role\"] == \"user\":\n",
    "                            user_content = message_dict[\"content\"]\n",
    "                        elif message_dict[\"role\"] == \"assistant\":\n",
    "                            assistant_content = message_dict[\"content\"]\n",
    "                \n",
    "                if user_content is not None and assistant_content is not None:\n",
    "                    input_texts_from_user.append(user_content)\n",
    "                    target_quadruples_from_assistant.append(assistant_content)\n",
    "                # else:\n",
    "                    # print(f\"警告: 跳过行 (行号 {line_num})，未能从 'messages' 中同时找到 'user' 和 'assistant' 的有效内容。\")\n",
    "\n",
    "            except json.JSONDecodeError:\n",
    "                # print(f\"警告: 跳过无效的JSON行 (行号 {line_num}): {line.strip()}\")\n",
    "                pass \n",
    "            except Exception: \n",
    "                # print(f\"警告: 处理行 (行号 {line_num}) 时发生未知错误 '{e}': {line.strip()}\")\n",
    "                pass \n",
    "    \n",
    "    if not input_texts_from_user or not target_quadruples_from_assistant:\n",
    "        raise ValueError(f\"错误: 未能从 '{file_path}' 加载任何有效的 'user'/'assistant' 对话数据。\")\n",
    "    \n",
    "    print(f\"成功从 '{file_path}' 加载了 {len(input_texts_from_user)} 条有效的对话记录。\")\n",
    "\n",
    "    print(f\"正在将数据划分为训练集和验证集 (验证集比例: {test_size})...\")\n",
    "    train_texts, val_texts, train_quads, val_quads = train_test_split(\n",
    "        input_texts_from_user, target_quadruples_from_assistant, \n",
    "        test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    print(f\"划分完成: 训练集 {len(train_texts)} 条, 验证集 {len(val_texts)} 条。\")\n",
    "\n",
    "    train_dataset = Dataset.from_dict({\"text\": train_texts, \"quadruples_str\": train_quads})\n",
    "    val_dataset = Dataset.from_dict({\"text\": val_texts, \"quadruples_str\": val_quads})\n",
    "    \n",
    "    return DatasetDict({\"train\": train_dataset, \"validation\": val_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67979915c9d1c1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准备从文件 './train_formatted_for_llm.jsonl' 加载数据...\n",
      "开始从 './train_formatted_for_llm.jsonl' 加载数据 (适配 'messages' 格式)...\n",
      "成功从 './train_formatted_for_llm.jsonl' 加载了 4000 条有效的对话记录。\n",
      "正在将数据划分为训练集和验证集 (验证集比例: 0.1)...\n",
      "划分完成: 训练集 3600 条, 验证集 400 条。\n",
      "\n",
      "数据加载和初步划分成功:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'quadruples_str'],\n",
      "        num_rows: 3600\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'quadruples_str'],\n",
      "        num_rows: 400\n",
      "    })\n",
      "})\n",
      "\n",
      "训练集中的第一个样本示例:\n",
      "  输入文本 (text): 现实里可不是这样的，至少我认识的安徽人民，都是不惹事，不怕事，遇到挑衅，生死看淡不服就干😄😎\n",
      "  目标标签 (quadruples_str): 安徽人民 | 不惹事，不怕事 | non-hate | non-hate [END]\n"
     ]
    }
   ],
   "source": [
    "# --- 单元格 4: 加载并检查原始数据 🧐 ---\n",
    "print(f\"准备从文件 '{TRAIN_FILE_PATH}' 加载数据...\")\n",
    "raw_datasets = None # 初始化\n",
    "try:\n",
    "    raw_datasets = load_and_prepare_data(TRAIN_FILE_PATH)\n",
    "    print(\"\\n数据加载和初步划分成功:\")\n",
    "    print(raw_datasets) \n",
    "    \n",
    "    if raw_datasets and 'train' in raw_datasets and len(raw_datasets['train']) > 0:\n",
    "        print(f\"\\n训练集中的第一个样本示例:\")\n",
    "        print(f\"  输入文本 (text): {raw_datasets['train'][0]['text']}\")\n",
    "        print(f\"  目标标签 (quadruples_str): {raw_datasets['train'][0]['quadruples_str']}\")\n",
    "    else:\n",
    "        print(\"\\n警告: 加载后的 'raw_datasets' 为空或 'train' 部分不完整。请检查数据加载过程。\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n数据加载或准备过程中发生严重错误: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8210e7073a043d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准备为 3600 条训练文本生成伪标签...\n",
      "正在从 '/root/autodl-tmp/models/Qwen3-8B' 加载用于生成伪标签的LLM和Tokenizer...\n",
      "生成器LLM将使用量化: nf4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/accelerate/utils/modeling.py:1569: UserWarning: Current model requires 32.0 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eace4c80237c4605b3ecb25511a6658c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成器LLM及相关资源已尝试清理。\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 65\u001b[0m\n\u001b[1;32m     62\u001b[0m         generator_bnb_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(load_in_8bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m生成器LLM将使用量化: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mGENERATOR_QUANTIZATION_TYPE_FOR_PSEUDO\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mgenerator_bnb_config\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m无\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m generator_model_instance \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mGENERATOR_MODEL_NAME_FOR_PSEUDO\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator_bnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     70\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m generator_model_instance\u001b[38;5;241m.\u001b[39meval() \n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generator_tokenizer_instance\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:571\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    570\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[0;32m--> 571\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    577\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:309\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    311\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:4574\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4564\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4565\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   4567\u001b[0m     (\n\u001b[1;32m   4568\u001b[0m         model,\n\u001b[1;32m   4569\u001b[0m         missing_keys,\n\u001b[1;32m   4570\u001b[0m         unexpected_keys,\n\u001b[1;32m   4571\u001b[0m         mismatched_keys,\n\u001b[1;32m   4572\u001b[0m         offload_index,\n\u001b[1;32m   4573\u001b[0m         error_msgs,\n\u001b[0;32m-> 4574\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4575\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4576\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4580\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4583\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4589\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4590\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4592\u001b[0m \u001b[38;5;66;03m# record tp degree the model sharded to\u001b[39;00m\n\u001b[1;32m   4593\u001b[0m model\u001b[38;5;241m.\u001b[39m_tp_size \u001b[38;5;241m=\u001b[39m tp_size\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:5031\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[0m\n\u001b[1;32m   5029\u001b[0m \u001b[38;5;66;03m# Skip it with fsdp on ranks other than 0\u001b[39;00m\n\u001b[1;32m   5030\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_fsdp_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_quantized):\n\u001b[0;32m-> 5031\u001b[0m     disk_offload_index, cpu_offload_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5032\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5033\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5034\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5035\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5036\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreverse_key_renaming_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5037\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5038\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5039\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5040\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcpu_offload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcpu_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5041\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcpu_offload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcpu_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5042\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5043\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_offloaded_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5044\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5045\u001b[0m \u001b[43m        \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5046\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5047\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5049\u001b[0m \u001b[38;5;66;03m# force memory release if loading multiple shards, to avoid having 2 state dicts in memory in next loop\u001b[39;00m\n\u001b[1;32m   5050\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m state_dict\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:846\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, shard_file, expected_keys, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, cpu_offload_folder, cpu_offload_index, hf_quantizer, is_safetensors, keep_in_fp32_regex, unexpected_keys, device_mesh)\u001b[0m\n\u001b[1;32m    843\u001b[0m     _load_parameter_into_model(model, param_name, param\u001b[38;5;241m.\u001b[39mto(param_device))\n\u001b[1;32m    845\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 846\u001b[0m     \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_quantized_param\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munexpected_keys\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    849\u001b[0m     \u001b[38;5;66;03m# For quantized modules with FSDP/DeepSpeed Stage 3, we need to quantize the parameter on the GPU\u001b[39;00m\n\u001b[1;32m    850\u001b[0m     \u001b[38;5;66;03m# and then cast it to CPU to avoid excessive memory usage on each GPU\u001b[39;00m\n\u001b[1;32m    851\u001b[0m     \u001b[38;5;66;03m# in comparison to the sharded model across GPUs.\u001b[39;00m\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_fsdp_enabled() \u001b[38;5;129;01mor\u001b[39;00m is_deepspeed_zero3_enabled():\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:243\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.create_quantized_param\u001b[0;34m(self, model, param_value, param_name, target_device, state_dict, unexpected_keys)\u001b[0m\n\u001b[1;32m    240\u001b[0m         new_value \u001b[38;5;241m=\u001b[39m new_value\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    242\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m old_value\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n\u001b[0;32m--> 243\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mParams4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m new_value\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/bitsandbytes/nn/modules.py:343\u001b[0m, in \u001b[0;36mParams4bit.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m device, dtype, non_blocking, convert_to_format \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39m_parse_to(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m device\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbnb_quantized:\n\u001b[0;32m--> 343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_quantize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/bitsandbytes/nn/modules.py:302\u001b[0m, in \u001b[0;36mParams4bit._quantize\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_quantize\u001b[39m(\u001b[38;5;28mself\u001b[39m, device):\n\u001b[1;32m    301\u001b[0m     w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 302\u001b[0m     w_4bit, quant_state \u001b[38;5;241m=\u001b[39m \u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantize_4bit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m        \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblocksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompress_statistics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompress_statistics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquant_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquant_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m w_4bit\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_state \u001b[38;5;241m=\u001b[39m quant_state\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/bitsandbytes/functional.py:1008\u001b[0m, in \u001b[0;36mquantize_4bit\u001b[0;34m(A, absmax, out, blocksize, compress_statistics, quant_type, quant_storage)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Quantize tensor A in blocks of 4-bit values.\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \n\u001b[1;32m    985\u001b[0m \u001b[38;5;124;03mQuantizes tensor A by dividing it into blocks which are independently quantized.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;124;03m    - [`QuantState`]: The state object used to undo the quantization.\u001b[39;00m\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1006\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m-> 1008\u001b[0m _out, _absmax \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbitsandbytes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantize_4bit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblocksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquant_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquant_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1015\u001b[0m code \u001b[38;5;241m=\u001b[39m get_4bit_type(quant_type, device\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compress_statistics:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/_ops.py:716\u001b[0m, in \u001b[0;36mOpOverload.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 716\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/_compile.py:32\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[1;32m     30\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m prior \u001b[38;5;241m=\u001b[39m _maybe_set_eval_frame(callback)\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    634\u001b[0m     _maybe_set_eval_frame(prior)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/library.py:566\u001b[0m, in \u001b[0;36m_impl.<locals>.register.<locals>.func_no_dynamo\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_dynamo\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc_no_dynamo\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 566\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/bitsandbytes/backends/cpu/ops.py:115\u001b[0m, in \u001b[0;36m_\u001b[0;34m(A, blocksize, quant_type, quant_storage)\u001b[0m\n\u001b[1;32m    112\u001b[0m scaled \u001b[38;5;241m=\u001b[39m blocks \u001b[38;5;241m/\u001b[39m absmax\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# Quantize with the lookup table\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m quantized \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmin(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscaled\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_NF4_QUANT_TABLE\u001b[49m\u001b[43m)\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39muint8)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# Pack two quantized values per byte\u001b[39;00m\n\u001b[1;32m    118\u001b[0m packed \u001b[38;5;241m=\u001b[39m quantized[::\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m<<\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m|\u001b[39m quantized[\u001b[38;5;241m1\u001b[39m::\u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- 单元格 3.5 (新增): 利用LLM零样本能力生成伪标签数据 🤖🏷️ ---\n",
    "# --- 配置用于生成伪标签的LLM ---\n",
    "GENERATOR_MODEL_NAME_FOR_PSEUDO = \"/root/autodl-tmp/models/Qwen3-8B\" # 示例：使用与微调相同的模型路径，或另一个更强的模型\n",
    "GENERATOR_USE_QUANTIZATION_FOR_PSEUDO = True \n",
    "GENERATOR_QUANTIZATION_TYPE_FOR_PSEUDO = \"nf4\"\n",
    "\n",
    "# 用于生成伪标签的提示模板 (与微调的PROMPT_TEMPLATE类似，但不包含 \"模型输出：\" 后的答案部分)\n",
    "# 注意：这里的 GENERATOR_PROMPT_TEMPLATE 与主 PROMPT_TEMPLATE 几乎一致，\n",
    "# 确保 \"模型输出：\" 之后是空的，以便LLM填充。\n",
    "GENERATOR_PROMPT_TEMPLATE = \"\"\"<s>[INST] <<SYS>>\n",
    "你是一个专业的中文社交媒体内容分析助手，专门用于细粒度片段级仇恨言论识别。请根据用户提供的文本，识别其中存在的仇恨言论或非仇恨的评论性言论，并按照以下格式输出一个或多个四元组：\n",
    "评论对象 (Target) | 论点 (Argument) | 目标群体 (Targeted Group) | 是否仇恨 (Hateful) [END]\n",
    "详细说明：\n",
    "1.  **评论对象 (Target)：** 帖子中被评论或提及的具体人物、群体、事物或概念。如果是针对文本中隐含的、没有明确指出的对象，或者评论是泛指，则设为 \"NULL\"。\n",
    "2.  **论点 (Argument)：** 针对“评论对象”所发表的核心观点、描述或行为，应为文本中的一个关键信息片段。\n",
    "3.  **目标群体 (Targeted Group)：** 指该“评论对象-论点”所涉及或指向的社会群体。必须从以下预设类别中选择：\n",
    "    * `Region`：针对特定地域（国家、省份、城市等）人群的评论。\n",
    "    * `Racism`：针对特定种族或民族人群的评论。\n",
    "    * `Sexism`：针对特定性别人群（男性、女性）的评论，或性别歧视、刻板印象。\n",
    "    * `LGBTQ`：针对性少数群体的评论（如同性恋、跨性别等）。\n",
    "    * `others`：针对上述四类之外的特定群体（如特定职业、疾病群体、政治立场群体等）或不构成对特定社会群体的攻击，而是个人攻击、观点评论等。\n",
    "    * `non-hate`：不存在攻击群体。\n",
    "4.  **是否仇恨 (Hateful)：** 判断该“评论对象-论点”是否构成了对“目标群体”的仇恨言论。\n",
    "    * `hate`：构成仇恨。\n",
    "    * `non-hate`：不构成仇恨（包括中性、积极、或一般性负面评论但未达到仇恨程度）。\n",
    "格式要求：\n",
    "* 四元组内各元素之间用 \" | \"（空格竖杠空格）分隔。\n",
    "* 每个四元组必须以 \" [END]\"（空格[END]）结尾。\n",
    "* 如果一条评论中识别出多个独立的评论对象和论点，应输出多个四元组，不同四元组之间用 \" [SEP] \"（空格[SEP]空格）分隔。\n",
    "\n",
    "现在，请处理以下新的输入内容：\n",
    "<</SYS>>\n",
    "\n",
    "用户提供的文本如下：\n",
    "{input_text} [/INST]\n",
    "模型输出：\n",
    "\"\"\"\n",
    "\n",
    "pseudo_labels_list = []\n",
    "texts_for_pseudo_generation = []\n",
    "\n",
    "if raw_datasets and 'train' in raw_datasets and raw_datasets['train'] is not None:\n",
    "    texts_for_pseudo_generation = list(raw_datasets['train']['text'])\n",
    "    print(f\"准备为 {len(texts_for_pseudo_generation)} 条训练文本生成伪标签...\")\n",
    "\n",
    "    # --- 加载生成器LLM和Tokenizer ---\n",
    "    # 为避免与主模型冲突，使用不同的变量名\n",
    "    generator_model_instance = None\n",
    "    generator_tokenizer_instance = None\n",
    "    print(f\"正在从 '{GENERATOR_MODEL_NAME_FOR_PSEUDO}' 加载用于生成伪标签的LLM和Tokenizer...\")\n",
    "    try:\n",
    "        generator_tokenizer_instance = AutoTokenizer.from_pretrained(GENERATOR_MODEL_NAME_FOR_PSEUDO, trust_remote_code=True)\n",
    "        \n",
    "        generator_bnb_config = None\n",
    "        if GENERATOR_USE_QUANTIZATION_FOR_PSEUDO:\n",
    "            if GENERATOR_QUANTIZATION_TYPE_FOR_PSEUDO == \"nf4\" or GENERATOR_QUANTIZATION_TYPE_FOR_PSEUDO == \"fp4\":\n",
    "                generator_bnb_config = BitsAndBytesConfig(\n",
    "                    load_in_4bit=True, bnb_4bit_quant_type=GENERATOR_QUANTIZATION_TYPE_FOR_PSEUDO,\n",
    "                    bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_use_double_quant=True,\n",
    "                )\n",
    "            elif GENERATOR_QUANTIZATION_TYPE_FOR_PSEUDO == \"int8\":\n",
    "                generator_bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "            print(f\"生成器LLM将使用量化: {GENERATOR_QUANTIZATION_TYPE_FOR_PSEUDO if generator_bnb_config else '无'}\")\n",
    "\n",
    "        generator_model_instance = AutoModelForCausalLM.from_pretrained(\n",
    "            GENERATOR_MODEL_NAME_FOR_PSEUDO,\n",
    "            quantization_config=generator_bnb_config,\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        generator_model_instance.eval() \n",
    "\n",
    "        if generator_tokenizer_instance.pad_token is None:\n",
    "            generator_tokenizer_instance.pad_token = generator_tokenizer_instance.eos_token\n",
    "            if generator_model_instance.config.pad_token_id is None:\n",
    "                 generator_model_instance.config.pad_token_id = generator_tokenizer_instance.pad_token_id\n",
    "            print(f\"生成器Tokenizer的pad_token已设置为eos_token: '{generator_tokenizer_instance.eos_token}'\")\n",
    "        \n",
    "        print(\"生成器LLM和Tokenizer加载成功。\")\n",
    "\n",
    "        GENERATION_BATCH_SIZE = 4 # 伪标签生成批次大小\n",
    "        \n",
    "        generation_config_pseudo = GenerationConfig(\n",
    "            max_new_tokens=MAX_TARGET_LENGTH, \n",
    "            num_beams=1, \n",
    "            do_sample=False, \n",
    "            pad_token_id=generator_tokenizer_instance.pad_token_id if generator_tokenizer_instance.pad_token_id is not None else generator_tokenizer_instance.eos_token_id,\n",
    "            eos_token_id=generator_tokenizer_instance.eos_token_id\n",
    "        )\n",
    "\n",
    "        for i in tqdm(range(0, len(texts_for_pseudo_generation), GENERATION_BATCH_SIZE), desc=\"生成伪标签\"):\n",
    "            batch_texts = texts_for_pseudo_generation[i : i + GENERATION_BATCH_SIZE]\n",
    "            batch_prompts = [GENERATOR_PROMPT_TEMPLATE.format(input_text=text) for text in batch_texts]\n",
    "            \n",
    "            inputs = generator_tokenizer_instance(\n",
    "                batch_prompts, \n",
    "                return_tensors=\"pt\", \n",
    "                padding=True, \n",
    "                truncation=True, \n",
    "                max_length=MAX_INPUT_LENGTH - MAX_TARGET_LENGTH \n",
    "            ).to(generator_model_instance.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = generator_model_instance.generate(**inputs, generation_config=generation_config_pseudo)\n",
    "            \n",
    "            # 解码并提取答案部分\n",
    "            # outputs 包含完整的序列 (提示+答案)。我们需要提取模型生成的部分。\n",
    "            # input_len = inputs.input_ids.shape[1]\n",
    "            # generated_ids_batch = outputs[:, input_len:] # 获取每个样本新生成的token\n",
    "            # decoded_answers = generator_tokenizer_instance.batch_decode(generated_ids_batch, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "            \n",
    "            # 或者，使用之前的方法从完整解码文本中分割\n",
    "            full_decoded_outputs = generator_tokenizer_instance.batch_decode(outputs, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "            keyword_separator_pseudo = \"模型输出：\" # 与GENERATOR_PROMPT_TEMPLATE末尾一致\n",
    "            \n",
    "            for full_output_text in full_decoded_outputs:\n",
    "                answer_part_str = \"\"\n",
    "                if keyword_separator_pseudo in full_output_text:\n",
    "                    answer_part_str = full_output_text.split(keyword_separator_pseudo, 1)[-1].strip()\n",
    "                else: # 后备方案\n",
    "                    original_prompt_text_no_answer = GENERATOR_PROMPT_TEMPLATE.format(input_text=\"DUMMY\").split(keyword_separator_pseudo)[0] # 获取提示头\n",
    "                    # 这是一个粗略的移除，可能不完美\n",
    "                    if full_output_text.startswith(original_prompt_text_no_answer.split(\"用户提供的文本如下：\")[0]): # 尝试匹配系统提示部分\n",
    "                         answer_part_str = full_output_text # 如果无法清晰分割，保留完整输出，后续质量评估时处理\n",
    "                    else:\n",
    "                         answer_part_str = full_output_text\n",
    "                pseudo_labels_list.append(answer_part_str)\n",
    "        \n",
    "        print(f\"成功为 {len(pseudo_labels_list)} 条文本生成了伪标签。\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"加载生成器LLM或生成伪标签过程中发生错误: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        print(\"将使用空的伪标签列表。\")\n",
    "        pseudo_labels_list = [] \n",
    "    finally:\n",
    "        # 清理生成器模型以释放显存\n",
    "        if 'generator_model_instance' in locals() and generator_model_instance is not None:\n",
    "            del generator_model_instance\n",
    "        if 'generator_tokenizer_instance' in locals() and generator_tokenizer_instance is not None:\n",
    "            del generator_tokenizer_instance\n",
    "        if 'inputs' in locals() and inputs is not None: del inputs\n",
    "        if 'outputs' in locals() and outputs is not None: del outputs\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        print(\"生成器LLM及相关资源已尝试清理。\")\n",
    "else:\n",
    "    print(\"警告: 原始数据集 'raw_datasets' 未加载，无法生成伪标签。\")\n",
    "    pseudo_labels_list = []\n",
    "\n",
    "if pseudo_labels_list:\n",
    "    print(\"\\n生成的一些伪标签样本:\")\n",
    "    for i in range(min(3, len(pseudo_labels_list))):\n",
    "        print(f\"  原始文本 (部分): {texts_for_pseudo_generation[i][:50]}...\")\n",
    "        print(f\"  生成伪标签: {pseudo_labels_list[i]}\")\n",
    "else:\n",
    "    print(\"\\n未能生成或加载任何伪标签。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e63aab90f691bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "警告：单元格8的 'parse_quadruples' 函数定义先于此单元格执行。将使用临时占位符。\n",
      "开始基于LLM生成的伪标签（作为负例）进行对比数据增强...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "创建对比增强SFT数据: 100%|██████████| 3600/3600 [00:00<00:00, 500629.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "对比数据增强完成。\n",
      "原始训练样本数: 3600\n",
      "额外创建了 3600 个对比增强样本。\n",
      "对比增强SFT数据准备完成。训练集现在包含 7200 条样本。\n",
      "增强后训练集的第一个样本 text (可能为原始): 现实里可不是这样的，至少我认识的安徽人民，都是不惹事，不怕事，遇到挑衅，生死看淡不服就干😄😎...\n",
      "增强后训练集的第一个样本 quadruple: 安徽人民 | 不惹事，不怕事 | non-hate | non-hate [END]\n",
      "一个对比增强样本的 text (部分): 原始文本内容：\n",
      "\"生而为蝻，我很抱歉这种牧人代表十成是觉得仙女们要彩礼要车要房不愿生孩子还动不动生孩子警告跟你争冠性权的行为很合理吧？？？？？？？？？劳资改个屁的择偶观\"\n",
      "\n",
      "一个AI助手针对以上文本给出了如下可能是错误或不完善的四元组提取结果：\n",
      "\"评论对象 (Target) | 论点 (Argument) | 目标群体 (Targeted Group) | 是否仇恨 (Hateful) [END]\n",
      "评论对象 (Target) | 论点 (Argument) | 目标群体 (Targeted Gr...\n",
      "该增强样本的目标 quadruple: 蝻 | 生而为蝻 | Sexism | hate [SEP] 这种牧人代表 | 十成是觉得仙女们要彩礼要车要房不愿生孩子还动不动生孩子警告跟你争冠性权的行为很合理吧？？？？？？？？？ | Sexism | hate [END]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 单元格 3.6: 利用LLM生成的负例进行对比增强SFT数据 ---\n",
    "\n",
    "# 是否启用基于LLM负例的对比增强\n",
    "ENABLE_CONTRASTIVE_AUGMENTATION_WITH_NEGATIVES = True\n",
    "\n",
    "# 确保 parse_quadruples 函数已定义 (通常在单元格8)\n",
    "# 如果单元格8的还未执行, 使用临时占位符 (主要用于结构完整性, 实际运行前应确保已定义)\n",
    "if 'parse_quadruples' not in globals(): \n",
    "    def parse_quadruples_placeholder(text_str_dummy): \n",
    "        if not text_str_dummy: return []\n",
    "        quads = []\n",
    "        parts = text_str_dummy.split(SEP_TOKEN if 'SEP_TOKEN' in globals() else \"[SEP]\") # 使用全局变量或默认值\n",
    "        for part in parts:\n",
    "            part_c = part.strip()\n",
    "            if part_c.endswith(END_TOKEN if 'END_TOKEN' in globals() else \"[END]\"):\n",
    "                part_c = part_c[:-len(END_TOKEN if 'END_TOKEN' in globals() else \"[END]\")].strip()\n",
    "            if not part_c: continue\n",
    "            elements = [e.strip() for e in part_c.split(\" | \")]\n",
    "            if len(elements) == 4:\n",
    "                quads.append(elements)\n",
    "        return quads\n",
    "    parse_quadruples_fn_to_use = parse_quadruples_placeholder\n",
    "    print(\"警告：单元格8的 'parse_quadruples' 函数定义先于此单元格执行。将使用临时占位符。\")\n",
    "else:\n",
    "    parse_quadruples_fn_to_use = parse_quadruples\n",
    "\n",
    "\n",
    "if ENABLE_CONTRASTIVE_AUGMENTATION_WITH_NEGATIVES and \\\n",
    "   'raw_datasets' in locals() and raw_datasets and \\\n",
    "   'pseudo_labels_list' in locals() and \\\n",
    "   len(pseudo_labels_list) == len(raw_datasets['train']): # 确保伪标签列表与训练数据对齐\n",
    "\n",
    "    print(f\"开始基于LLM生成的伪标签（作为负例）进行对比数据增强...\")\n",
    "    \n",
    "    original_train_texts = list(raw_datasets['train']['text'])\n",
    "    original_train_quads = list(raw_datasets['train']['quadruples_str']) # 真实标签 (正例)\n",
    "    \n",
    "    # pseudo_labels_list 包含的是LLM生成的伪标签 (将被视为负例或干扰项)\n",
    "    \n",
    "    contrastive_augmented_texts = []\n",
    "    contrastive_augmented_quads = [] # 目标输出始终是真实的四元组\n",
    "    \n",
    "    num_augmented_samples_created = 0\n",
    "\n",
    "    for i in tqdm(range(len(original_train_texts)), desc=\"创建对比增强SFT数据\"):\n",
    "        original_text_content = original_train_texts[i]\n",
    "        true_quad_str = original_train_quads[i]         # 正例输出\n",
    "        negative_pseudo_quad_str = pseudo_labels_list[i] # LLM生成的，作为负例/干扰项\n",
    "\n",
    "        # 1. 添加标准的SFT样本： (原始提示 -> 真实四元组)\n",
    "        #    PROMPT_TEMPLATE 中的 {input_text} 将直接使用 original_text_content\n",
    "        contrastive_augmented_texts.append(original_text_content) \n",
    "        contrastive_augmented_quads.append(true_quad_str)\n",
    "        \n",
    "        # 2. 创建对比增强的SFT样本：\n",
    "        #    (包含负例的复杂提示 -> 真实四元组)\n",
    "        #    只有当伪标签与真实标签确实不同时，这种增强才有意义\n",
    "        if negative_pseudo_quad_str and negative_pseudo_quad_str.strip() and \\\n",
    "           negative_pseudo_quad_str.strip() != true_quad_str.strip():\n",
    "            \n",
    "            # 构建包含原始文本和“错误提案”（负例）的新输入文本\n",
    "            # 这个 new_input_for_prompt 会被填入主 PROMPT_TEMPLATE 的 {input_text} 占位符\n",
    "            new_input_for_prompt = (\n",
    "                f\"原始文本内容：\\n\\\"{original_text_content}\\\"\\n\\n\"\n",
    "                f\"一个AI助手针对以上文本给出了如下可能是错误或不完善的四元组提取结果：\\n\"\n",
    "                f\"\\\"{negative_pseudo_quad_str}\\\"\\n\\n\"\n",
    "                f\"请你忽略上述AI助手的提取结果（它可能包含错误），并严格按照指令，根据“原始文本内容”重新分析并给出正确的四元组。\"\n",
    "            )\n",
    "            \n",
    "            contrastive_augmented_texts.append(new_input_for_prompt)\n",
    "            contrastive_augmented_quads.append(true_quad_str) # 目标仍然是真实的四元组\n",
    "            num_augmented_samples_created += 1\n",
    "\n",
    "    print(f\"对比数据增强完成。\")\n",
    "    print(f\"原始训练样本数: {len(original_train_texts)}\")\n",
    "    print(f\"额外创建了 {num_augmented_samples_created} 个对比增强样本。\")\n",
    "    \n",
    "    if num_augmented_samples_created > 0 or len(contrastive_augmented_texts) != len(original_train_texts) :\n",
    "        contrastive_augmented_train_dataset = Dataset.from_dict({\n",
    "            \"text\": contrastive_augmented_texts, # \"text\" 字段现在包含原始文本或增强后的复杂提示\n",
    "            \"quadruples_str\": contrastive_augmented_quads # 目标始终是真实的四元组\n",
    "        })\n",
    "        \n",
    "        # 更新 raw_datasets 中的训练集\n",
    "        # 验证集保持不变，用于评估原始任务性能\n",
    "        raw_datasets['train'] = contrastive_augmented_train_dataset\n",
    "        print(f\"对比增强SFT数据准备完成。训练集现在包含 {len(raw_datasets['train'])} 条样本。\")\n",
    "        if len(raw_datasets['train']) > 0:\n",
    "            print(f\"增强后训练集的第一个样本 text (可能为原始): {raw_datasets['train'][0]['text'][:150]}...\") \n",
    "            print(f\"增强后训练集的第一个样本 quadruple: {raw_datasets['train'][0]['quadruples_str']}\")\n",
    "            if len(raw_datasets['train']) > len(original_train_texts): # 如果确实添加了增强样本\n",
    "                 print(f\"一个对比增强样本的 text (部分): {raw_datasets['train'][-1]['text'][:250]}...\") # 打印最后一个（可能是增强的）\n",
    "                 print(f\"该增强样本的目标 quadruple: {raw_datasets['train'][-1]['quadruples_str']}\")\n",
    "    else:\n",
    "        print(\"没有新的对比增强样本被添加到训练集（可能因为所有伪标签都与真实标签相同，或者伪标签为空）。\")\n",
    "\n",
    "else:\n",
    "    if not ENABLE_CONTRASTIVE_AUGMENTATION_WITH_NEGATIVES:\n",
    "        print(\"基于LLM负例的对比数据增强被禁用。\")\n",
    "    else:\n",
    "        print(\"警告: 未执行基于LLM负例的对比数据增强，因为 'raw_datasets' 或 'pseudo_labels_list' 未正确准备或数量不匹配。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc8304a0035d0de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在从 '/root/autodl-tmp/models/Qwen3-8B' 加载用于微调的Tokenizer...\n",
      "微调Tokenizer加载完成。\n",
      "微调Tokenizer的pad_token已设置为: '<|endoftext|>' (ID: 151643)\n"
     ]
    }
   ],
   "source": [
    "# --- 单元格 5: Tokenizer 初始化和数据预处理函数 📝 ---\n",
    "print(f\"正在从 '{MODEL_NAME}' 加载用于微调的Tokenizer...\") # 主微调模型的Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "print(\"微调Tokenizer加载完成。\")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token \n",
    "    print(f\"微调Tokenizer的pad_token未设置，已将其设置为eos_token: '{tokenizer.eos_token}' (ID: {tokenizer.eos_token_id})\")\n",
    "else:\n",
    "    print(f\"微调Tokenizer的pad_token已设置为: '{tokenizer.pad_token}' (ID: {tokenizer.pad_token_id})\")\n",
    "\n",
    "def preprocess_function_causal(examples):\n",
    "    full_prompts = []\n",
    "    input_texts_for_prompt = examples[\"text\"]\n",
    "    target_outputs = examples[\"quadruples_str\"]\n",
    "\n",
    "    for input_text, target_output in zip(input_texts_for_prompt, target_outputs):\n",
    "        input_text_str = str(input_text) if input_text is not None else \"\"\n",
    "        target_output_str = str(target_output) if target_output is not None else \"\"\n",
    "        \n",
    "        prompt_part = PROMPT_TEMPLATE.format(input_text=input_text_str)\n",
    "        full_text = prompt_part + target_output_str + tokenizer.eos_token\n",
    "        full_prompts.append(full_text)\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        full_prompts,\n",
    "        max_length=MAX_INPUT_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=False, \n",
    "        return_attention_mask=True \n",
    "    )\n",
    "\n",
    "    labels = [list(ids) for ids in model_inputs[\"input_ids\"]] \n",
    "\n",
    "    for i in range(len(examples[\"text\"])):\n",
    "        # input_text_str = str(examples[\"text\"][i]) if examples[\"text\"][i] is not None else \"\"\n",
    "        # prompt_part_only = PROMPT_TEMPLATE.format(input_text=input_text_str)\n",
    "        current_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        current_labels = labels[i]\n",
    "        \n",
    "        answer_part_str = str(examples[\"quadruples_str\"][i]) if examples[\"quadruples_str\"][i] is not None else \"\"\n",
    "        # Tokenize答案部分（不加特殊token，因为它们已在full_text中处理）\n",
    "        answer_tokens = tokenizer(answer_part_str + tokenizer.eos_token, add_special_tokens=False)[\"input_ids\"]\n",
    "        \n",
    "        len_to_mask = len(current_input_ids) - len(answer_tokens)\n",
    "        \n",
    "        if len_to_mask < 0: \n",
    "            # print(f\"警告: 样本 {i} 的计算屏蔽长度为负 ({len_to_mask})。Input: '{str(examples['text'][i])[:50]}...', Target: '{answer_part_str}'\")\n",
    "            # print(f\"  Input IDs len: {len(current_input_ids)}, Answer tokens len: {len(answer_tokens)}\")\n",
    "            # 如果答案比整个序列还长（或因截断导致不匹配），则不屏蔽任何内容，或仅屏蔽BOS\n",
    "            if current_input_ids and current_input_ids[0] == tokenizer.bos_token_id:\n",
    "                 len_to_mask = 1 # 只屏蔽BOS\n",
    "            else:\n",
    "                 len_to_mask = 0 # 不屏蔽\n",
    "        \n",
    "        for j in range(min(len_to_mask, len(current_labels))): \n",
    "            current_labels[j] = -100\n",
    "        \n",
    "        if answer_part_str and all(l == -100 for l in current_labels):\n",
    "            # print(f\"警告: 样本 {i} 的所有标签都被屏蔽，但目标输出不为空 ('{answer_part_str}')。屏蔽长度: {len_to_mask}\")\n",
    "            if current_labels:\n",
    "                 current_labels[-1] = current_input_ids[-1]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfe46af28c5e475a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始对数据集进行tokenize和预处理 (适配Causal LM)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33994ee4649643f4b8bbfac089bf310f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae22d26541a4cae86d722ccce26e68a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "数据tokenize和预处理完成:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 7200\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 400\n",
      "    })\n",
      "})\n",
      "\n",
      "Tokenize后的训练集样本 (检查input_ids和labels的屏蔽情况):\n",
      "  原始/增强后输入文本 (text): 现实里可不是这样的，至少我认识的安徽人民，都是不惹事，不怕事，遇到挑衅，生死看淡不服就干😄😎\n",
      "  原始目标输出 (quadruples_str): 安徽人民 | 不惹事，不怕事 | non-hate | non-hate [END]\n",
      "\n",
      "  Tokenized input_ids (前60): [44047, 30768, 64462, 60, 1115, 37931, 39071, 56568, 101909, 104715, 104811, 116253, 43815, 101042, 110498, 3837, 102093, 100751, 99338, 101425, 26381, 115076, 52334, 117828, 109445, 102450, 1773, 14880, 100345, 20002, 103008, 108704, 3837, 102450, 90919, 102670, 117828, 109445, 57191, 65676, 117828, 9370, 85641, 33071, 109445, 90395, 101892, 87752, 68805, 66017, 46944, 57191, 101213, 63703, 23305, 40027, 28311, 85641, 64429, 320]\n",
      "  Decoded input_ids (前60): <s>[INST] <<SYS>>\n",
      "你是一个专业的中文社交媒体内容分析助手，专门用于细粒度片段级仇恨言论识别。请根据用户提供的文本，识别其中存在的仇恨言论或非仇恨的评论性言论，并按照以下格式输出一个或多个四元组：\n",
      "评论对象 (\n",
      "\n",
      "  Tokenized labels (前60, -100表示已屏蔽): [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "  Decoded labels from first non-masked token (部分): 安徽人民 | 不惹事，不怕事 | non-hate | non-hate [END]<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# --- 单元格 6: 对数据集进行Tokenize 🧩 ---\n",
    "print(\"开始对数据集进行tokenize和预处理 (适配Causal LM)...\")\n",
    "if 'raw_datasets' in locals() and raw_datasets and 'train' in raw_datasets and raw_datasets['train'] is not None:\n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "        preprocess_function_causal, \n",
    "        batched=True, \n",
    "        remove_columns=raw_datasets[\"train\"].column_names \n",
    "    )\n",
    "    print(\"\\n数据tokenize和预处理完成:\")\n",
    "    print(tokenized_datasets) \n",
    "\n",
    "    if tokenized_datasets and 'train' in tokenized_datasets and len(tokenized_datasets['train']) > 0:\n",
    "        print(f\"\\nTokenize后的训练集样本 (检查input_ids和labels的屏蔽情况):\")\n",
    "        sample_idx = 0 \n",
    "        if sample_idx < len(tokenized_datasets['train']) and sample_idx < len(raw_datasets['train']):\n",
    "            print(f\"  原始/增强后输入文本 (text): {raw_datasets['train'][sample_idx]['text']}\") \n",
    "            print(f\"  原始目标输出 (quadruples_str): {raw_datasets['train'][sample_idx]['quadruples_str']}\")\n",
    "            \n",
    "            tokenized_sample = tokenized_datasets['train'][sample_idx]\n",
    "            print(f\"\\n  Tokenized input_ids (前60): {tokenized_sample['input_ids'][:60]}\")\n",
    "            print(f\"  Decoded input_ids (前60): {tokenizer.decode(tokenized_sample['input_ids'][:60])}\")\n",
    "            \n",
    "            print(f\"\\n  Tokenized labels (前60, -100表示已屏蔽): {tokenized_sample['labels'][:60]}\")\n",
    "            \n",
    "            first_label_idx = -1\n",
    "            for idx, lbl_id in enumerate(tokenized_sample['labels']):\n",
    "                if lbl_id != -100:\n",
    "                    first_label_idx = idx\n",
    "                    break\n",
    "            \n",
    "            if first_label_idx != -1:\n",
    "                decoded_label_part = tokenizer.decode([l for l in tokenized_sample['labels'][first_label_idx:] if l != -100])\n",
    "                print(f\"  Decoded labels from first non-masked token (部分): {decoded_label_part}\")\n",
    "            else:\n",
    "                print(\"  注意：该样本的所有标签都被屏蔽了。\")\n",
    "                if raw_datasets['train'][sample_idx]['quadruples_str']: \n",
    "                     print(f\"  原始目标输出非空 ('{raw_datasets['train'][sample_idx]['quadruples_str']}'), 但所有标签被屏蔽，请仔细检查preprocess_function_causal中的屏蔽逻辑。\")\n",
    "        else:\n",
    "            print(f\"警告：选择的样本索引 {sample_idx} 超出训练集范围。\")\n",
    "    else:\n",
    "        print(\"\\n警告: Tokenize后的数据集为空或不完整。\")\n",
    "else:\n",
    "    print(\"错误: 'raw_datasets' 或其训练集未定义/为空，无法进行tokenize。请先成功执行数据加载和（可选的）增强单元格。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0a265c566257823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准备从 '/root/autodl-tmp/models/Qwen3-8B' 加载用于微调的Causal LM...\n",
      "微调模型将使用4-bit量化 (nf4) 加载。\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0aadf8863ae42a6b093ad7db3486f2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用于微调的模型 '/root/autodl-tmp/models/Qwen3-8B' 加载完成。\n",
      "微调模型配置的pad_token_id已设置为tokenizer的pad_token_id: 151643\n",
      "\n",
      "为微调模型启用LoRA。\n",
      "检测到微调模型已量化加载，准备k-bit训练...\n",
      "微调模型已为k-bit训练准备就绪。梯度检查点将启用。\n",
      "LoRA配置已创建:\n",
      "LoraConfig(task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=16, target_modules={'gate_proj', 'down_proj', 'k_proj', 'up_proj', 'v_proj', 'o_proj', 'q_proj'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)\n",
      "\n",
      "LoRA适配器已应用到微调模型。\n",
      "trainable params: 43,646,976 || all params: 8,234,382,336 || trainable%: 0.5301\n",
      "当前微调模型所在设备: cuda:0\n",
      "微调模型层设备分布: {'': 0}\n"
     ]
    }
   ],
   "source": [
    "# --- 单元格 7: 模型加载与PEFT (LoRA) 配置 🧱 ---\n",
    "# 主微调模型使用 MODEL_NAME\n",
    "print(f\"准备从 '{MODEL_NAME}' 加载用于微调的Causal LM...\")\n",
    "\n",
    "bnb_config_finetune = None # 与生成器LLM的bnb_config区分开\n",
    "if USE_QUANTIZATION: # 使用主配置中的量化设置\n",
    "    if QUANTIZATION_TYPE == \"nf4\" or QUANTIZATION_TYPE == \"fp4\":\n",
    "        bnb_config_finetune = BitsAndBytesConfig(\n",
    "            load_in_4bit=True, bnb_4bit_quant_type=QUANTIZATION_TYPE, \n",
    "            bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_use_double_quant=True, \n",
    "        )\n",
    "        print(f\"微调模型将使用4-bit量化 ({QUANTIZATION_TYPE}) 加载。\")\n",
    "    elif QUANTIZATION_TYPE == \"int8\":\n",
    "        bnb_config_finetune = BitsAndBytesConfig(load_in_8bit=True)\n",
    "        print(\"微调模型将使用8-bit量化加载。\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config_finetune,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\" \n",
    ")\n",
    "print(f\"用于微调的模型 '{MODEL_NAME}' 加载完成。\")\n",
    "\n",
    "if tokenizer.pad_token_id is not None and model.config.pad_token_id is None:\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    print(f\"微调模型配置的pad_token_id已设置为tokenizer的pad_token_id: {tokenizer.pad_token_id}\")\n",
    "\n",
    "if hasattr(model, 'config') and model.config.model_type and \"qwen2\" in model.config.model_type.lower() and hasattr(model, 'enable_input_require_grads'):\n",
    "    try:\n",
    "        model.enable_input_require_grads()\n",
    "        print(\"已为Qwen2微调模型调用 enable_input_require_grads()\")\n",
    "    except Exception as e_grad:\n",
    "        print(f\"为Qwen2微调模型调用 enable_input_require_grads() 时发生错误 (可能不需要或不适用): {e_grad}\")\n",
    "\n",
    "if USE_LORA:\n",
    "    print(\"\\n为微调模型启用LoRA。\")\n",
    "    # training_args 现在应该在Cell 9中定义，这里我们假设它会被定义\n",
    "    # 为了更安全，可以在 prepare_model_for_kbit_training 调用时直接传递布尔值\n",
    "    use_grad_ckpt_for_lora = True # 默认启用，除非在TrainingArguments中显式关闭\n",
    "    if 'training_args' in locals() and hasattr(training_args, 'gradient_checkpointing'):\n",
    "        use_grad_ckpt_for_lora = training_args.gradient_checkpointing\n",
    "\n",
    "    if hasattr(model, \"is_loaded_in_8bit\") or hasattr(model, \"is_loaded_in_4bit\") or (USE_QUANTIZATION and bnb_config_finetune is not None):\n",
    "        print(\"检测到微调模型已量化加载，准备k-bit训练...\")\n",
    "        model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=use_grad_ckpt_for_lora) \n",
    "        print(f\"微调模型已为k-bit训练准备就绪。梯度检查点将{'启用' if use_grad_ckpt_for_lora else '禁用'}。\")\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        r=LORA_R, lora_alpha=LORA_ALPHA, target_modules=LORA_TARGET_MODULES, \n",
    "        lora_dropout=LORA_DROPOUT, bias=\"none\", task_type=TaskType.CAUSAL_LM \n",
    "    )\n",
    "    print(\"LoRA配置已创建:\")\n",
    "    print(lora_config)\n",
    "    \n",
    "    model = get_peft_model(model, lora_config) \n",
    "    print(\"\\nLoRA适配器已应用到微调模型。\")\n",
    "    model.print_trainable_parameters() \n",
    "else:\n",
    "    print(\"\\n未启用LoRA，将进行全参数微调。\")\n",
    "\n",
    "print(f\"当前微调模型所在设备: {model.device}\")\n",
    "if hasattr(model, 'hf_device_map'):\n",
    "    print(f\"微调模型层设备分布: {model.hf_device_map}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e8b9e84f0c0fe1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "评估指标相关函数 (parse_quadruples, calculate_f1_metrics, compute_metrics_causal) 已定义。\n",
      "DEBUG: 已将 parse_quadruples_fn_to_use 更新为单元格8的完整定义。\n"
     ]
    }
   ],
   "source": [
    "# --- 单元格 8: 评估指标计算函数 📊 ---\n",
    "# (保持不变，但确保 parse_quadruples 在此定义或之前已定义)\n",
    "def parse_quadruples(text_str): # 确保这是完整的定义\n",
    "    quadruples = [] \n",
    "    if not isinstance(text_str, str) or not text_str.strip(): \n",
    "        return [] \n",
    "        \n",
    "    parts = text_str.split(SEP_TOKEN) \n",
    "    for part_idx, part in enumerate(parts):\n",
    "        part_cleaned = part.strip() \n",
    "        \n",
    "        if part_cleaned.endswith(END_TOKEN):\n",
    "            part_cleaned = part_cleaned[:-len(END_TOKEN)].strip() \n",
    "        elif not part_cleaned and part_idx == len(parts) -1 : \n",
    "             continue\n",
    "\n",
    "        if not part_cleaned: \n",
    "            continue\n",
    "            \n",
    "        elements = [e.strip() for e in part_cleaned.split(\" | \")] # 注意分隔符中的空格\n",
    "        \n",
    "        if len(elements) == 4:\n",
    "            quadruples.append(elements)\n",
    "    return quadruples\n",
    "\n",
    "def calculate_f1_metrics(preds_quads_list, labels_quads_list):\n",
    "    true_positives_hard = 0\n",
    "    predicted_positives_hard = 0 \n",
    "    actual_positives_hard = 0    \n",
    "    true_positives_soft = 0\n",
    "    predicted_positives_soft = 0\n",
    "    actual_positives_soft = 0\n",
    "\n",
    "    for pred_quads_for_sample, gold_quads_for_sample in zip(preds_quads_list, labels_quads_list):\n",
    "        predicted_positives_hard += len(pred_quads_for_sample)\n",
    "        actual_positives_hard += len(gold_quads_for_sample)\n",
    "        predicted_positives_soft += len(pred_quads_for_sample)\n",
    "        actual_positives_soft += len(gold_quads_for_sample)\n",
    "\n",
    "        matched_gold_indices_hard = set()\n",
    "        for p_quad in pred_quads_for_sample:\n",
    "            for i, g_quad in enumerate(gold_quads_for_sample):\n",
    "                if i in matched_gold_indices_hard: continue\n",
    "                if p_quad == g_quad: \n",
    "                    true_positives_hard += 1\n",
    "                    matched_gold_indices_hard.add(i)\n",
    "                    break \n",
    "        \n",
    "        matched_gold_indices_soft = set()\n",
    "        for p_quad in pred_quads_for_sample:\n",
    "            if len(p_quad) != 4: continue\n",
    "            for i, g_quad in enumerate(gold_quads_for_sample):\n",
    "                if len(g_quad) != 4: continue \n",
    "                if i in matched_gold_indices_soft: continue\n",
    "                if p_quad[2].strip().lower() == g_quad[2].strip().lower() and \\\n",
    "                   p_quad[3].strip().lower().startswith(g_quad[3].strip().lower().split(\" \")[0]): # 比较主要部分\n",
    "                    sim_target = difflib.SequenceMatcher(None, p_quad[0], g_quad[0]).ratio()\n",
    "                    sim_argument = difflib.SequenceMatcher(None, p_quad[1], g_quad[1]).ratio()\n",
    "                    if sim_target > 0.5 and sim_argument > 0.5: \n",
    "                        true_positives_soft += 1\n",
    "                        matched_gold_indices_soft.add(i)\n",
    "                        break \n",
    "\n",
    "    precision_hard = true_positives_hard / predicted_positives_hard if predicted_positives_hard > 0 else 0\n",
    "    recall_hard = true_positives_hard / actual_positives_hard if actual_positives_hard > 0 else 0\n",
    "    f1_hard = 2 * (precision_hard * recall_hard) / (precision_hard + recall_hard) if (precision_hard + recall_hard) > 0 else 0\n",
    "    precision_soft = true_positives_soft / predicted_positives_soft if predicted_positives_soft > 0 else 0\n",
    "    recall_soft = true_positives_soft / actual_positives_soft if actual_positives_soft > 0 else 0\n",
    "    f1_soft = 2 * (precision_soft * recall_soft) / (precision_soft + recall_soft) if (precision_soft + recall_soft) > 0 else 0\n",
    "    avg_f1 = (f1_hard + f1_soft) / 2\n",
    "    return {\n",
    "        \"f1_hard\": f1_hard, \"precision_hard\": precision_hard, \"recall_hard\": recall_hard,\n",
    "        \"f1_soft\": f1_soft, \"precision_soft\": precision_soft, \"recall_soft\": recall_soft,\n",
    "        \"avg_f1\": avg_f1\n",
    "    }\n",
    "\n",
    "def compute_metrics_causal(eval_preds):\n",
    "    generated_token_ids, label_ids_from_input = eval_preds \n",
    "    # label_ids_from_input 包含了 -100 用于屏蔽提示部分\n",
    "    # generated_token_ids 是模型生成的序列，可能也包含提示部分（如果generate未正确配置只输出新token）\n",
    "    # 通常，Trainer的generate会处理好，只返回新生成的tokens，或者我们需要从完整序列中提取\n",
    "\n",
    "    # 假设 generated_token_ids 是模型新生成的token (不含提示)\n",
    "    # 如果它包含了提示，我们需要从模型输出中移除提示部分\n",
    "    # decoded_preds_str = tokenizer.batch_decode(generated_token_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    \n",
    "    # 一个更鲁棒的方法是，我们知道label_ids_from_input是完整的，包含-100\n",
    "    # 而 generated_token_ids 是模型针对这些输入生成的完整序列（提示+答案）\n",
    "    # 我们需要从 generated_token_ids 中提取答案部分，或者从 decoded_preds_str 中提取\n",
    "\n",
    "    # 方案1: 假设 generated_token_ids 是完整的（提示+答案）\n",
    "    decoded_preds_full_str = tokenizer.batch_decode(generated_token_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    \n",
    "    # 从解码的完整预测中提取真实的答案部分\n",
    "    pred_answer_strs = []\n",
    "    keyword_separator = \"模型输出：\" # 与PROMPT_TEMPLATE一致\n",
    "    for full_pred_text in decoded_preds_full_str:\n",
    "        if keyword_separator in full_pred_text:\n",
    "            pred_answer_strs.append(full_pred_text.split(keyword_separator, 1)[-1].strip())\n",
    "        else: # 如果模型没按套路出牌\n",
    "            # print(f\"警告: 预测结果中未找到分隔符 '{keyword_separator}'. Full pred: '{full_pred_text[:100]}...'\")\n",
    "            # 尝试移除已知的提示头（这比较脆弱）\n",
    "            # prompt_head_approx = PROMPT_TEMPLATE.split(\"{input_text}\")[0].split(\"<<SYS>>\")[-1].strip() # 取系统提示之后的部分\n",
    "            # if full_pred_text.startswith(prompt_head_approx):\n",
    "            #     pred_answer_strs.append(full_pred_text[len(prompt_head_approx):].strip())\n",
    "            # else:\n",
    "            pred_answer_strs.append(full_pred_text) # 后备：使用全部，可能包含提示\n",
    "\n",
    "    # 解码真实标签（答案部分）\n",
    "    processed_label_ids = np.where(label_ids_from_input != -100, label_ids_from_input, tokenizer.pad_token_id)\n",
    "    decoded_labels_full_str = tokenizer.batch_decode(processed_label_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    actual_target_strs = []\n",
    "    for full_label_text in decoded_labels_full_str:\n",
    "        if keyword_separator in full_label_text:\n",
    "            actual_target_strs.append(full_label_text.split(keyword_separator, 1)[-1].strip())\n",
    "        else:\n",
    "            # print(f\"警告: 解码的标签中未找到分隔符 '{keyword_separator}'. Full label: '{full_label_text[:100]}...'\")\n",
    "            actual_target_strs.append(\"\") \n",
    "\n",
    "    pred_quads_list = [parse_quadruples(p_str) for p_str in pred_answer_strs]\n",
    "    label_quads_list = [parse_quadruples(l_str) for l_str in actual_target_strs] \n",
    "    \n",
    "    results = calculate_f1_metrics(pred_quads_list, label_quads_list)\n",
    "    return results\n",
    "\n",
    "print(\"评估指标相关函数 (parse_quadruples, calculate_f1_metrics, compute_metrics_causal) 已定义。\")\n",
    "# 更新 parse_quadruples_fn_to_use (如果单元格3.6在单元格8之前运行了)\n",
    "if 'parse_quadruples_fn_to_use' in globals() and parse_quadruples_fn_to_use.__name__ == 'parse_quadruples_placeholder':\n",
    "    parse_quadruples_fn_to_use = parse_quadruples\n",
    "    print(\"DEBUG: 已将 parse_quadruples_fn_to_use 更新为单元格8的完整定义。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28b1e756d2e5eb8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: 最终确认的 CALCULATED_STEPS_PER_EPOCH: 500\n",
      "训练参数 (TrainingArguments) 配置完成。评估和保存策略均设置为 'steps'，每 500 步执行一次。\n",
      "数据整理器 (DataCollatorForSeq2Seq) 初始化完成。\n"
     ]
    }
   ],
   "source": [
    "# --- 单元格 9: 训练参数配置与数据整理器 (最终确认版) 📋 ---\n",
    "# (保持与您确认可用的版本一致)\n",
    "if 'CALCULATED_STEPS_PER_EPOCH' not in locals(): CALCULATED_STEPS_PER_EPOCH = 500 # 安全默认值\n",
    "print(f\"DEBUG: 最终确认的 CALCULATED_STEPS_PER_EPOCH: {CALCULATED_STEPS_PER_EPOCH}\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    do_eval=True,                 \n",
    "    eval_strategy=\"steps\",        \n",
    "    eval_steps=CALCULATED_STEPS_PER_EPOCH, \n",
    "    save_strategy=\"steps\",        \n",
    "    save_steps=CALCULATED_STEPS_PER_EPOCH, \n",
    "    save_total_limit=2, \n",
    "    logging_dir=f\"{OUTPUT_DIR}/logs\", \n",
    "    logging_strategy=\"steps\", \n",
    "    logging_steps=max(1, CALCULATED_STEPS_PER_EPOCH // 10 if CALCULATED_STEPS_PER_EPOCH > 10 else 50),\n",
    "    load_best_model_at_end=True, \n",
    "    metric_for_best_model=\"avg_f1\", \n",
    "    greater_is_better=True,      \n",
    "    fp16=(torch.cuda.is_available() and not USE_QUANTIZATION), # fp16 与 4-bit/8-bit 量化通常不一起用\n",
    "    bf16=(torch.cuda.is_bf16_supported() and not USE_QUANTIZATION), # bf16 同上\n",
    "    lr_scheduler_type=LR_SCHEDULER_TYPE, \n",
    "    warmup_ratio=WARMUP_RATIO,           \n",
    "    report_to=[\"tensorboard\"], \n",
    "    seed=SEED,                 \n",
    "    optim=\"paged_adamw_8bit\" if USE_QUANTIZATION else \"adamw_torch\",\n",
    "    remove_unused_columns=True, # 推荐设置为True\n",
    "    gradient_checkpointing=True, # 为节省显存启用\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False}, # 推荐的梯度检查点设置\n",
    ")\n",
    "print(f\"训练参数 (TrainingArguments) 配置完成。评估和保存策略均设置为 'steps'，每 {CALCULATED_STEPS_PER_EPOCH} 步执行一次。\")\n",
    "\n",
    "if 'training_args' in locals() and training_args is not None:\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer, model=model, label_pad_token_id=-100, \n",
    "        pad_to_multiple_of=8 if (training_args.fp16 or training_args.bf16) else None \n",
    "    )\n",
    "    print(\"数据整理器 (DataCollatorForSeq2Seq) 初始化完成。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecd26deb3e217169",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1325/2158355083.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer 初始化完成。\n"
     ]
    }
   ],
   "source": [
    "# --- 单元格 10: 初始化 Trainer 👨‍🏫 ---\n",
    "# (保持不变)\n",
    "trainer = Trainer(\n",
    "    model=model, args=training_args,                  \n",
    "    train_dataset=tokenized_datasets[\"train\"] if 'tokenized_datasets' in locals() and tokenized_datasets and \"train\" in tokenized_datasets else None, \n",
    "    eval_dataset=tokenized_datasets[\"validation\"] if 'tokenized_datasets' in locals() and tokenized_datasets and \"validation\" in tokenized_datasets else None, \n",
    "    tokenizer=tokenizer, data_collator=data_collator, compute_metrics=compute_metrics_causal, \n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.001)] \n",
    ")\n",
    "print(\"Trainer 初始化完成。\")\n",
    "if not ('tokenized_datasets' in locals() and tokenized_datasets and \"train\" in tokenized_datasets and tokenized_datasets[\"train\"]):\n",
    "    print(\"警告: Trainer的训练集未正确设置。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a620759a4f617c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "即将开始模型训练...\n",
      "模型评估时将使用以下生成配置: num_beams=3, max_new_tokens=256\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='501' max='1350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 501/1350 3:02:47 < 5:11:00, 0.05 it/s, Epoch 1.11/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  7/200 00:04 < 02:21, 1.36 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "模型训练过程中发生严重错误: CUDA out of memory. Tried to allocate 6.21 GiB. GPU 0 has a total capacity of 23.55 GiB of which 4.89 GiB is free. Including non-PyTorch memory, this process has 0 bytes memory in use. Of the allocated memory 14.41 GiB is allocated by PyTorch, and 3.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1325/2744667030.py\", line 21, in <module>\n",
      "    train_result = trainer.train()\n",
      "                   ^^^^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/transformers/trainer.py\", line 2240, in train\n",
      "    return inner_training_loop(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/transformers/trainer.py\", line 2622, in _inner_training_loop\n",
      "    self._maybe_log_save_evaluate(\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/transformers/trainer.py\", line 3095, in _maybe_log_save_evaluate\n",
      "    metrics = self._evaluate(trial, ignore_keys_for_eval)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/transformers/trainer.py\", line 3044, in _evaluate\n",
      "    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/transformers/trainer.py\", line 4173, in evaluate\n",
      "    output = eval_loop(\n",
      "             ^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/transformers/trainer.py\", line 4395, in evaluation_loop\n",
      "    all_preds.add(logits)\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/transformers/trainer_pt_utils.py\", line 316, in add\n",
      "    self.tensors = nested_concat(self.tensors, tensors, padding_index=self.padding_index)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/transformers/trainer_pt_utils.py\", line 130, in nested_concat\n",
      "    return torch_pad_and_concatenate(tensors, new_tensors, padding_index=padding_index)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/transformers/trainer_pt_utils.py\", line 94, in torch_pad_and_concatenate\n",
      "    result = tensor1.new_full(new_shape, padding_index)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.21 GiB. GPU 0 has a total capacity of 23.55 GiB of which 4.89 GiB is free. Including non-PyTorch memory, this process has 0 bytes memory in use. Of the allocated memory 14.41 GiB is allocated by PyTorch, and 3.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "# --- 单元格 11: 开始模型训练 🚀 ---\n",
    "# (保持不变)\n",
    "print(\"即将开始模型训练...\")\n",
    "if trainer.train_dataset is None:\n",
    "    print(\"错误: 训练数据集未设置，无法开始训练。\")\n",
    "else:\n",
    "    try:\n",
    "        if model.generation_config is None: \n",
    "            model.generation_config = GenerationConfig.from_model_config(model.config)\n",
    "            print(\"已为模型设置默认的GenerationConfig。\")\n",
    "        \n",
    "        model.generation_config.max_new_tokens = MAX_TARGET_LENGTH \n",
    "        model.generation_config.num_beams = 3 \n",
    "        model.generation_config.early_stopping = True\n",
    "        if tokenizer.pad_token_id is not None:\n",
    "            model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "        if tokenizer.eos_token_id is not None: # 确保eos_token_id也设置\n",
    "             model.generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "        print(f\"模型评估时将使用以下生成配置: num_beams={model.generation_config.num_beams}, max_new_tokens={model.generation_config.max_new_tokens}\")\n",
    "        train_result = trainer.train()\n",
    "        print(\"\\n模型训练完成!\")\n",
    "        print(\"正在保存模型 (LoRA adapter)...\")\n",
    "        trainer.save_model(OUTPUT_DIR) \n",
    "        tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "        print(f\"模型适配器和tokenizer已保存到 '{OUTPUT_DIR}'。\")\n",
    "        metrics = train_result.metrics\n",
    "        trainer.log_metrics(\"train\", metrics) \n",
    "        trainer.save_metrics(\"train\", metrics) \n",
    "        trainer.save_state() \n",
    "        print(\"\\n训练指标已记录和保存。\")\n",
    "        print(f\"训练统计指标: {metrics}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n模型训练过程中发生严重错误: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cec30d2c052b7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用于预测的模型已准备好，当前设备: cuda:0\n",
      "预测/推理相关函数 (predict_quadruples_causal) 已定义。\n"
     ]
    }
   ],
   "source": [
    "# --- 单元格 12: 预测/推理函数设置 🔮 ---\n",
    "# (保持不变，但确保 MAX_INPUT_LENGTH 和 MAX_TARGET_LENGTH 从Cell 2正确传递)\n",
    "model_to_predict = trainer.model if 'trainer' in locals() and hasattr(trainer, 'model') else None\n",
    "if model_to_predict:\n",
    "    model_to_predict.eval() \n",
    "    print(f\"用于预测的模型已准备好，当前设备: {model_to_predict.device}\")\n",
    "else:\n",
    "    print(\"警告: 'trainer.model' 未找到，无法设置 model_to_predict。示例预测和提交文件生成可能失败。\")\n",
    "\n",
    "\n",
    "def predict_quadruples_causal(text_list, model, tokenizer_pred, max_input_len_pred, max_target_gen_len_pred):\n",
    "    parsed_results_list = []\n",
    "    if model is None or tokenizer_pred is None:\n",
    "        print(\"错误: 预测所需的模型或tokenizer未提供。\")\n",
    "        return [{\"original_text\": t, \"extracted_answer_string\": \"ERROR: Model/Tokenizer missing\", \"parsed_quadruples\": []} for t in text_list]\n",
    "\n",
    "    for text_input in text_list:\n",
    "        prompt_for_inference = PROMPT_TEMPLATE.format(input_text=text_input)\n",
    "        \n",
    "        # 确保 max_length 对于提示是合理的\n",
    "        max_prompt_len = max_input_len_pred - max_target_gen_len_pred\n",
    "        if max_prompt_len <= 0 : max_prompt_len = max_input_len_pred // 2 # 至少给提示一半空间\n",
    "\n",
    "        inputs = tokenizer_pred(\n",
    "            prompt_for_inference, return_tensors=\"pt\", truncation=True, \n",
    "            max_length=max_prompt_len, padding=False \n",
    "        ).to(model.device) \n",
    "\n",
    "        with torch.no_grad():\n",
    "            # 复制并更新生成配置，以防修改全局配置\n",
    "            current_gen_config = GenerationConfig(**model.generation_config.to_dict())\n",
    "            current_gen_config.max_new_tokens = max_target_gen_len_pred\n",
    "            # num_beams, early_stopping 等应已在 model.generation_config 中设置\n",
    "\n",
    "            outputs = model.generate(**inputs, generation_config=current_gen_config)\n",
    "        \n",
    "        full_generated_text = tokenizer_pred.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        \n",
    "        answer_part_str = \"\"\n",
    "        keyword_separator = \"模型输出：\" \n",
    "        split_parts = full_generated_text.split(keyword_separator, 1)\n",
    "        if len(split_parts) > 1:\n",
    "            answer_part_str = split_parts[1].strip()\n",
    "        else: \n",
    "            prompt_head_for_removal = PROMPT_TEMPLATE.format(input_text=text_input).split(keyword_separator)[0] + keyword_separator\n",
    "            decoded_prompt_head = tokenizer_pred.decode(tokenizer_pred.encode(prompt_head_for_removal.split(\"用户提供的文本如下：\")[0], add_special_tokens=False), skip_special_tokens=True) # 尝试解码提示头部分\n",
    "            \n",
    "            # 这是一个更复杂的尝试，试图移除提示部分\n",
    "            # 如果解码后的完整文本以解码后的提示头开始，则移除它\n",
    "            # 这非常依赖于解码的一致性\n",
    "            # if full_generated_text.startswith(decoded_prompt_head.strip()):\n",
    "            #    answer_part_str = full_generated_text[len(decoded_prompt_head.strip()):].strip()\n",
    "            #    if answer_part_str.startswith(keyword_separator): # 再次检查\n",
    "            #         answer_part_str = answer_part_str[len(keyword_separator):].strip()\n",
    "            # else:\n",
    "            # print(f\"警告：无法在生成文本中通过关键词 '{keyword_separator}' 定位答案，且提示头不匹配。Full: '{full_generated_text[:200]}...'\")\n",
    "            answer_part_str = full_generated_text # 作为最后的手段，可能包含提示\n",
    "\n",
    "        parsed_quads = parse_quadruples(answer_part_str) # 使用Cell 8定义的parse_quadruples\n",
    "        parsed_results_list.append({\n",
    "            \"original_text\": text_input, \"full_generated_text\": full_generated_text, \n",
    "            \"extracted_answer_string\": answer_part_str, \"parsed_quadruples\": parsed_quads\n",
    "        })\n",
    "    return parsed_results_list\n",
    "\n",
    "print(\"预测/推理相关函数 (predict_quadruples_causal) 已定义。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5dd17ab5c4cc693f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "开始运行示例预测...\n",
      "\n",
      "示例预测结果:\n",
      "原始文本: 那些同性恋真恶心，败坏社会风气。\n",
      "提取答案: 同性恋 | 真恶心，败坏社会风气 | LGBTQ | hate [END]\n",
      "解析四元组: [['同性恋', '真恶心，败坏社会风气', 'LGBTQ', 'hate']]\n",
      "------------------------------\n",
      "原始文本: 这道菜味道不错，下次还来。\n",
      "提取答案: 这道菜 | 味道不错 | non-hate | non-hate [END]\n",
      "解析四元组: [['这道菜', '味道不错', 'non-hate', 'non-hate']]\n",
      "------------------------------\n",
      "原始文本: 上海人就是排外，看不起外地人。\n",
      "提取答案: 上海人 | 排外，看不起外地人 | Region | hate [END]\n",
      "解析四元组: [['上海人', '排外，看不起外地人', 'Region', 'hate']]\n",
      "------------------------------\n",
      "原始文本: 黑人都是罪犯，应该被赶走。\n",
      "提取答案: 黑人 | 罪犯 | Racism | hate [END]\n",
      "解析四元组: [['黑人', '罪犯', 'Racism', 'hate']]\n",
      "------------------------------\n",
      "原始文本: 你可真是头蠢驴，这都做不好。\n",
      "提取答案: 你 | 头蠢驴 | non-hate | non-hate [END]\n",
      "解析四元组: [['你', '头蠢驴', 'non-hate', 'non-hate']]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# (保持不变)\n",
    "sample_test_texts_for_prediction = [\n",
    "    \"那些同性恋真恶心，败坏社会风气。\", \"这道菜味道不错，下次还来。\",\n",
    "    \"上海人就是排外，看不起外地人。\", \"黑人都是罪犯，应该被赶走。\",\n",
    "    \"你可真是头蠢驴，这都做不好。\"\n",
    "]\n",
    "print(\"\\n开始运行示例预测...\")\n",
    "if 'model_to_predict' in locals() and model_to_predict is not None:\n",
    "    predictions = predict_quadruples_causal(\n",
    "        sample_test_texts_for_prediction, model_to_predict, tokenizer,\n",
    "        MAX_INPUT_LENGTH, MAX_TARGET_LENGTH \n",
    "    )\n",
    "    print(\"\\n示例预测结果:\")\n",
    "    for item in predictions:\n",
    "        print(f\"原始文本: {item['original_text']}\")\n",
    "        print(f\"提取答案: {item['extracted_answer_string']}\")\n",
    "        print(f\"解析四元组: {item['parsed_quadruples']}\")\n",
    "        print(\"-\" * 30)\n",
    "else:\n",
    "    print(\"错误: 'model_to_predict' 未定义或为None。无法运行示例预测。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51016ed5040cd5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "开始处理官方测试文件: ./test1.json\n",
      "正在从 './test1.json' 加载官方测试数据...\n",
      "成功从 './test1.json' 加载了 2000 条测试数据。\n",
      "开始对 2000 条测试数据进行预测 (批次大小: 3)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "官方测试集预测: 100%|██████████| 667/667 [2:18:28<00:00, 12.46s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "提交文件已成功生成: ./newsubmission.txt\n",
      "该文件包含 2000 行预测。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 单元格 14: 加载官方测试数据并生成提交文件 📤 ---\n",
    "# (保持不变，但确保路径和变量名正确)\n",
    "# official_test_file_path = \"/kaggle/input/nlptrain/test1.json\" # 您的测试文件路径\n",
    "# official_test_file_path = \"./test1.json\" # 假设在当前目录\n",
    "# official_test_file_path = \"./test2.json\" # 或者 test2.json\n",
    "\n",
    "# 确保以下变量已定义：\n",
    "# official_test_file_path, model_to_predict, tokenizer, EVAL_BATCH_SIZE, MAX_INPUT_LENGTH, MAX_TARGET_LENGTH\n",
    "\n",
    "# 示例路径，请根据您的实际情况修改\n",
    "# official_test_file_path_to_use = \"./test1.json\" \n",
    "import json # 确保导入json库\n",
    "import os   # 确保导入os库\n",
    "\n",
    "# --- 如何加载官方测试数据并生成提交文件的示例 ---\n",
    "\n",
    "def load_official_test_data(file_path):\n",
    "    \"\"\"\n",
    "    加载官方测试数据。\n",
    "    假设文件是一个JSON，其顶级结构是一个列表，列表中的每个元素是一个包含 \"id\" 和 \"content\" 键的字典。\n",
    "    \n",
    "    参数:\n",
    "    - file_path (str): 测试数据JSON文件的路径。\n",
    "    \n",
    "    返回:\n",
    "    - list: 包含所有 \"content\" 字符串的列表。\n",
    "    - list: 包含所有对应 \"id\" 的列表 (可选, 如果需要id进行映射或调试)。\n",
    "    \"\"\"\n",
    "    texts_to_predict = []\n",
    "    ids_from_test_data = [] # 可选，用于追踪ID\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"错误: 测试文件 '{file_path}' 未找到。\")\n",
    "        return texts_to_predict, ids_from_test_data # 返回空列表\n",
    "\n",
    "    print(f\"正在从 '{file_path}' 加载官方测试数据...\")\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f) # 整个文件是一个JSON列表\n",
    "            if not isinstance(data, list):\n",
    "                print(f\"错误: 测试文件 '{file_path}' 的顶级结构不是一个列表。请检查文件格式。\")\n",
    "                return texts_to_predict, ids_from_test_data\n",
    "\n",
    "            for item_num, item in enumerate(data, 1):\n",
    "                if isinstance(item, dict) and \"content\" in item and \"id\" in item:\n",
    "                    texts_to_predict.append(item[\"content\"])\n",
    "                    ids_from_test_data.append(item[\"id\"])\n",
    "                else:\n",
    "                    print(f\"警告: 测试文件 '{file_path}' 中的第 {item_num} 项格式不正确或缺少 'id'/'content' 键，已跳过: {item}\")\n",
    "        \n",
    "        print(f\"成功从 '{file_path}' 加载了 {len(texts_to_predict)} 条测试数据。\")\n",
    "\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"错误: 解析测试文件 '{file_path}' 时发生JSON解码错误。请检查文件是否为有效的JSON格式。\")\n",
    "    except Exception as e:\n",
    "        print(f\"加载测试文件 '{file_path}' 时发生其他错误: {e}\")\n",
    "        \n",
    "    return texts_to_predict, ids_from_test_data\n",
    "\n",
    "official_test_file_path_to_use = \"./test1.json\" # 或您的 test2.json 路径\n",
    "\n",
    "if 'model_to_predict' not in locals() or model_to_predict is None:\n",
    "    print(\"错误: 'model_to_predict' 未定义。无法进行官方测试数据预测。\")\n",
    "elif 'tokenizer' not in locals() or tokenizer is None:\n",
    "    print(\"错误: 'tokenizer' 未定义。无法进行官方测试数据预测。\")\n",
    "elif not os.path.exists(official_test_file_path_to_use):\n",
    "    print(f\"错误: 测试文件路径 '{official_test_file_path_to_use}' 不存在。\")\n",
    "else:\n",
    "    print(f\"\\n开始处理官方测试文件: {official_test_file_path_to_use}\")\n",
    "    official_test_texts, official_test_ids = load_official_test_data(official_test_file_path_to_use) # load_official_test_data 在您之前的代码中定义\n",
    "    \n",
    "    if official_test_texts:\n",
    "        submission_outputs_strings = []\n",
    "        inference_batch_size = EVAL_BATCH_SIZE \n",
    "        print(f\"开始对 {len(official_test_texts)} 条测试数据进行预测 (批次大小: {inference_batch_size})...\")\n",
    "        for i in tqdm(range(0, len(official_test_texts), inference_batch_size), desc=\"官方测试集预测\"):\n",
    "            batch_texts = official_test_texts[i : i + inference_batch_size]\n",
    "            batch_predictions = predict_quadruples_causal(\n",
    "                batch_texts, model_to_predict, tokenizer,\n",
    "                MAX_INPUT_LENGTH, MAX_TARGET_LENGTH\n",
    "            )\n",
    "            for item_prediction in batch_predictions:\n",
    "                submission_outputs_strings.append(item_prediction['extracted_answer_string'])\n",
    "        \n",
    "        #submission_file_path = \"/kaggle/working/submission.txt\" # Kaggle 工作目录\n",
    "        submission_file_path = \"./newsubmission.txt\" # 或者本地路径\n",
    "        try:\n",
    "            with open(submission_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                for line_content in submission_outputs_strings:\n",
    "                    f.write(line_content + \"\\n\")\n",
    "            print(f\"\\n提交文件已成功生成: {submission_file_path}\")\n",
    "            print(f\"该文件包含 {len(submission_outputs_strings)} 行预测。\")\n",
    "        except Exception as e:\n",
    "            print(f\"写入提交文件 '{submission_file_path}' 时发生错误: {e}\")\n",
    "    else:\n",
    "        print(f\"未能从 '{official_test_file_path_to_use}' 加载任何测试数据进行预测。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc782e77-9281-4d18-9add-98bde4227c21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
