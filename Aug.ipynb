{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aef1d2476f82cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bitsandbytes ç‰ˆæœ¬: 0.46.0 å·²æˆåŠŸå¯¼å…¥ã€‚\n",
      "å½“å‰ä½¿ç”¨çš„è®¾å¤‡: cuda\n",
      "GPUåç§°: NVIDIA GeForce RTX 3090\n",
      "å·²è®¾ç½® PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n"
     ]
    }
   ],
   "source": [
    "# --- å•å…ƒæ ¼ 1: ç¯å¢ƒè®¾ç½®å’Œåº“å¯¼å…¥ ğŸ› ï¸ ---\n",
    "\n",
    "\n",
    "# åœ¨Notebookå†…éƒ¨éªŒè¯å’Œæç¤ºbitsandbytesçš„å®‰è£…\n",
    "try:\n",
    "    import bitsandbytes as bnb\n",
    "    print(f\"bitsandbytes ç‰ˆæœ¬: {bnb.__version__} å·²æˆåŠŸå¯¼å…¥ã€‚\")\n",
    "except ImportError:\n",
    "    print(\"é”™è¯¯: bitsandbytes æœªå®‰è£…æˆ–å¯¼å…¥å¤±è´¥ã€‚\")\n",
    "    print(\"è¯·å°è¯•åœ¨æ–°çš„å•å…ƒæ ¼ä¸­è¿è¡Œ: !pip install -U bitsandbytes\")\n",
    "    print(\"æˆ–è€…ï¼Œå¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯ç‰¹å®šCUDAç‰ˆæœ¬ï¼Œå¯èƒ½éœ€è¦æŸ¥æ‰¾ç‰¹å®šçš„bitsandbyteså®‰è£…å‘½ä»¤ã€‚\")\n",
    "    print(\"å®‰è£…ååŠ¡å¿…é‡å¯Jupyter Kernelï¼\")\n",
    "    raise\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import difflib\n",
    "import gc \n",
    "from tqdm import tqdm \n",
    "\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq, \n",
    "    EarlyStoppingCallback,\n",
    "    BitsAndBytesConfig,\n",
    "    GenerationConfig\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# æ£€æŸ¥å¯ç”¨GPU\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"å½“å‰ä½¿ç”¨çš„è®¾å¤‡: {DEVICE}\")\n",
    "if DEVICE.type == 'cuda':\n",
    "    print(f\"GPUåç§°: {torch.cuda.get_device_name(0)}\")\n",
    "    # è®¾ç½® PYTORCH_CUDA_ALLOC_CONF æ¥å‡å°‘æ˜¾å­˜ç¢ç‰‡\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "    print(\"å·²è®¾ç½® PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e6f45414641998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- å•å…ƒæ ¼ 2: é…ç½®å‚æ•° âš™ï¸ ---\n",
    "MODEL_NAME = \"/root/autodl-tmp/models/Qwen3-1.7B\" \n",
    "# LoRA é…ç½®\n",
    "USE_LORA = True \n",
    "LORA_R = 16 \n",
    "LORA_ALPHA = 32 \n",
    "LORA_DROPOUT = 0.05 \n",
    "LORA_TARGET_MODULES = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "\n",
    "# é‡åŒ–é…ç½® \n",
    "USE_QUANTIZATION = True \n",
    "QUANTIZATION_TYPE = \"nf4\" \n",
    "\n",
    "# è®­ç»ƒç›¸å…³å‚æ•° (å› ä¸ºç»å¸¸OOMæ‰€ä»¥è¿›è¡Œä¿å®ˆè®¾ç½®)\n",
    "OUTPUT_DIR = \"/root/autodl-tmp/qwen_hate_speech_finetuned_llm_aug\" # è¾“å‡ºç›®å½•å\n",
    "TRAIN_FILE_PATH = \"./train_formatted_for_llm.jsonl\" \n",
    "\n",
    "TRAIN_BATCH_SIZE = 2 # éå¸¸å°çš„æ‰¹æ¬¡å¤§å°ä»¥é¿å…OOM\n",
    "EVAL_BATCH_SIZE = 3  \n",
    "NUM_TRAIN_EPOCHS = 3 \n",
    "LEARNING_RATE = 2e-4 \n",
    "WEIGHT_DECAY = 0.01  \n",
    "MAX_INPUT_LENGTH = 1024 # \n",
    "MAX_TARGET_LENGTH = 256 # ç”Ÿæˆç›®æ ‡ï¼ˆå››å…ƒç»„å­—ç¬¦ä¸²ï¼‰çš„æœ€å¤§tokené•¿åº¦\n",
    "GRADIENT_ACCUMULATION_STEPS = 8 # å¢å¤§æ¢¯åº¦ç´¯ç§¯ä»¥è¡¥å¿å°æ‰¹æ¬¡å¤§å°\n",
    "WARMUP_RATIO = 0.03 \n",
    "LR_SCHEDULER_TYPE = \"cosine\" \n",
    "\n",
    "SEED = 42 \n",
    "\n",
    "# ç‰¹æ®Šæ ‡è®°å®šä¹‰\n",
    "END_TOKEN = \"[END]\" \n",
    "SEP_TOKEN = \"[SEP]\" \n",
    "TARGET_GROUPS = [\"Region\", \"Racism\", \"Sexism\", \"LGBTQ\", \"others\", \"non-hate\"] \n",
    "HATEFUL_STATUS = [\"hate\", \"non-hate\"]\n",
    "\n",
    "# å®šä¹‰æç¤ºæ¨¡æ¿ç»“æ„ \n",
    "PROMPT_TEMPLATE = \"\"\"<s>[INST] <<SYS>>\n",
    "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¸­æ–‡ç¤¾äº¤åª’ä½“å†…å®¹åˆ†æåŠ©æ‰‹ï¼Œä¸“é—¨ç”¨äºç»†ç²’åº¦ç‰‡æ®µçº§ä»‡æ¨è¨€è®ºè¯†åˆ«ã€‚è¯·æ ¹æ®ç”¨æˆ·æä¾›çš„æ–‡æœ¬ï¼Œè¯†åˆ«å…¶ä¸­å­˜åœ¨çš„ä»‡æ¨è¨€è®ºæˆ–éä»‡æ¨çš„è¯„è®ºæ€§è¨€è®ºï¼Œå¹¶æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºä¸€ä¸ªæˆ–å¤šä¸ªå››å…ƒç»„ï¼š\n",
    "è¯„è®ºå¯¹è±¡ (Target) | è®ºç‚¹ (Argument) | ç›®æ ‡ç¾¤ä½“ (Targeted Group) | æ˜¯å¦ä»‡æ¨ (Hateful) [END]\n",
    "è¯¦ç»†è¯´æ˜ï¼š\n",
    "1.  **è¯„è®ºå¯¹è±¡ (Target)ï¼š** å¸–å­ä¸­è¢«è¯„è®ºæˆ–æåŠçš„å…·ä½“äººç‰©ã€ç¾¤ä½“ã€äº‹ç‰©æˆ–æ¦‚å¿µã€‚å¦‚æœæ˜¯é’ˆå¯¹æ–‡æœ¬ä¸­éšå«çš„ã€æ²¡æœ‰æ˜ç¡®æŒ‡å‡ºçš„å¯¹è±¡ï¼Œæˆ–è€…è¯„è®ºæ˜¯æ³›æŒ‡ï¼Œåˆ™è®¾ä¸º \"NULL\"ã€‚\n",
    "2.  **è®ºç‚¹ (Argument)ï¼š** é’ˆå¯¹â€œè¯„è®ºå¯¹è±¡â€æ‰€å‘è¡¨çš„æ ¸å¿ƒè§‚ç‚¹ã€æè¿°æˆ–è¡Œä¸ºï¼Œåº”ä¸ºæ–‡æœ¬ä¸­çš„ä¸€ä¸ªå…³é”®ä¿¡æ¯ç‰‡æ®µã€‚\n",
    "3.  **ç›®æ ‡ç¾¤ä½“ (Targeted Group)ï¼š** æŒ‡è¯¥â€œè¯„è®ºå¯¹è±¡-è®ºç‚¹â€æ‰€æ¶‰åŠæˆ–æŒ‡å‘çš„ç¤¾ä¼šç¾¤ä½“ã€‚å…¶ä¸­ï¼Œç›®æ ‡ç¾¤ä½“å¯ä»¥æœ‰å¤šé¡¹ï¼Œä½†å¿…é¡»ä»ä»¥ä¸‹é¢„è®¾ç±»åˆ«ä¸­é€‰æ‹©ï¼š\n",
    "    * `Region`ï¼šé’ˆå¯¹ç‰¹å®šåœ°åŸŸï¼ˆå›½å®¶ã€çœä»½ã€åŸå¸‚ç­‰ï¼‰äººç¾¤çš„è¯„è®ºã€‚\n",
    "    * `Racism`ï¼šé’ˆå¯¹ç‰¹å®šç§æ—æˆ–æ°‘æ—äººç¾¤çš„è¯„è®ºã€‚\n",
    "    * `Sexism`ï¼šé’ˆå¯¹ç‰¹å®šæ€§åˆ«äººç¾¤ï¼ˆç”·æ€§ã€å¥³æ€§ï¼‰çš„è¯„è®ºï¼Œæˆ–æ€§åˆ«æ­§è§†ã€åˆ»æ¿å°è±¡ã€‚\n",
    "    * `LGBTQ`ï¼šé’ˆå¯¹æ€§å°‘æ•°ç¾¤ä½“çš„è¯„è®ºï¼ˆå¦‚åŒæ€§æ‹ã€è·¨æ€§åˆ«ç­‰ï¼‰ã€‚\n",
    "    * `others`ï¼šé’ˆå¯¹ä¸Šè¿°å››ç±»ä¹‹å¤–çš„ç‰¹å®šç¾¤ä½“ï¼ˆå¦‚ç‰¹å®šèŒä¸šã€ç–¾ç—…ç¾¤ä½“ã€æ”¿æ²»ç«‹åœºç¾¤ä½“ç­‰ï¼‰æˆ–ä¸æ„æˆå¯¹ç‰¹å®šç¤¾ä¼šç¾¤ä½“çš„æ”»å‡»ï¼Œè€Œæ˜¯ä¸ªäººæ”»å‡»ã€è§‚ç‚¹è¯„è®ºç­‰ã€‚\n",
    "    * `non-hate`ï¼šä¸å­˜åœ¨æ”»å‡»ç¾¤ä½“ã€‚\n",
    "4.  **æ˜¯å¦ä»‡æ¨ (Hateful)ï¼š** åˆ¤æ–­è¯¥â€œè¯„è®ºå¯¹è±¡-è®ºç‚¹â€æ˜¯å¦æ„æˆäº†å¯¹â€œç›®æ ‡ç¾¤ä½“â€çš„ä»‡æ¨è¨€è®ºã€‚\n",
    "    * `hate`ï¼šæ„æˆä»‡æ¨ã€‚\n",
    "    * `non-hate`ï¼šä¸æ„æˆä»‡æ¨ï¼ˆåŒ…æ‹¬ä¸­æ€§ã€ç§¯æã€æˆ–ä¸€èˆ¬æ€§è´Ÿé¢è¯„è®ºä½†æœªè¾¾åˆ°ä»‡æ¨ç¨‹åº¦ï¼‰ã€‚\n",
    "æ ¼å¼è¦æ±‚ï¼š\n",
    "* å››å…ƒç»„å†…å„å…ƒç´ ä¹‹é—´ç”¨ \" | \"ï¼ˆç©ºæ ¼ç«–æ ç©ºæ ¼ï¼‰åˆ†éš”ã€‚\n",
    "* æ¯ä¸ªå››å…ƒç»„å¿…é¡»ä»¥ \" [END]\"ï¼ˆç©ºæ ¼[END]ï¼‰ç»“å°¾ã€‚\n",
    "* å¦‚æœä¸€æ¡è¯„è®ºä¸­è¯†åˆ«å‡ºå¤šä¸ªç‹¬ç«‹çš„è¯„è®ºå¯¹è±¡å’Œè®ºç‚¹ï¼Œåº”è¾“å‡ºå¤šä¸ªå››å…ƒç»„ï¼Œä¸åŒå››å…ƒç»„ä¹‹é—´ç”¨ \" [SEP] \"ï¼ˆç©ºæ ¼[SEP]ç©ºæ ¼ï¼‰åˆ†éš”ã€‚\n",
    "\n",
    "ç°åœ¨ï¼Œè¯·å¤„ç†ä»¥ä¸‹æ–°çš„è¾“å…¥å†…å®¹ï¼š\n",
    "<</SYS>>\n",
    "\n",
    "ç”¨æˆ·æä¾›çš„æ–‡æœ¬å¦‚ä¸‹ï¼š\n",
    "{input_text} [/INST]\n",
    "æ¨¡å‹è¾“å‡ºï¼š\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285c2d5fea04c6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- å•å…ƒæ ¼ 3: æ•°æ®åŠ è½½ä¸å‡†å¤‡å‡½æ•°  ğŸ“„ ---\n",
    "def load_and_prepare_data(file_path, test_size=0.1, random_state=SEED):\n",
    "    input_texts_from_user = []      \n",
    "    target_quadruples_from_assistant = [] \n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"é”™è¯¯: è®­ç»ƒæ–‡ä»¶ '{file_path}' æœªæ‰¾åˆ°ã€‚è¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®ã€‚\")\n",
    "        \n",
    "    print(f\"å¼€å§‹ä» '{file_path}' åŠ è½½æ•°æ® (é€‚é… 'messages' æ ¼å¼)...\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f, 1): \n",
    "            try:\n",
    "                data_item = json.loads(line) \n",
    "                \n",
    "                if \"messages\" not in data_item or not isinstance(data_item[\"messages\"], list):\n",
    "                    # print(f\"è­¦å‘Š: è·³è¿‡è¡Œ (è¡Œå· {line_num})ï¼Œå› ä¸ºç¼ºå°‘ 'messages' é”®æˆ–å…¶å€¼ä¸æ˜¯åˆ—è¡¨: {line.strip()}\")\n",
    "                    continue\n",
    "\n",
    "                messages_list = data_item[\"messages\"]\n",
    "                user_content = None\n",
    "                assistant_content = None\n",
    "                \n",
    "                for message_dict in messages_list:\n",
    "                    if \"role\" in message_dict and \"content\" in message_dict:\n",
    "                        if message_dict[\"role\"] == \"user\":\n",
    "                            user_content = message_dict[\"content\"]\n",
    "                        elif message_dict[\"role\"] == \"assistant\":\n",
    "                            assistant_content = message_dict[\"content\"]\n",
    "                \n",
    "                if user_content is not None and assistant_content is not None:\n",
    "                    input_texts_from_user.append(user_content)\n",
    "                    target_quadruples_from_assistant.append(assistant_content)\n",
    "                # else:\n",
    "                    # print(f\"è­¦å‘Š: è·³è¿‡è¡Œ (è¡Œå· {line_num})ï¼Œæœªèƒ½ä» 'messages' ä¸­åŒæ—¶æ‰¾åˆ° 'user' å’Œ 'assistant' çš„æœ‰æ•ˆå†…å®¹ã€‚\")\n",
    "\n",
    "            except json.JSONDecodeError:\n",
    "                # print(f\"è­¦å‘Š: è·³è¿‡æ— æ•ˆçš„JSONè¡Œ (è¡Œå· {line_num}): {line.strip()}\")\n",
    "                pass \n",
    "            except Exception: \n",
    "                # print(f\"è­¦å‘Š: å¤„ç†è¡Œ (è¡Œå· {line_num}) æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯ '{e}': {line.strip()}\")\n",
    "                pass \n",
    "    \n",
    "    if not input_texts_from_user or not target_quadruples_from_assistant:\n",
    "        raise ValueError(f\"é”™è¯¯: æœªèƒ½ä» '{file_path}' åŠ è½½ä»»ä½•æœ‰æ•ˆçš„ 'user'/'assistant' å¯¹è¯æ•°æ®ã€‚\")\n",
    "    \n",
    "    print(f\"æˆåŠŸä» '{file_path}' åŠ è½½äº† {len(input_texts_from_user)} æ¡æœ‰æ•ˆçš„å¯¹è¯è®°å½•ã€‚\")\n",
    "\n",
    "    print(f\"æ­£åœ¨å°†æ•°æ®åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›† (éªŒè¯é›†æ¯”ä¾‹: {test_size})...\")\n",
    "    train_texts, val_texts, train_quads, val_quads = train_test_split(\n",
    "        input_texts_from_user, target_quadruples_from_assistant, \n",
    "        test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    print(f\"åˆ’åˆ†å®Œæˆ: è®­ç»ƒé›† {len(train_texts)} æ¡, éªŒè¯é›† {len(val_texts)} æ¡ã€‚\")\n",
    "\n",
    "    train_dataset = Dataset.from_dict({\"text\": train_texts, \"quadruples_str\": train_quads})\n",
    "    val_dataset = Dataset.from_dict({\"text\": val_texts, \"quadruples_str\": val_quads})\n",
    "    \n",
    "    return DatasetDict({\"train\": train_dataset, \"validation\": val_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67979915c9d1c1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å‡†å¤‡ä»æ–‡ä»¶ './train_formatted_for_llm.jsonl' åŠ è½½æ•°æ®...\n",
      "å¼€å§‹ä» './train_formatted_for_llm.jsonl' åŠ è½½æ•°æ® (é€‚é… 'messages' æ ¼å¼)...\n",
      "æˆåŠŸä» './train_formatted_for_llm.jsonl' åŠ è½½äº† 4000 æ¡æœ‰æ•ˆçš„å¯¹è¯è®°å½•ã€‚\n",
      "æ­£åœ¨å°†æ•°æ®åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›† (éªŒè¯é›†æ¯”ä¾‹: 0.1)...\n",
      "åˆ’åˆ†å®Œæˆ: è®­ç»ƒé›† 3600 æ¡, éªŒè¯é›† 400 æ¡ã€‚\n",
      "\n",
      "æ•°æ®åŠ è½½å’Œåˆæ­¥åˆ’åˆ†æˆåŠŸ:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'quadruples_str'],\n",
      "        num_rows: 3600\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'quadruples_str'],\n",
      "        num_rows: 400\n",
      "    })\n",
      "})\n",
      "\n",
      "è®­ç»ƒé›†ä¸­çš„ç¬¬ä¸€ä¸ªæ ·æœ¬ç¤ºä¾‹:\n",
      "  è¾“å…¥æ–‡æœ¬ (text): ç°å®é‡Œå¯ä¸æ˜¯è¿™æ ·çš„ï¼Œè‡³å°‘æˆ‘è®¤è¯†çš„å®‰å¾½äººæ°‘ï¼Œéƒ½æ˜¯ä¸æƒ¹äº‹ï¼Œä¸æ€•äº‹ï¼Œé‡åˆ°æŒ‘è¡…ï¼Œç”Ÿæ­»çœ‹æ·¡ä¸æœå°±å¹²ğŸ˜„ğŸ˜\n",
      "  ç›®æ ‡æ ‡ç­¾ (quadruples_str): å®‰å¾½äººæ°‘ | ä¸æƒ¹äº‹ï¼Œä¸æ€•äº‹ | non-hate | non-hate [END]\n"
     ]
    }
   ],
   "source": [
    "# --- å•å…ƒæ ¼ 4: åŠ è½½å¹¶æ£€æŸ¥åŸå§‹æ•°æ® ğŸ§ ---\n",
    "print(f\"å‡†å¤‡ä»æ–‡ä»¶ '{TRAIN_FILE_PATH}' åŠ è½½æ•°æ®...\")\n",
    "raw_datasets = None # åˆå§‹åŒ–\n",
    "try:\n",
    "    raw_datasets = load_and_prepare_data(TRAIN_FILE_PATH)\n",
    "    print(\"\\næ•°æ®åŠ è½½å’Œåˆæ­¥åˆ’åˆ†æˆåŠŸ:\")\n",
    "    print(raw_datasets) \n",
    "    \n",
    "    if raw_datasets and 'train' in raw_datasets and len(raw_datasets['train']) > 0:\n",
    "        print(f\"\\nè®­ç»ƒé›†ä¸­çš„ç¬¬ä¸€ä¸ªæ ·æœ¬ç¤ºä¾‹:\")\n",
    "        print(f\"  è¾“å…¥æ–‡æœ¬ (text): {raw_datasets['train'][0]['text']}\")\n",
    "        print(f\"  ç›®æ ‡æ ‡ç­¾ (quadruples_str): {raw_datasets['train'][0]['quadruples_str']}\")\n",
    "    else:\n",
    "        print(\"\\nè­¦å‘Š: åŠ è½½åçš„ 'raw_datasets' ä¸ºç©ºæˆ– 'train' éƒ¨åˆ†ä¸å®Œæ•´ã€‚è¯·æ£€æŸ¥æ•°æ®åŠ è½½è¿‡ç¨‹ã€‚\")\n",
    "except Exception as e:\n",
    "    print(f\"\\næ•°æ®åŠ è½½æˆ–å‡†å¤‡è¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8210e7073a043d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å‡†å¤‡ä¸º 3600 æ¡è®­ç»ƒæ–‡æœ¬ç”Ÿæˆä¼ªæ ‡ç­¾...\n",
      "æ­£åœ¨ä» '/root/autodl-tmp/models/Qwen3-8B' åŠ è½½ç”¨äºç”Ÿæˆä¼ªæ ‡ç­¾çš„LLMå’ŒTokenizer...\n",
      "ç”Ÿæˆå™¨LLMå°†ä½¿ç”¨é‡åŒ–: nf4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/accelerate/utils/modeling.py:1569: UserWarning: Current model requires 32.0 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eace4c80237c4605b3ecb25511a6658c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç”Ÿæˆå™¨LLMåŠç›¸å…³èµ„æºå·²å°è¯•æ¸…ç†ã€‚\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 65\u001b[0m\n\u001b[1;32m     62\u001b[0m         generator_bnb_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(load_in_8bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mç”Ÿæˆå™¨LLMå°†ä½¿ç”¨é‡åŒ–: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mGENERATOR_QUANTIZATION_TYPE_FOR_PSEUDO\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mgenerator_bnb_config\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mæ— \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m generator_model_instance \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mGENERATOR_MODEL_NAME_FOR_PSEUDO\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator_bnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     70\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m generator_model_instance\u001b[38;5;241m.\u001b[39meval() \n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generator_tokenizer_instance\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:571\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    570\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[0;32m--> 571\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    577\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:309\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    311\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:4574\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4564\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4565\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   4567\u001b[0m     (\n\u001b[1;32m   4568\u001b[0m         model,\n\u001b[1;32m   4569\u001b[0m         missing_keys,\n\u001b[1;32m   4570\u001b[0m         unexpected_keys,\n\u001b[1;32m   4571\u001b[0m         mismatched_keys,\n\u001b[1;32m   4572\u001b[0m         offload_index,\n\u001b[1;32m   4573\u001b[0m         error_msgs,\n\u001b[0;32m-> 4574\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4575\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4576\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4580\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4583\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4589\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4590\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4592\u001b[0m \u001b[38;5;66;03m# record tp degree the model sharded to\u001b[39;00m\n\u001b[1;32m   4593\u001b[0m model\u001b[38;5;241m.\u001b[39m_tp_size \u001b[38;5;241m=\u001b[39m tp_size\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:5031\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[0m\n\u001b[1;32m   5029\u001b[0m \u001b[38;5;66;03m# Skip it with fsdp on ranks other than 0\u001b[39;00m\n\u001b[1;32m   5030\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_fsdp_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_quantized):\n\u001b[0;32m-> 5031\u001b[0m     disk_offload_index, cpu_offload_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5032\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5033\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5034\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5035\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5036\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreverse_key_renaming_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5037\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5038\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5039\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5040\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcpu_offload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcpu_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5041\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcpu_offload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcpu_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5042\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5043\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_offloaded_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5044\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5045\u001b[0m \u001b[43m        \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5046\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5047\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5049\u001b[0m \u001b[38;5;66;03m# force memory release if loading multiple shards, to avoid having 2 state dicts in memory in next loop\u001b[39;00m\n\u001b[1;32m   5050\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m state_dict\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:846\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, shard_file, expected_keys, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, cpu_offload_folder, cpu_offload_index, hf_quantizer, is_safetensors, keep_in_fp32_regex, unexpected_keys, device_mesh)\u001b[0m\n\u001b[1;32m    843\u001b[0m     _load_parameter_into_model(model, param_name, param\u001b[38;5;241m.\u001b[39mto(param_device))\n\u001b[1;32m    845\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 846\u001b[0m     \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_quantized_param\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munexpected_keys\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    849\u001b[0m     \u001b[38;5;66;03m# For quantized modules with FSDP/DeepSpeed Stage 3, we need to quantize the parameter on the GPU\u001b[39;00m\n\u001b[1;32m    850\u001b[0m     \u001b[38;5;66;03m# and then cast it to CPU to avoid excessive memory usage on each GPU\u001b[39;00m\n\u001b[1;32m    851\u001b[0m     \u001b[38;5;66;03m# in comparison to the sharded model across GPUs.\u001b[39;00m\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_fsdp_enabled() \u001b[38;5;129;01mor\u001b[39;00m is_deepspeed_zero3_enabled():\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:243\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.create_quantized_param\u001b[0;34m(self, model, param_value, param_name, target_device, state_dict, unexpected_keys)\u001b[0m\n\u001b[1;32m    240\u001b[0m         new_value \u001b[38;5;241m=\u001b[39m new_value\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    242\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m old_value\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n\u001b[0;32m--> 243\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mParams4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m new_value\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/bitsandbytes/nn/modules.py:343\u001b[0m, in \u001b[0;36mParams4bit.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m device, dtype, non_blocking, convert_to_format \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39m_parse_to(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m device\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbnb_quantized:\n\u001b[0;32m--> 343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_quantize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/bitsandbytes/nn/modules.py:302\u001b[0m, in \u001b[0;36mParams4bit._quantize\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_quantize\u001b[39m(\u001b[38;5;28mself\u001b[39m, device):\n\u001b[1;32m    301\u001b[0m     w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 302\u001b[0m     w_4bit, quant_state \u001b[38;5;241m=\u001b[39m \u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantize_4bit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m        \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblocksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompress_statistics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompress_statistics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquant_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquant_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m w_4bit\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_state \u001b[38;5;241m=\u001b[39m quant_state\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/bitsandbytes/functional.py:1008\u001b[0m, in \u001b[0;36mquantize_4bit\u001b[0;34m(A, absmax, out, blocksize, compress_statistics, quant_type, quant_storage)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Quantize tensor A in blocks of 4-bit values.\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \n\u001b[1;32m    985\u001b[0m \u001b[38;5;124;03mQuantizes tensor A by dividing it into blocks which are independently quantized.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;124;03m    - [`QuantState`]: The state object used to undo the quantization.\u001b[39;00m\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1006\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m-> 1008\u001b[0m _out, _absmax \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbitsandbytes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantize_4bit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblocksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquant_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquant_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1015\u001b[0m code \u001b[38;5;241m=\u001b[39m get_4bit_type(quant_type, device\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compress_statistics:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/_ops.py:716\u001b[0m, in \u001b[0;36mOpOverload.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 716\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/_compile.py:32\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[1;32m     30\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m prior \u001b[38;5;241m=\u001b[39m _maybe_set_eval_frame(callback)\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    634\u001b[0m     _maybe_set_eval_frame(prior)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/library.py:566\u001b[0m, in \u001b[0;36m_impl.<locals>.register.<locals>.func_no_dynamo\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_dynamo\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc_no_dynamo\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 566\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/bitsandbytes/backends/cpu/ops.py:115\u001b[0m, in \u001b[0;36m_\u001b[0;34m(A, blocksize, quant_type, quant_storage)\u001b[0m\n\u001b[1;32m    112\u001b[0m scaled \u001b[38;5;241m=\u001b[39m blocks \u001b[38;5;241m/\u001b[39m absmax\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# Quantize with the lookup table\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m quantized \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmin(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscaled\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_NF4_QUANT_TABLE\u001b[49m\u001b[43m)\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39muint8)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# Pack two quantized values per byte\u001b[39;00m\n\u001b[1;32m    118\u001b[0m packed \u001b[38;5;241m=\u001b[39m quantized[::\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m<<\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m|\u001b[39m quantized[\u001b[38;5;241m1\u001b[39m::\u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- å•å…ƒæ ¼ 3.5 (æ–°å¢): åˆ©ç”¨LLMé›¶æ ·æœ¬èƒ½åŠ›ç”Ÿæˆä¼ªæ ‡ç­¾æ•°æ® ğŸ¤–ğŸ·ï¸ ---\n",
    "# --- é…ç½®ç”¨äºç”Ÿæˆä¼ªæ ‡ç­¾çš„LLM ---\n",
    "GENERATOR_MODEL_NAME_FOR_PSEUDO = \"/root/autodl-tmp/models/Qwen3-8B\" # ç¤ºä¾‹ï¼šä½¿ç”¨ä¸å¾®è°ƒç›¸åŒçš„æ¨¡å‹è·¯å¾„ï¼Œæˆ–å¦ä¸€ä¸ªæ›´å¼ºçš„æ¨¡å‹\n",
    "GENERATOR_USE_QUANTIZATION_FOR_PSEUDO = True \n",
    "GENERATOR_QUANTIZATION_TYPE_FOR_PSEUDO = \"nf4\"\n",
    "\n",
    "# ç”¨äºç”Ÿæˆä¼ªæ ‡ç­¾çš„æç¤ºæ¨¡æ¿ (ä¸å¾®è°ƒçš„PROMPT_TEMPLATEç±»ä¼¼ï¼Œä½†ä¸åŒ…å« \"æ¨¡å‹è¾“å‡ºï¼š\" åçš„ç­”æ¡ˆéƒ¨åˆ†)\n",
    "# æ³¨æ„ï¼šè¿™é‡Œçš„ GENERATOR_PROMPT_TEMPLATE ä¸ä¸» PROMPT_TEMPLATE å‡ ä¹ä¸€è‡´ï¼Œ\n",
    "# ç¡®ä¿ \"æ¨¡å‹è¾“å‡ºï¼š\" ä¹‹åæ˜¯ç©ºçš„ï¼Œä»¥ä¾¿LLMå¡«å……ã€‚\n",
    "GENERATOR_PROMPT_TEMPLATE = \"\"\"<s>[INST] <<SYS>>\n",
    "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¸­æ–‡ç¤¾äº¤åª’ä½“å†…å®¹åˆ†æåŠ©æ‰‹ï¼Œä¸“é—¨ç”¨äºç»†ç²’åº¦ç‰‡æ®µçº§ä»‡æ¨è¨€è®ºè¯†åˆ«ã€‚è¯·æ ¹æ®ç”¨æˆ·æä¾›çš„æ–‡æœ¬ï¼Œè¯†åˆ«å…¶ä¸­å­˜åœ¨çš„ä»‡æ¨è¨€è®ºæˆ–éä»‡æ¨çš„è¯„è®ºæ€§è¨€è®ºï¼Œå¹¶æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºä¸€ä¸ªæˆ–å¤šä¸ªå››å…ƒç»„ï¼š\n",
    "è¯„è®ºå¯¹è±¡ (Target) | è®ºç‚¹ (Argument) | ç›®æ ‡ç¾¤ä½“ (Targeted Group) | æ˜¯å¦ä»‡æ¨ (Hateful) [END]\n",
    "è¯¦ç»†è¯´æ˜ï¼š\n",
    "1.  **è¯„è®ºå¯¹è±¡ (Target)ï¼š** å¸–å­ä¸­è¢«è¯„è®ºæˆ–æåŠçš„å…·ä½“äººç‰©ã€ç¾¤ä½“ã€äº‹ç‰©æˆ–æ¦‚å¿µã€‚å¦‚æœæ˜¯é’ˆå¯¹æ–‡æœ¬ä¸­éšå«çš„ã€æ²¡æœ‰æ˜ç¡®æŒ‡å‡ºçš„å¯¹è±¡ï¼Œæˆ–è€…è¯„è®ºæ˜¯æ³›æŒ‡ï¼Œåˆ™è®¾ä¸º \"NULL\"ã€‚\n",
    "2.  **è®ºç‚¹ (Argument)ï¼š** é’ˆå¯¹â€œè¯„è®ºå¯¹è±¡â€æ‰€å‘è¡¨çš„æ ¸å¿ƒè§‚ç‚¹ã€æè¿°æˆ–è¡Œä¸ºï¼Œåº”ä¸ºæ–‡æœ¬ä¸­çš„ä¸€ä¸ªå…³é”®ä¿¡æ¯ç‰‡æ®µã€‚\n",
    "3.  **ç›®æ ‡ç¾¤ä½“ (Targeted Group)ï¼š** æŒ‡è¯¥â€œè¯„è®ºå¯¹è±¡-è®ºç‚¹â€æ‰€æ¶‰åŠæˆ–æŒ‡å‘çš„ç¤¾ä¼šç¾¤ä½“ã€‚å¿…é¡»ä»ä»¥ä¸‹é¢„è®¾ç±»åˆ«ä¸­é€‰æ‹©ï¼š\n",
    "    * `Region`ï¼šé’ˆå¯¹ç‰¹å®šåœ°åŸŸï¼ˆå›½å®¶ã€çœä»½ã€åŸå¸‚ç­‰ï¼‰äººç¾¤çš„è¯„è®ºã€‚\n",
    "    * `Racism`ï¼šé’ˆå¯¹ç‰¹å®šç§æ—æˆ–æ°‘æ—äººç¾¤çš„è¯„è®ºã€‚\n",
    "    * `Sexism`ï¼šé’ˆå¯¹ç‰¹å®šæ€§åˆ«äººç¾¤ï¼ˆç”·æ€§ã€å¥³æ€§ï¼‰çš„è¯„è®ºï¼Œæˆ–æ€§åˆ«æ­§è§†ã€åˆ»æ¿å°è±¡ã€‚\n",
    "    * `LGBTQ`ï¼šé’ˆå¯¹æ€§å°‘æ•°ç¾¤ä½“çš„è¯„è®ºï¼ˆå¦‚åŒæ€§æ‹ã€è·¨æ€§åˆ«ç­‰ï¼‰ã€‚\n",
    "    * `others`ï¼šé’ˆå¯¹ä¸Šè¿°å››ç±»ä¹‹å¤–çš„ç‰¹å®šç¾¤ä½“ï¼ˆå¦‚ç‰¹å®šèŒä¸šã€ç–¾ç—…ç¾¤ä½“ã€æ”¿æ²»ç«‹åœºç¾¤ä½“ç­‰ï¼‰æˆ–ä¸æ„æˆå¯¹ç‰¹å®šç¤¾ä¼šç¾¤ä½“çš„æ”»å‡»ï¼Œè€Œæ˜¯ä¸ªäººæ”»å‡»ã€è§‚ç‚¹è¯„è®ºç­‰ã€‚\n",
    "    * `non-hate`ï¼šä¸å­˜åœ¨æ”»å‡»ç¾¤ä½“ã€‚\n",
    "4.  **æ˜¯å¦ä»‡æ¨ (Hateful)ï¼š** åˆ¤æ–­è¯¥â€œè¯„è®ºå¯¹è±¡-è®ºç‚¹â€æ˜¯å¦æ„æˆäº†å¯¹â€œç›®æ ‡ç¾¤ä½“â€çš„ä»‡æ¨è¨€è®ºã€‚\n",
    "    * `hate`ï¼šæ„æˆä»‡æ¨ã€‚\n",
    "    * `non-hate`ï¼šä¸æ„æˆä»‡æ¨ï¼ˆåŒ…æ‹¬ä¸­æ€§ã€ç§¯æã€æˆ–ä¸€èˆ¬æ€§è´Ÿé¢è¯„è®ºä½†æœªè¾¾åˆ°ä»‡æ¨ç¨‹åº¦ï¼‰ã€‚\n",
    "æ ¼å¼è¦æ±‚ï¼š\n",
    "* å››å…ƒç»„å†…å„å…ƒç´ ä¹‹é—´ç”¨ \" | \"ï¼ˆç©ºæ ¼ç«–æ ç©ºæ ¼ï¼‰åˆ†éš”ã€‚\n",
    "* æ¯ä¸ªå››å…ƒç»„å¿…é¡»ä»¥ \" [END]\"ï¼ˆç©ºæ ¼[END]ï¼‰ç»“å°¾ã€‚\n",
    "* å¦‚æœä¸€æ¡è¯„è®ºä¸­è¯†åˆ«å‡ºå¤šä¸ªç‹¬ç«‹çš„è¯„è®ºå¯¹è±¡å’Œè®ºç‚¹ï¼Œåº”è¾“å‡ºå¤šä¸ªå››å…ƒç»„ï¼Œä¸åŒå››å…ƒç»„ä¹‹é—´ç”¨ \" [SEP] \"ï¼ˆç©ºæ ¼[SEP]ç©ºæ ¼ï¼‰åˆ†éš”ã€‚\n",
    "\n",
    "ç°åœ¨ï¼Œè¯·å¤„ç†ä»¥ä¸‹æ–°çš„è¾“å…¥å†…å®¹ï¼š\n",
    "<</SYS>>\n",
    "\n",
    "ç”¨æˆ·æä¾›çš„æ–‡æœ¬å¦‚ä¸‹ï¼š\n",
    "{input_text} [/INST]\n",
    "æ¨¡å‹è¾“å‡ºï¼š\n",
    "\"\"\"\n",
    "\n",
    "pseudo_labels_list = []\n",
    "texts_for_pseudo_generation = []\n",
    "\n",
    "if raw_datasets and 'train' in raw_datasets and raw_datasets['train'] is not None:\n",
    "    texts_for_pseudo_generation = list(raw_datasets['train']['text'])\n",
    "    print(f\"å‡†å¤‡ä¸º {len(texts_for_pseudo_generation)} æ¡è®­ç»ƒæ–‡æœ¬ç”Ÿæˆä¼ªæ ‡ç­¾...\")\n",
    "\n",
    "    # --- åŠ è½½ç”Ÿæˆå™¨LLMå’ŒTokenizer ---\n",
    "    # ä¸ºé¿å…ä¸ä¸»æ¨¡å‹å†²çªï¼Œä½¿ç”¨ä¸åŒçš„å˜é‡å\n",
    "    generator_model_instance = None\n",
    "    generator_tokenizer_instance = None\n",
    "    print(f\"æ­£åœ¨ä» '{GENERATOR_MODEL_NAME_FOR_PSEUDO}' åŠ è½½ç”¨äºç”Ÿæˆä¼ªæ ‡ç­¾çš„LLMå’ŒTokenizer...\")\n",
    "    try:\n",
    "        generator_tokenizer_instance = AutoTokenizer.from_pretrained(GENERATOR_MODEL_NAME_FOR_PSEUDO, trust_remote_code=True)\n",
    "        \n",
    "        generator_bnb_config = None\n",
    "        if GENERATOR_USE_QUANTIZATION_FOR_PSEUDO:\n",
    "            if GENERATOR_QUANTIZATION_TYPE_FOR_PSEUDO == \"nf4\" or GENERATOR_QUANTIZATION_TYPE_FOR_PSEUDO == \"fp4\":\n",
    "                generator_bnb_config = BitsAndBytesConfig(\n",
    "                    load_in_4bit=True, bnb_4bit_quant_type=GENERATOR_QUANTIZATION_TYPE_FOR_PSEUDO,\n",
    "                    bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_use_double_quant=True,\n",
    "                )\n",
    "            elif GENERATOR_QUANTIZATION_TYPE_FOR_PSEUDO == \"int8\":\n",
    "                generator_bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "            print(f\"ç”Ÿæˆå™¨LLMå°†ä½¿ç”¨é‡åŒ–: {GENERATOR_QUANTIZATION_TYPE_FOR_PSEUDO if generator_bnb_config else 'æ— '}\")\n",
    "\n",
    "        generator_model_instance = AutoModelForCausalLM.from_pretrained(\n",
    "            GENERATOR_MODEL_NAME_FOR_PSEUDO,\n",
    "            quantization_config=generator_bnb_config,\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        generator_model_instance.eval() \n",
    "\n",
    "        if generator_tokenizer_instance.pad_token is None:\n",
    "            generator_tokenizer_instance.pad_token = generator_tokenizer_instance.eos_token\n",
    "            if generator_model_instance.config.pad_token_id is None:\n",
    "                 generator_model_instance.config.pad_token_id = generator_tokenizer_instance.pad_token_id\n",
    "            print(f\"ç”Ÿæˆå™¨Tokenizerçš„pad_tokenå·²è®¾ç½®ä¸ºeos_token: '{generator_tokenizer_instance.eos_token}'\")\n",
    "        \n",
    "        print(\"ç”Ÿæˆå™¨LLMå’ŒTokenizeråŠ è½½æˆåŠŸã€‚\")\n",
    "\n",
    "        GENERATION_BATCH_SIZE = 4 # ä¼ªæ ‡ç­¾ç”Ÿæˆæ‰¹æ¬¡å¤§å°\n",
    "        \n",
    "        generation_config_pseudo = GenerationConfig(\n",
    "            max_new_tokens=MAX_TARGET_LENGTH, \n",
    "            num_beams=1, \n",
    "            do_sample=False, \n",
    "            pad_token_id=generator_tokenizer_instance.pad_token_id if generator_tokenizer_instance.pad_token_id is not None else generator_tokenizer_instance.eos_token_id,\n",
    "            eos_token_id=generator_tokenizer_instance.eos_token_id\n",
    "        )\n",
    "\n",
    "        for i in tqdm(range(0, len(texts_for_pseudo_generation), GENERATION_BATCH_SIZE), desc=\"ç”Ÿæˆä¼ªæ ‡ç­¾\"):\n",
    "            batch_texts = texts_for_pseudo_generation[i : i + GENERATION_BATCH_SIZE]\n",
    "            batch_prompts = [GENERATOR_PROMPT_TEMPLATE.format(input_text=text) for text in batch_texts]\n",
    "            \n",
    "            inputs = generator_tokenizer_instance(\n",
    "                batch_prompts, \n",
    "                return_tensors=\"pt\", \n",
    "                padding=True, \n",
    "                truncation=True, \n",
    "                max_length=MAX_INPUT_LENGTH - MAX_TARGET_LENGTH \n",
    "            ).to(generator_model_instance.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = generator_model_instance.generate(**inputs, generation_config=generation_config_pseudo)\n",
    "            \n",
    "            # è§£ç å¹¶æå–ç­”æ¡ˆéƒ¨åˆ†\n",
    "            # outputs åŒ…å«å®Œæ•´çš„åºåˆ— (æç¤º+ç­”æ¡ˆ)ã€‚æˆ‘ä»¬éœ€è¦æå–æ¨¡å‹ç”Ÿæˆçš„éƒ¨åˆ†ã€‚\n",
    "            # input_len = inputs.input_ids.shape[1]\n",
    "            # generated_ids_batch = outputs[:, input_len:] # è·å–æ¯ä¸ªæ ·æœ¬æ–°ç”Ÿæˆçš„token\n",
    "            # decoded_answers = generator_tokenizer_instance.batch_decode(generated_ids_batch, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "            \n",
    "            # æˆ–è€…ï¼Œä½¿ç”¨ä¹‹å‰çš„æ–¹æ³•ä»å®Œæ•´è§£ç æ–‡æœ¬ä¸­åˆ†å‰²\n",
    "            full_decoded_outputs = generator_tokenizer_instance.batch_decode(outputs, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "            keyword_separator_pseudo = \"æ¨¡å‹è¾“å‡ºï¼š\" # ä¸GENERATOR_PROMPT_TEMPLATEæœ«å°¾ä¸€è‡´\n",
    "            \n",
    "            for full_output_text in full_decoded_outputs:\n",
    "                answer_part_str = \"\"\n",
    "                if keyword_separator_pseudo in full_output_text:\n",
    "                    answer_part_str = full_output_text.split(keyword_separator_pseudo, 1)[-1].strip()\n",
    "                else: # åå¤‡æ–¹æ¡ˆ\n",
    "                    original_prompt_text_no_answer = GENERATOR_PROMPT_TEMPLATE.format(input_text=\"DUMMY\").split(keyword_separator_pseudo)[0] # è·å–æç¤ºå¤´\n",
    "                    # è¿™æ˜¯ä¸€ä¸ªç²—ç•¥çš„ç§»é™¤ï¼Œå¯èƒ½ä¸å®Œç¾\n",
    "                    if full_output_text.startswith(original_prompt_text_no_answer.split(\"ç”¨æˆ·æä¾›çš„æ–‡æœ¬å¦‚ä¸‹ï¼š\")[0]): # å°è¯•åŒ¹é…ç³»ç»Ÿæç¤ºéƒ¨åˆ†\n",
    "                         answer_part_str = full_output_text # å¦‚æœæ— æ³•æ¸…æ™°åˆ†å‰²ï¼Œä¿ç•™å®Œæ•´è¾“å‡ºï¼Œåç»­è´¨é‡è¯„ä¼°æ—¶å¤„ç†\n",
    "                    else:\n",
    "                         answer_part_str = full_output_text\n",
    "                pseudo_labels_list.append(answer_part_str)\n",
    "        \n",
    "        print(f\"æˆåŠŸä¸º {len(pseudo_labels_list)} æ¡æ–‡æœ¬ç”Ÿæˆäº†ä¼ªæ ‡ç­¾ã€‚\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"åŠ è½½ç”Ÿæˆå™¨LLMæˆ–ç”Ÿæˆä¼ªæ ‡ç­¾è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        print(\"å°†ä½¿ç”¨ç©ºçš„ä¼ªæ ‡ç­¾åˆ—è¡¨ã€‚\")\n",
    "        pseudo_labels_list = [] \n",
    "    finally:\n",
    "        # æ¸…ç†ç”Ÿæˆå™¨æ¨¡å‹ä»¥é‡Šæ”¾æ˜¾å­˜\n",
    "        if 'generator_model_instance' in locals() and generator_model_instance is not None:\n",
    "            del generator_model_instance\n",
    "        if 'generator_tokenizer_instance' in locals() and generator_tokenizer_instance is not None:\n",
    "            del generator_tokenizer_instance\n",
    "        if 'inputs' in locals() and inputs is not None: del inputs\n",
    "        if 'outputs' in locals() and outputs is not None: del outputs\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        print(\"ç”Ÿæˆå™¨LLMåŠç›¸å…³èµ„æºå·²å°è¯•æ¸…ç†ã€‚\")\n",
    "else:\n",
    "    print(\"è­¦å‘Š: åŸå§‹æ•°æ®é›† 'raw_datasets' æœªåŠ è½½ï¼Œæ— æ³•ç”Ÿæˆä¼ªæ ‡ç­¾ã€‚\")\n",
    "    pseudo_labels_list = []\n",
    "\n",
    "if pseudo_labels_list:\n",
    "    print(\"\\nç”Ÿæˆçš„ä¸€äº›ä¼ªæ ‡ç­¾æ ·æœ¬:\")\n",
    "    for i in range(min(3, len(pseudo_labels_list))):\n",
    "        print(f\"  åŸå§‹æ–‡æœ¬ (éƒ¨åˆ†): {texts_for_pseudo_generation[i][:50]}...\")\n",
    "        print(f\"  ç”Ÿæˆä¼ªæ ‡ç­¾: {pseudo_labels_list[i]}\")\n",
    "else:\n",
    "    print(\"\\næœªèƒ½ç”Ÿæˆæˆ–åŠ è½½ä»»ä½•ä¼ªæ ‡ç­¾ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e63aab90f691bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è­¦å‘Šï¼šå•å…ƒæ ¼8çš„ 'parse_quadruples' å‡½æ•°å®šä¹‰å…ˆäºæ­¤å•å…ƒæ ¼æ‰§è¡Œã€‚å°†ä½¿ç”¨ä¸´æ—¶å ä½ç¬¦ã€‚\n",
      "å¼€å§‹åŸºäºLLMç”Ÿæˆçš„ä¼ªæ ‡ç­¾ï¼ˆä½œä¸ºè´Ÿä¾‹ï¼‰è¿›è¡Œå¯¹æ¯”æ•°æ®å¢å¼º...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "åˆ›å»ºå¯¹æ¯”å¢å¼ºSFTæ•°æ®: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3600/3600 [00:00<00:00, 500629.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¯¹æ¯”æ•°æ®å¢å¼ºå®Œæˆã€‚\n",
      "åŸå§‹è®­ç»ƒæ ·æœ¬æ•°: 3600\n",
      "é¢å¤–åˆ›å»ºäº† 3600 ä¸ªå¯¹æ¯”å¢å¼ºæ ·æœ¬ã€‚\n",
      "å¯¹æ¯”å¢å¼ºSFTæ•°æ®å‡†å¤‡å®Œæˆã€‚è®­ç»ƒé›†ç°åœ¨åŒ…å« 7200 æ¡æ ·æœ¬ã€‚\n",
      "å¢å¼ºåè®­ç»ƒé›†çš„ç¬¬ä¸€ä¸ªæ ·æœ¬ text (å¯èƒ½ä¸ºåŸå§‹): ç°å®é‡Œå¯ä¸æ˜¯è¿™æ ·çš„ï¼Œè‡³å°‘æˆ‘è®¤è¯†çš„å®‰å¾½äººæ°‘ï¼Œéƒ½æ˜¯ä¸æƒ¹äº‹ï¼Œä¸æ€•äº‹ï¼Œé‡åˆ°æŒ‘è¡…ï¼Œç”Ÿæ­»çœ‹æ·¡ä¸æœå°±å¹²ğŸ˜„ğŸ˜...\n",
      "å¢å¼ºåè®­ç»ƒé›†çš„ç¬¬ä¸€ä¸ªæ ·æœ¬ quadruple: å®‰å¾½äººæ°‘ | ä¸æƒ¹äº‹ï¼Œä¸æ€•äº‹ | non-hate | non-hate [END]\n",
      "ä¸€ä¸ªå¯¹æ¯”å¢å¼ºæ ·æœ¬çš„ text (éƒ¨åˆ†): åŸå§‹æ–‡æœ¬å†…å®¹ï¼š\n",
      "\"ç”Ÿè€Œä¸ºè»ï¼Œæˆ‘å¾ˆæŠ±æ­‰è¿™ç§ç‰§äººä»£è¡¨åæˆæ˜¯è§‰å¾—ä»™å¥³ä»¬è¦å½©ç¤¼è¦è½¦è¦æˆ¿ä¸æ„¿ç”Ÿå­©å­è¿˜åŠ¨ä¸åŠ¨ç”Ÿå­©å­è­¦å‘Šè·Ÿä½ äº‰å† æ€§æƒçš„è¡Œä¸ºå¾ˆåˆç†å§ï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼ŸåŠ³èµ„æ”¹ä¸ªå±çš„æ‹©å¶è§‚\"\n",
      "\n",
      "ä¸€ä¸ªAIåŠ©æ‰‹é’ˆå¯¹ä»¥ä¸Šæ–‡æœ¬ç»™å‡ºäº†å¦‚ä¸‹å¯èƒ½æ˜¯é”™è¯¯æˆ–ä¸å®Œå–„çš„å››å…ƒç»„æå–ç»“æœï¼š\n",
      "\"è¯„è®ºå¯¹è±¡ (Target) | è®ºç‚¹ (Argument) | ç›®æ ‡ç¾¤ä½“ (Targeted Group) | æ˜¯å¦ä»‡æ¨ (Hateful) [END]\n",
      "è¯„è®ºå¯¹è±¡ (Target) | è®ºç‚¹ (Argument) | ç›®æ ‡ç¾¤ä½“ (Targeted Gr...\n",
      "è¯¥å¢å¼ºæ ·æœ¬çš„ç›®æ ‡ quadruple: è» | ç”Ÿè€Œä¸ºè» | Sexism | hate [SEP] è¿™ç§ç‰§äººä»£è¡¨ | åæˆæ˜¯è§‰å¾—ä»™å¥³ä»¬è¦å½©ç¤¼è¦è½¦è¦æˆ¿ä¸æ„¿ç”Ÿå­©å­è¿˜åŠ¨ä¸åŠ¨ç”Ÿå­©å­è­¦å‘Šè·Ÿä½ äº‰å† æ€§æƒçš„è¡Œä¸ºå¾ˆåˆç†å§ï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼Ÿ | Sexism | hate [END]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- å•å…ƒæ ¼ 3.6: åˆ©ç”¨LLMç”Ÿæˆçš„è´Ÿä¾‹è¿›è¡Œå¯¹æ¯”å¢å¼ºSFTæ•°æ® ---\n",
    "\n",
    "# æ˜¯å¦å¯ç”¨åŸºäºLLMè´Ÿä¾‹çš„å¯¹æ¯”å¢å¼º\n",
    "ENABLE_CONTRASTIVE_AUGMENTATION_WITH_NEGATIVES = True\n",
    "\n",
    "# ç¡®ä¿ parse_quadruples å‡½æ•°å·²å®šä¹‰ (é€šå¸¸åœ¨å•å…ƒæ ¼8)\n",
    "# å¦‚æœå•å…ƒæ ¼8çš„è¿˜æœªæ‰§è¡Œ, ä½¿ç”¨ä¸´æ—¶å ä½ç¬¦ (ä¸»è¦ç”¨äºç»“æ„å®Œæ•´æ€§, å®é™…è¿è¡Œå‰åº”ç¡®ä¿å·²å®šä¹‰)\n",
    "if 'parse_quadruples' not in globals(): \n",
    "    def parse_quadruples_placeholder(text_str_dummy): \n",
    "        if not text_str_dummy: return []\n",
    "        quads = []\n",
    "        parts = text_str_dummy.split(SEP_TOKEN if 'SEP_TOKEN' in globals() else \"[SEP]\") # ä½¿ç”¨å…¨å±€å˜é‡æˆ–é»˜è®¤å€¼\n",
    "        for part in parts:\n",
    "            part_c = part.strip()\n",
    "            if part_c.endswith(END_TOKEN if 'END_TOKEN' in globals() else \"[END]\"):\n",
    "                part_c = part_c[:-len(END_TOKEN if 'END_TOKEN' in globals() else \"[END]\")].strip()\n",
    "            if not part_c: continue\n",
    "            elements = [e.strip() for e in part_c.split(\" | \")]\n",
    "            if len(elements) == 4:\n",
    "                quads.append(elements)\n",
    "        return quads\n",
    "    parse_quadruples_fn_to_use = parse_quadruples_placeholder\n",
    "    print(\"è­¦å‘Šï¼šå•å…ƒæ ¼8çš„ 'parse_quadruples' å‡½æ•°å®šä¹‰å…ˆäºæ­¤å•å…ƒæ ¼æ‰§è¡Œã€‚å°†ä½¿ç”¨ä¸´æ—¶å ä½ç¬¦ã€‚\")\n",
    "else:\n",
    "    parse_quadruples_fn_to_use = parse_quadruples\n",
    "\n",
    "\n",
    "if ENABLE_CONTRASTIVE_AUGMENTATION_WITH_NEGATIVES and \\\n",
    "   'raw_datasets' in locals() and raw_datasets and \\\n",
    "   'pseudo_labels_list' in locals() and \\\n",
    "   len(pseudo_labels_list) == len(raw_datasets['train']): # ç¡®ä¿ä¼ªæ ‡ç­¾åˆ—è¡¨ä¸è®­ç»ƒæ•°æ®å¯¹é½\n",
    "\n",
    "    print(f\"å¼€å§‹åŸºäºLLMç”Ÿæˆçš„ä¼ªæ ‡ç­¾ï¼ˆä½œä¸ºè´Ÿä¾‹ï¼‰è¿›è¡Œå¯¹æ¯”æ•°æ®å¢å¼º...\")\n",
    "    \n",
    "    original_train_texts = list(raw_datasets['train']['text'])\n",
    "    original_train_quads = list(raw_datasets['train']['quadruples_str']) # çœŸå®æ ‡ç­¾ (æ­£ä¾‹)\n",
    "    \n",
    "    # pseudo_labels_list åŒ…å«çš„æ˜¯LLMç”Ÿæˆçš„ä¼ªæ ‡ç­¾ (å°†è¢«è§†ä¸ºè´Ÿä¾‹æˆ–å¹²æ‰°é¡¹)\n",
    "    \n",
    "    contrastive_augmented_texts = []\n",
    "    contrastive_augmented_quads = [] # ç›®æ ‡è¾“å‡ºå§‹ç»ˆæ˜¯çœŸå®çš„å››å…ƒç»„\n",
    "    \n",
    "    num_augmented_samples_created = 0\n",
    "\n",
    "    for i in tqdm(range(len(original_train_texts)), desc=\"åˆ›å»ºå¯¹æ¯”å¢å¼ºSFTæ•°æ®\"):\n",
    "        original_text_content = original_train_texts[i]\n",
    "        true_quad_str = original_train_quads[i]         # æ­£ä¾‹è¾“å‡º\n",
    "        negative_pseudo_quad_str = pseudo_labels_list[i] # LLMç”Ÿæˆçš„ï¼Œä½œä¸ºè´Ÿä¾‹/å¹²æ‰°é¡¹\n",
    "\n",
    "        # 1. æ·»åŠ æ ‡å‡†çš„SFTæ ·æœ¬ï¼š (åŸå§‹æç¤º -> çœŸå®å››å…ƒç»„)\n",
    "        #    PROMPT_TEMPLATE ä¸­çš„ {input_text} å°†ç›´æ¥ä½¿ç”¨ original_text_content\n",
    "        contrastive_augmented_texts.append(original_text_content) \n",
    "        contrastive_augmented_quads.append(true_quad_str)\n",
    "        \n",
    "        # 2. åˆ›å»ºå¯¹æ¯”å¢å¼ºçš„SFTæ ·æœ¬ï¼š\n",
    "        #    (åŒ…å«è´Ÿä¾‹çš„å¤æ‚æç¤º -> çœŸå®å››å…ƒç»„)\n",
    "        #    åªæœ‰å½“ä¼ªæ ‡ç­¾ä¸çœŸå®æ ‡ç­¾ç¡®å®ä¸åŒæ—¶ï¼Œè¿™ç§å¢å¼ºæ‰æœ‰æ„ä¹‰\n",
    "        if negative_pseudo_quad_str and negative_pseudo_quad_str.strip() and \\\n",
    "           negative_pseudo_quad_str.strip() != true_quad_str.strip():\n",
    "            \n",
    "            # æ„å»ºåŒ…å«åŸå§‹æ–‡æœ¬å’Œâ€œé”™è¯¯ææ¡ˆâ€ï¼ˆè´Ÿä¾‹ï¼‰çš„æ–°è¾“å…¥æ–‡æœ¬\n",
    "            # è¿™ä¸ª new_input_for_prompt ä¼šè¢«å¡«å…¥ä¸» PROMPT_TEMPLATE çš„ {input_text} å ä½ç¬¦\n",
    "            new_input_for_prompt = (\n",
    "                f\"åŸå§‹æ–‡æœ¬å†…å®¹ï¼š\\n\\\"{original_text_content}\\\"\\n\\n\"\n",
    "                f\"ä¸€ä¸ªAIåŠ©æ‰‹é’ˆå¯¹ä»¥ä¸Šæ–‡æœ¬ç»™å‡ºäº†å¦‚ä¸‹å¯èƒ½æ˜¯é”™è¯¯æˆ–ä¸å®Œå–„çš„å››å…ƒç»„æå–ç»“æœï¼š\\n\"\n",
    "                f\"\\\"{negative_pseudo_quad_str}\\\"\\n\\n\"\n",
    "                f\"è¯·ä½ å¿½ç•¥ä¸Šè¿°AIåŠ©æ‰‹çš„æå–ç»“æœï¼ˆå®ƒå¯èƒ½åŒ…å«é”™è¯¯ï¼‰ï¼Œå¹¶ä¸¥æ ¼æŒ‰ç…§æŒ‡ä»¤ï¼Œæ ¹æ®â€œåŸå§‹æ–‡æœ¬å†…å®¹â€é‡æ–°åˆ†æå¹¶ç»™å‡ºæ­£ç¡®çš„å››å…ƒç»„ã€‚\"\n",
    "            )\n",
    "            \n",
    "            contrastive_augmented_texts.append(new_input_for_prompt)\n",
    "            contrastive_augmented_quads.append(true_quad_str) # ç›®æ ‡ä»ç„¶æ˜¯çœŸå®çš„å››å…ƒç»„\n",
    "            num_augmented_samples_created += 1\n",
    "\n",
    "    print(f\"å¯¹æ¯”æ•°æ®å¢å¼ºå®Œæˆã€‚\")\n",
    "    print(f\"åŸå§‹è®­ç»ƒæ ·æœ¬æ•°: {len(original_train_texts)}\")\n",
    "    print(f\"é¢å¤–åˆ›å»ºäº† {num_augmented_samples_created} ä¸ªå¯¹æ¯”å¢å¼ºæ ·æœ¬ã€‚\")\n",
    "    \n",
    "    if num_augmented_samples_created > 0 or len(contrastive_augmented_texts) != len(original_train_texts) :\n",
    "        contrastive_augmented_train_dataset = Dataset.from_dict({\n",
    "            \"text\": contrastive_augmented_texts, # \"text\" å­—æ®µç°åœ¨åŒ…å«åŸå§‹æ–‡æœ¬æˆ–å¢å¼ºåçš„å¤æ‚æç¤º\n",
    "            \"quadruples_str\": contrastive_augmented_quads # ç›®æ ‡å§‹ç»ˆæ˜¯çœŸå®çš„å››å…ƒç»„\n",
    "        })\n",
    "        \n",
    "        # æ›´æ–° raw_datasets ä¸­çš„è®­ç»ƒé›†\n",
    "        # éªŒè¯é›†ä¿æŒä¸å˜ï¼Œç”¨äºè¯„ä¼°åŸå§‹ä»»åŠ¡æ€§èƒ½\n",
    "        raw_datasets['train'] = contrastive_augmented_train_dataset\n",
    "        print(f\"å¯¹æ¯”å¢å¼ºSFTæ•°æ®å‡†å¤‡å®Œæˆã€‚è®­ç»ƒé›†ç°åœ¨åŒ…å« {len(raw_datasets['train'])} æ¡æ ·æœ¬ã€‚\")\n",
    "        if len(raw_datasets['train']) > 0:\n",
    "            print(f\"å¢å¼ºåè®­ç»ƒé›†çš„ç¬¬ä¸€ä¸ªæ ·æœ¬ text (å¯èƒ½ä¸ºåŸå§‹): {raw_datasets['train'][0]['text'][:150]}...\") \n",
    "            print(f\"å¢å¼ºåè®­ç»ƒé›†çš„ç¬¬ä¸€ä¸ªæ ·æœ¬ quadruple: {raw_datasets['train'][0]['quadruples_str']}\")\n",
    "            if len(raw_datasets['train']) > len(original_train_texts): # å¦‚æœç¡®å®æ·»åŠ äº†å¢å¼ºæ ·æœ¬\n",
    "                 print(f\"ä¸€ä¸ªå¯¹æ¯”å¢å¼ºæ ·æœ¬çš„ text (éƒ¨åˆ†): {raw_datasets['train'][-1]['text'][:250]}...\") # æ‰“å°æœ€åä¸€ä¸ªï¼ˆå¯èƒ½æ˜¯å¢å¼ºçš„ï¼‰\n",
    "                 print(f\"è¯¥å¢å¼ºæ ·æœ¬çš„ç›®æ ‡ quadruple: {raw_datasets['train'][-1]['quadruples_str']}\")\n",
    "    else:\n",
    "        print(\"æ²¡æœ‰æ–°çš„å¯¹æ¯”å¢å¼ºæ ·æœ¬è¢«æ·»åŠ åˆ°è®­ç»ƒé›†ï¼ˆå¯èƒ½å› ä¸ºæ‰€æœ‰ä¼ªæ ‡ç­¾éƒ½ä¸çœŸå®æ ‡ç­¾ç›¸åŒï¼Œæˆ–è€…ä¼ªæ ‡ç­¾ä¸ºç©ºï¼‰ã€‚\")\n",
    "\n",
    "else:\n",
    "    if not ENABLE_CONTRASTIVE_AUGMENTATION_WITH_NEGATIVES:\n",
    "        print(\"åŸºäºLLMè´Ÿä¾‹çš„å¯¹æ¯”æ•°æ®å¢å¼ºè¢«ç¦ç”¨ã€‚\")\n",
    "    else:\n",
    "        print(\"è­¦å‘Š: æœªæ‰§è¡ŒåŸºäºLLMè´Ÿä¾‹çš„å¯¹æ¯”æ•°æ®å¢å¼ºï¼Œå› ä¸º 'raw_datasets' æˆ– 'pseudo_labels_list' æœªæ­£ç¡®å‡†å¤‡æˆ–æ•°é‡ä¸åŒ¹é…ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc8304a0035d0de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨ä» '/root/autodl-tmp/models/Qwen3-8B' åŠ è½½ç”¨äºå¾®è°ƒçš„Tokenizer...\n",
      "å¾®è°ƒTokenizeråŠ è½½å®Œæˆã€‚\n",
      "å¾®è°ƒTokenizerçš„pad_tokenå·²è®¾ç½®ä¸º: '<|endoftext|>' (ID: 151643)\n"
     ]
    }
   ],
   "source": [
    "# --- å•å…ƒæ ¼ 5: Tokenizer åˆå§‹åŒ–å’Œæ•°æ®é¢„å¤„ç†å‡½æ•° ğŸ“ ---\n",
    "print(f\"æ­£åœ¨ä» '{MODEL_NAME}' åŠ è½½ç”¨äºå¾®è°ƒçš„Tokenizer...\") # ä¸»å¾®è°ƒæ¨¡å‹çš„Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "print(\"å¾®è°ƒTokenizeråŠ è½½å®Œæˆã€‚\")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token \n",
    "    print(f\"å¾®è°ƒTokenizerçš„pad_tokenæœªè®¾ç½®ï¼Œå·²å°†å…¶è®¾ç½®ä¸ºeos_token: '{tokenizer.eos_token}' (ID: {tokenizer.eos_token_id})\")\n",
    "else:\n",
    "    print(f\"å¾®è°ƒTokenizerçš„pad_tokenå·²è®¾ç½®ä¸º: '{tokenizer.pad_token}' (ID: {tokenizer.pad_token_id})\")\n",
    "\n",
    "def preprocess_function_causal(examples):\n",
    "    full_prompts = []\n",
    "    input_texts_for_prompt = examples[\"text\"]\n",
    "    target_outputs = examples[\"quadruples_str\"]\n",
    "\n",
    "    for input_text, target_output in zip(input_texts_for_prompt, target_outputs):\n",
    "        input_text_str = str(input_text) if input_text is not None else \"\"\n",
    "        target_output_str = str(target_output) if target_output is not None else \"\"\n",
    "        \n",
    "        prompt_part = PROMPT_TEMPLATE.format(input_text=input_text_str)\n",
    "        full_text = prompt_part + target_output_str + tokenizer.eos_token\n",
    "        full_prompts.append(full_text)\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        full_prompts,\n",
    "        max_length=MAX_INPUT_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=False, \n",
    "        return_attention_mask=True \n",
    "    )\n",
    "\n",
    "    labels = [list(ids) for ids in model_inputs[\"input_ids\"]] \n",
    "\n",
    "    for i in range(len(examples[\"text\"])):\n",
    "        # input_text_str = str(examples[\"text\"][i]) if examples[\"text\"][i] is not None else \"\"\n",
    "        # prompt_part_only = PROMPT_TEMPLATE.format(input_text=input_text_str)\n",
    "        current_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        current_labels = labels[i]\n",
    "        \n",
    "        answer_part_str = str(examples[\"quadruples_str\"][i]) if examples[\"quadruples_str\"][i] is not None else \"\"\n",
    "        # Tokenizeç­”æ¡ˆéƒ¨åˆ†ï¼ˆä¸åŠ ç‰¹æ®Štokenï¼Œå› ä¸ºå®ƒä»¬å·²åœ¨full_textä¸­å¤„ç†ï¼‰\n",
    "        answer_tokens = tokenizer(answer_part_str + tokenizer.eos_token, add_special_tokens=False)[\"input_ids\"]\n",
    "        \n",
    "        len_to_mask = len(current_input_ids) - len(answer_tokens)\n",
    "        \n",
    "        if len_to_mask < 0: \n",
    "            # print(f\"è­¦å‘Š: æ ·æœ¬ {i} çš„è®¡ç®—å±è”½é•¿åº¦ä¸ºè´Ÿ ({len_to_mask})ã€‚Input: '{str(examples['text'][i])[:50]}...', Target: '{answer_part_str}'\")\n",
    "            # print(f\"  Input IDs len: {len(current_input_ids)}, Answer tokens len: {len(answer_tokens)}\")\n",
    "            # å¦‚æœç­”æ¡ˆæ¯”æ•´ä¸ªåºåˆ—è¿˜é•¿ï¼ˆæˆ–å› æˆªæ–­å¯¼è‡´ä¸åŒ¹é…ï¼‰ï¼Œåˆ™ä¸å±è”½ä»»ä½•å†…å®¹ï¼Œæˆ–ä»…å±è”½BOS\n",
    "            if current_input_ids and current_input_ids[0] == tokenizer.bos_token_id:\n",
    "                 len_to_mask = 1 # åªå±è”½BOS\n",
    "            else:\n",
    "                 len_to_mask = 0 # ä¸å±è”½\n",
    "        \n",
    "        for j in range(min(len_to_mask, len(current_labels))): \n",
    "            current_labels[j] = -100\n",
    "        \n",
    "        if answer_part_str and all(l == -100 for l in current_labels):\n",
    "            # print(f\"è­¦å‘Š: æ ·æœ¬ {i} çš„æ‰€æœ‰æ ‡ç­¾éƒ½è¢«å±è”½ï¼Œä½†ç›®æ ‡è¾“å‡ºä¸ä¸ºç©º ('{answer_part_str}')ã€‚å±è”½é•¿åº¦: {len_to_mask}\")\n",
    "            if current_labels:\n",
    "                 current_labels[-1] = current_input_ids[-1]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfe46af28c5e475a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¼€å§‹å¯¹æ•°æ®é›†è¿›è¡Œtokenizeå’Œé¢„å¤„ç† (é€‚é…Causal LM)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33994ee4649643f4b8bbfac089bf310f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae22d26541a4cae86d722ccce26e68a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "æ•°æ®tokenizeå’Œé¢„å¤„ç†å®Œæˆ:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 7200\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 400\n",
      "    })\n",
      "})\n",
      "\n",
      "Tokenizeåçš„è®­ç»ƒé›†æ ·æœ¬ (æ£€æŸ¥input_idså’Œlabelsçš„å±è”½æƒ…å†µ):\n",
      "  åŸå§‹/å¢å¼ºåè¾“å…¥æ–‡æœ¬ (text): ç°å®é‡Œå¯ä¸æ˜¯è¿™æ ·çš„ï¼Œè‡³å°‘æˆ‘è®¤è¯†çš„å®‰å¾½äººæ°‘ï¼Œéƒ½æ˜¯ä¸æƒ¹äº‹ï¼Œä¸æ€•äº‹ï¼Œé‡åˆ°æŒ‘è¡…ï¼Œç”Ÿæ­»çœ‹æ·¡ä¸æœå°±å¹²ğŸ˜„ğŸ˜\n",
      "  åŸå§‹ç›®æ ‡è¾“å‡º (quadruples_str): å®‰å¾½äººæ°‘ | ä¸æƒ¹äº‹ï¼Œä¸æ€•äº‹ | non-hate | non-hate [END]\n",
      "\n",
      "  Tokenized input_ids (å‰60): [44047, 30768, 64462, 60, 1115, 37931, 39071, 56568, 101909, 104715, 104811, 116253, 43815, 101042, 110498, 3837, 102093, 100751, 99338, 101425, 26381, 115076, 52334, 117828, 109445, 102450, 1773, 14880, 100345, 20002, 103008, 108704, 3837, 102450, 90919, 102670, 117828, 109445, 57191, 65676, 117828, 9370, 85641, 33071, 109445, 90395, 101892, 87752, 68805, 66017, 46944, 57191, 101213, 63703, 23305, 40027, 28311, 85641, 64429, 320]\n",
      "  Decoded input_ids (å‰60): <s>[INST] <<SYS>>\n",
      "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¸­æ–‡ç¤¾äº¤åª’ä½“å†…å®¹åˆ†æåŠ©æ‰‹ï¼Œä¸“é—¨ç”¨äºç»†ç²’åº¦ç‰‡æ®µçº§ä»‡æ¨è¨€è®ºè¯†åˆ«ã€‚è¯·æ ¹æ®ç”¨æˆ·æä¾›çš„æ–‡æœ¬ï¼Œè¯†åˆ«å…¶ä¸­å­˜åœ¨çš„ä»‡æ¨è¨€è®ºæˆ–éä»‡æ¨çš„è¯„è®ºæ€§è¨€è®ºï¼Œå¹¶æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºä¸€ä¸ªæˆ–å¤šä¸ªå››å…ƒç»„ï¼š\n",
      "è¯„è®ºå¯¹è±¡ (\n",
      "\n",
      "  Tokenized labels (å‰60, -100è¡¨ç¤ºå·²å±è”½): [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "  Decoded labels from first non-masked token (éƒ¨åˆ†): å®‰å¾½äººæ°‘ | ä¸æƒ¹äº‹ï¼Œä¸æ€•äº‹ | non-hate | non-hate [END]<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# --- å•å…ƒæ ¼ 6: å¯¹æ•°æ®é›†è¿›è¡ŒTokenize ğŸ§© ---\n",
    "print(\"å¼€å§‹å¯¹æ•°æ®é›†è¿›è¡Œtokenizeå’Œé¢„å¤„ç† (é€‚é…Causal LM)...\")\n",
    "if 'raw_datasets' in locals() and raw_datasets and 'train' in raw_datasets and raw_datasets['train'] is not None:\n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "        preprocess_function_causal, \n",
    "        batched=True, \n",
    "        remove_columns=raw_datasets[\"train\"].column_names \n",
    "    )\n",
    "    print(\"\\næ•°æ®tokenizeå’Œé¢„å¤„ç†å®Œæˆ:\")\n",
    "    print(tokenized_datasets) \n",
    "\n",
    "    if tokenized_datasets and 'train' in tokenized_datasets and len(tokenized_datasets['train']) > 0:\n",
    "        print(f\"\\nTokenizeåçš„è®­ç»ƒé›†æ ·æœ¬ (æ£€æŸ¥input_idså’Œlabelsçš„å±è”½æƒ…å†µ):\")\n",
    "        sample_idx = 0 \n",
    "        if sample_idx < len(tokenized_datasets['train']) and sample_idx < len(raw_datasets['train']):\n",
    "            print(f\"  åŸå§‹/å¢å¼ºåè¾“å…¥æ–‡æœ¬ (text): {raw_datasets['train'][sample_idx]['text']}\") \n",
    "            print(f\"  åŸå§‹ç›®æ ‡è¾“å‡º (quadruples_str): {raw_datasets['train'][sample_idx]['quadruples_str']}\")\n",
    "            \n",
    "            tokenized_sample = tokenized_datasets['train'][sample_idx]\n",
    "            print(f\"\\n  Tokenized input_ids (å‰60): {tokenized_sample['input_ids'][:60]}\")\n",
    "            print(f\"  Decoded input_ids (å‰60): {tokenizer.decode(tokenized_sample['input_ids'][:60])}\")\n",
    "            \n",
    "            print(f\"\\n  Tokenized labels (å‰60, -100è¡¨ç¤ºå·²å±è”½): {tokenized_sample['labels'][:60]}\")\n",
    "            \n",
    "            first_label_idx = -1\n",
    "            for idx, lbl_id in enumerate(tokenized_sample['labels']):\n",
    "                if lbl_id != -100:\n",
    "                    first_label_idx = idx\n",
    "                    break\n",
    "            \n",
    "            if first_label_idx != -1:\n",
    "                decoded_label_part = tokenizer.decode([l for l in tokenized_sample['labels'][first_label_idx:] if l != -100])\n",
    "                print(f\"  Decoded labels from first non-masked token (éƒ¨åˆ†): {decoded_label_part}\")\n",
    "            else:\n",
    "                print(\"  æ³¨æ„ï¼šè¯¥æ ·æœ¬çš„æ‰€æœ‰æ ‡ç­¾éƒ½è¢«å±è”½äº†ã€‚\")\n",
    "                if raw_datasets['train'][sample_idx]['quadruples_str']: \n",
    "                     print(f\"  åŸå§‹ç›®æ ‡è¾“å‡ºéç©º ('{raw_datasets['train'][sample_idx]['quadruples_str']}'), ä½†æ‰€æœ‰æ ‡ç­¾è¢«å±è”½ï¼Œè¯·ä»”ç»†æ£€æŸ¥preprocess_function_causalä¸­çš„å±è”½é€»è¾‘ã€‚\")\n",
    "        else:\n",
    "            print(f\"è­¦å‘Šï¼šé€‰æ‹©çš„æ ·æœ¬ç´¢å¼• {sample_idx} è¶…å‡ºè®­ç»ƒé›†èŒƒå›´ã€‚\")\n",
    "    else:\n",
    "        print(\"\\nè­¦å‘Š: Tokenizeåçš„æ•°æ®é›†ä¸ºç©ºæˆ–ä¸å®Œæ•´ã€‚\")\n",
    "else:\n",
    "    print(\"é”™è¯¯: 'raw_datasets' æˆ–å…¶è®­ç»ƒé›†æœªå®šä¹‰/ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œtokenizeã€‚è¯·å…ˆæˆåŠŸæ‰§è¡Œæ•°æ®åŠ è½½å’Œï¼ˆå¯é€‰çš„ï¼‰å¢å¼ºå•å…ƒæ ¼ã€‚\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0a265c566257823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å‡†å¤‡ä» '/root/autodl-tmp/models/Qwen3-8B' åŠ è½½ç”¨äºå¾®è°ƒçš„Causal LM...\n",
      "å¾®è°ƒæ¨¡å‹å°†ä½¿ç”¨4-bité‡åŒ– (nf4) åŠ è½½ã€‚\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0aadf8863ae42a6b093ad7db3486f2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç”¨äºå¾®è°ƒçš„æ¨¡å‹ '/root/autodl-tmp/models/Qwen3-8B' åŠ è½½å®Œæˆã€‚\n",
      "å¾®è°ƒæ¨¡å‹é…ç½®çš„pad_token_idå·²è®¾ç½®ä¸ºtokenizerçš„pad_token_id: 151643\n",
      "\n",
      "ä¸ºå¾®è°ƒæ¨¡å‹å¯ç”¨LoRAã€‚\n",
      "æ£€æµ‹åˆ°å¾®è°ƒæ¨¡å‹å·²é‡åŒ–åŠ è½½ï¼Œå‡†å¤‡k-bitè®­ç»ƒ...\n",
      "å¾®è°ƒæ¨¡å‹å·²ä¸ºk-bitè®­ç»ƒå‡†å¤‡å°±ç»ªã€‚æ¢¯åº¦æ£€æŸ¥ç‚¹å°†å¯ç”¨ã€‚\n",
      "LoRAé…ç½®å·²åˆ›å»º:\n",
      "LoraConfig(task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=16, target_modules={'gate_proj', 'down_proj', 'k_proj', 'up_proj', 'v_proj', 'o_proj', 'q_proj'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)\n",
      "\n",
      "LoRAé€‚é…å™¨å·²åº”ç”¨åˆ°å¾®è°ƒæ¨¡å‹ã€‚\n",
      "trainable params: 43,646,976 || all params: 8,234,382,336 || trainable%: 0.5301\n",
      "å½“å‰å¾®è°ƒæ¨¡å‹æ‰€åœ¨è®¾å¤‡: cuda:0\n",
      "å¾®è°ƒæ¨¡å‹å±‚è®¾å¤‡åˆ†å¸ƒ: {'': 0}\n"
     ]
    }
   ],
   "source": [
    "# --- å•å…ƒæ ¼ 7: æ¨¡å‹åŠ è½½ä¸PEFT (LoRA) é…ç½® ğŸ§± ---\n",
    "# ä¸»å¾®è°ƒæ¨¡å‹ä½¿ç”¨ MODEL_NAME\n",
    "print(f\"å‡†å¤‡ä» '{MODEL_NAME}' åŠ è½½ç”¨äºå¾®è°ƒçš„Causal LM...\")\n",
    "\n",
    "bnb_config_finetune = None # ä¸ç”Ÿæˆå™¨LLMçš„bnb_configåŒºåˆ†å¼€\n",
    "if USE_QUANTIZATION: # ä½¿ç”¨ä¸»é…ç½®ä¸­çš„é‡åŒ–è®¾ç½®\n",
    "    if QUANTIZATION_TYPE == \"nf4\" or QUANTIZATION_TYPE == \"fp4\":\n",
    "        bnb_config_finetune = BitsAndBytesConfig(\n",
    "            load_in_4bit=True, bnb_4bit_quant_type=QUANTIZATION_TYPE, \n",
    "            bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_use_double_quant=True, \n",
    "        )\n",
    "        print(f\"å¾®è°ƒæ¨¡å‹å°†ä½¿ç”¨4-bité‡åŒ– ({QUANTIZATION_TYPE}) åŠ è½½ã€‚\")\n",
    "    elif QUANTIZATION_TYPE == \"int8\":\n",
    "        bnb_config_finetune = BitsAndBytesConfig(load_in_8bit=True)\n",
    "        print(\"å¾®è°ƒæ¨¡å‹å°†ä½¿ç”¨8-bité‡åŒ–åŠ è½½ã€‚\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config_finetune,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\" \n",
    ")\n",
    "print(f\"ç”¨äºå¾®è°ƒçš„æ¨¡å‹ '{MODEL_NAME}' åŠ è½½å®Œæˆã€‚\")\n",
    "\n",
    "if tokenizer.pad_token_id is not None and model.config.pad_token_id is None:\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    print(f\"å¾®è°ƒæ¨¡å‹é…ç½®çš„pad_token_idå·²è®¾ç½®ä¸ºtokenizerçš„pad_token_id: {tokenizer.pad_token_id}\")\n",
    "\n",
    "if hasattr(model, 'config') and model.config.model_type and \"qwen2\" in model.config.model_type.lower() and hasattr(model, 'enable_input_require_grads'):\n",
    "    try:\n",
    "        model.enable_input_require_grads()\n",
    "        print(\"å·²ä¸ºQwen2å¾®è°ƒæ¨¡å‹è°ƒç”¨ enable_input_require_grads()\")\n",
    "    except Exception as e_grad:\n",
    "        print(f\"ä¸ºQwen2å¾®è°ƒæ¨¡å‹è°ƒç”¨ enable_input_require_grads() æ—¶å‘ç”Ÿé”™è¯¯ (å¯èƒ½ä¸éœ€è¦æˆ–ä¸é€‚ç”¨): {e_grad}\")\n",
    "\n",
    "if USE_LORA:\n",
    "    print(\"\\nä¸ºå¾®è°ƒæ¨¡å‹å¯ç”¨LoRAã€‚\")\n",
    "    # training_args ç°åœ¨åº”è¯¥åœ¨Cell 9ä¸­å®šä¹‰ï¼Œè¿™é‡Œæˆ‘ä»¬å‡è®¾å®ƒä¼šè¢«å®šä¹‰\n",
    "    # ä¸ºäº†æ›´å®‰å…¨ï¼Œå¯ä»¥åœ¨ prepare_model_for_kbit_training è°ƒç”¨æ—¶ç›´æ¥ä¼ é€’å¸ƒå°”å€¼\n",
    "    use_grad_ckpt_for_lora = True # é»˜è®¤å¯ç”¨ï¼Œé™¤éåœ¨TrainingArgumentsä¸­æ˜¾å¼å…³é—­\n",
    "    if 'training_args' in locals() and hasattr(training_args, 'gradient_checkpointing'):\n",
    "        use_grad_ckpt_for_lora = training_args.gradient_checkpointing\n",
    "\n",
    "    if hasattr(model, \"is_loaded_in_8bit\") or hasattr(model, \"is_loaded_in_4bit\") or (USE_QUANTIZATION and bnb_config_finetune is not None):\n",
    "        print(\"æ£€æµ‹åˆ°å¾®è°ƒæ¨¡å‹å·²é‡åŒ–åŠ è½½ï¼Œå‡†å¤‡k-bitè®­ç»ƒ...\")\n",
    "        model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=use_grad_ckpt_for_lora) \n",
    "        print(f\"å¾®è°ƒæ¨¡å‹å·²ä¸ºk-bitè®­ç»ƒå‡†å¤‡å°±ç»ªã€‚æ¢¯åº¦æ£€æŸ¥ç‚¹å°†{'å¯ç”¨' if use_grad_ckpt_for_lora else 'ç¦ç”¨'}ã€‚\")\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        r=LORA_R, lora_alpha=LORA_ALPHA, target_modules=LORA_TARGET_MODULES, \n",
    "        lora_dropout=LORA_DROPOUT, bias=\"none\", task_type=TaskType.CAUSAL_LM \n",
    "    )\n",
    "    print(\"LoRAé…ç½®å·²åˆ›å»º:\")\n",
    "    print(lora_config)\n",
    "    \n",
    "    model = get_peft_model(model, lora_config) \n",
    "    print(\"\\nLoRAé€‚é…å™¨å·²åº”ç”¨åˆ°å¾®è°ƒæ¨¡å‹ã€‚\")\n",
    "    model.print_trainable_parameters() \n",
    "else:\n",
    "    print(\"\\næœªå¯ç”¨LoRAï¼Œå°†è¿›è¡Œå…¨å‚æ•°å¾®è°ƒã€‚\")\n",
    "\n",
    "print(f\"å½“å‰å¾®è°ƒæ¨¡å‹æ‰€åœ¨è®¾å¤‡: {model.device}\")\n",
    "if hasattr(model, 'hf_device_map'):\n",
    "    print(f\"å¾®è°ƒæ¨¡å‹å±‚è®¾å¤‡åˆ†å¸ƒ: {model.hf_device_map}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e8b9e84f0c0fe1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¯„ä¼°æŒ‡æ ‡ç›¸å…³å‡½æ•° (parse_quadruples, calculate_f1_metrics, compute_metrics_causal) å·²å®šä¹‰ã€‚\n",
      "DEBUG: å·²å°† parse_quadruples_fn_to_use æ›´æ–°ä¸ºå•å…ƒæ ¼8çš„å®Œæ•´å®šä¹‰ã€‚\n"
     ]
    }
   ],
   "source": [
    "# --- å•å…ƒæ ¼ 8: è¯„ä¼°æŒ‡æ ‡è®¡ç®—å‡½æ•° ğŸ“Š ---\n",
    "# (ä¿æŒä¸å˜ï¼Œä½†ç¡®ä¿ parse_quadruples åœ¨æ­¤å®šä¹‰æˆ–ä¹‹å‰å·²å®šä¹‰)\n",
    "def parse_quadruples(text_str): # ç¡®ä¿è¿™æ˜¯å®Œæ•´çš„å®šä¹‰\n",
    "    quadruples = [] \n",
    "    if not isinstance(text_str, str) or not text_str.strip(): \n",
    "        return [] \n",
    "        \n",
    "    parts = text_str.split(SEP_TOKEN) \n",
    "    for part_idx, part in enumerate(parts):\n",
    "        part_cleaned = part.strip() \n",
    "        \n",
    "        if part_cleaned.endswith(END_TOKEN):\n",
    "            part_cleaned = part_cleaned[:-len(END_TOKEN)].strip() \n",
    "        elif not part_cleaned and part_idx == len(parts) -1 : \n",
    "             continue\n",
    "\n",
    "        if not part_cleaned: \n",
    "            continue\n",
    "            \n",
    "        elements = [e.strip() for e in part_cleaned.split(\" | \")] # æ³¨æ„åˆ†éš”ç¬¦ä¸­çš„ç©ºæ ¼\n",
    "        \n",
    "        if len(elements) == 4:\n",
    "            quadruples.append(elements)\n",
    "    return quadruples\n",
    "\n",
    "def calculate_f1_metrics(preds_quads_list, labels_quads_list):\n",
    "    true_positives_hard = 0\n",
    "    predicted_positives_hard = 0 \n",
    "    actual_positives_hard = 0    \n",
    "    true_positives_soft = 0\n",
    "    predicted_positives_soft = 0\n",
    "    actual_positives_soft = 0\n",
    "\n",
    "    for pred_quads_for_sample, gold_quads_for_sample in zip(preds_quads_list, labels_quads_list):\n",
    "        predicted_positives_hard += len(pred_quads_for_sample)\n",
    "        actual_positives_hard += len(gold_quads_for_sample)\n",
    "        predicted_positives_soft += len(pred_quads_for_sample)\n",
    "        actual_positives_soft += len(gold_quads_for_sample)\n",
    "\n",
    "        matched_gold_indices_hard = set()\n",
    "        for p_quad in pred_quads_for_sample:\n",
    "            for i, g_quad in enumerate(gold_quads_for_sample):\n",
    "                if i in matched_gold_indices_hard: continue\n",
    "                if p_quad == g_quad: \n",
    "                    true_positives_hard += 1\n",
    "                    matched_gold_indices_hard.add(i)\n",
    "                    break \n",
    "        \n",
    "        matched_gold_indices_soft = set()\n",
    "        for p_quad in pred_quads_for_sample:\n",
    "            if len(p_quad) != 4: continue\n",
    "            for i, g_quad in enumerate(gold_quads_for_sample):\n",
    "                if len(g_quad) != 4: continue \n",
    "                if i in matched_gold_indices_soft: continue\n",
    "                if p_quad[2].strip().lower() == g_quad[2].strip().lower() and \\\n",
    "                   p_quad[3].strip().lower().startswith(g_quad[3].strip().lower().split(\" \")[0]): # æ¯”è¾ƒä¸»è¦éƒ¨åˆ†\n",
    "                    sim_target = difflib.SequenceMatcher(None, p_quad[0], g_quad[0]).ratio()\n",
    "                    sim_argument = difflib.SequenceMatcher(None, p_quad[1], g_quad[1]).ratio()\n",
    "                    if sim_target > 0.5 and sim_argument > 0.5: \n",
    "                        true_positives_soft += 1\n",
    "                        matched_gold_indices_soft.add(i)\n",
    "                        break \n",
    "\n",
    "    precision_hard = true_positives_hard / predicted_positives_hard if predicted_positives_hard > 0 else 0\n",
    "    recall_hard = true_positives_hard / actual_positives_hard if actual_positives_hard > 0 else 0\n",
    "    f1_hard = 2 * (precision_hard * recall_hard) / (precision_hard + recall_hard) if (precision_hard + recall_hard) > 0 else 0\n",
    "    precision_soft = true_positives_soft / predicted_positives_soft if predicted_positives_soft > 0 else 0\n",
    "    recall_soft = true_positives_soft / actual_positives_soft if actual_positives_soft > 0 else 0\n",
    "    f1_soft = 2 * (precision_soft * recall_soft) / (precision_soft + recall_soft) if (precision_soft + recall_soft) > 0 else 0\n",
    "    avg_f1 = (f1_hard + f1_soft) / 2\n",
    "    return {\n",
    "        \"f1_hard\": f1_hard, \"precision_hard\": precision_hard, \"recall_hard\": recall_hard,\n",
    "        \"f1_soft\": f1_soft, \"precision_soft\": precision_soft, \"recall_soft\": recall_soft,\n",
    "        \"avg_f1\": avg_f1\n",
    "    }\n",
    "\n",
    "def compute_metrics_causal(eval_preds):\n",
    "    generated_token_ids, label_ids_from_input = eval_preds \n",
    "    # label_ids_from_input åŒ…å«äº† -100 ç”¨äºå±è”½æç¤ºéƒ¨åˆ†\n",
    "    # generated_token_ids æ˜¯æ¨¡å‹ç”Ÿæˆçš„åºåˆ—ï¼Œå¯èƒ½ä¹ŸåŒ…å«æç¤ºéƒ¨åˆ†ï¼ˆå¦‚æœgenerateæœªæ­£ç¡®é…ç½®åªè¾“å‡ºæ–°tokenï¼‰\n",
    "    # é€šå¸¸ï¼ŒTrainerçš„generateä¼šå¤„ç†å¥½ï¼Œåªè¿”å›æ–°ç”Ÿæˆçš„tokensï¼Œæˆ–è€…æˆ‘ä»¬éœ€è¦ä»å®Œæ•´åºåˆ—ä¸­æå–\n",
    "\n",
    "    # å‡è®¾ generated_token_ids æ˜¯æ¨¡å‹æ–°ç”Ÿæˆçš„token (ä¸å«æç¤º)\n",
    "    # å¦‚æœå®ƒåŒ…å«äº†æç¤ºï¼Œæˆ‘ä»¬éœ€è¦ä»æ¨¡å‹è¾“å‡ºä¸­ç§»é™¤æç¤ºéƒ¨åˆ†\n",
    "    # decoded_preds_str = tokenizer.batch_decode(generated_token_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    \n",
    "    # ä¸€ä¸ªæ›´é²æ£’çš„æ–¹æ³•æ˜¯ï¼Œæˆ‘ä»¬çŸ¥é“label_ids_from_inputæ˜¯å®Œæ•´çš„ï¼ŒåŒ…å«-100\n",
    "    # è€Œ generated_token_ids æ˜¯æ¨¡å‹é’ˆå¯¹è¿™äº›è¾“å…¥ç”Ÿæˆçš„å®Œæ•´åºåˆ—ï¼ˆæç¤º+ç­”æ¡ˆï¼‰\n",
    "    # æˆ‘ä»¬éœ€è¦ä» generated_token_ids ä¸­æå–ç­”æ¡ˆéƒ¨åˆ†ï¼Œæˆ–è€…ä» decoded_preds_str ä¸­æå–\n",
    "\n",
    "    # æ–¹æ¡ˆ1: å‡è®¾ generated_token_ids æ˜¯å®Œæ•´çš„ï¼ˆæç¤º+ç­”æ¡ˆï¼‰\n",
    "    decoded_preds_full_str = tokenizer.batch_decode(generated_token_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    \n",
    "    # ä»è§£ç çš„å®Œæ•´é¢„æµ‹ä¸­æå–çœŸå®çš„ç­”æ¡ˆéƒ¨åˆ†\n",
    "    pred_answer_strs = []\n",
    "    keyword_separator = \"æ¨¡å‹è¾“å‡ºï¼š\" # ä¸PROMPT_TEMPLATEä¸€è‡´\n",
    "    for full_pred_text in decoded_preds_full_str:\n",
    "        if keyword_separator in full_pred_text:\n",
    "            pred_answer_strs.append(full_pred_text.split(keyword_separator, 1)[-1].strip())\n",
    "        else: # å¦‚æœæ¨¡å‹æ²¡æŒ‰å¥—è·¯å‡ºç‰Œ\n",
    "            # print(f\"è­¦å‘Š: é¢„æµ‹ç»“æœä¸­æœªæ‰¾åˆ°åˆ†éš”ç¬¦ '{keyword_separator}'. Full pred: '{full_pred_text[:100]}...'\")\n",
    "            # å°è¯•ç§»é™¤å·²çŸ¥çš„æç¤ºå¤´ï¼ˆè¿™æ¯”è¾ƒè„†å¼±ï¼‰\n",
    "            # prompt_head_approx = PROMPT_TEMPLATE.split(\"{input_text}\")[0].split(\"<<SYS>>\")[-1].strip() # å–ç³»ç»Ÿæç¤ºä¹‹åçš„éƒ¨åˆ†\n",
    "            # if full_pred_text.startswith(prompt_head_approx):\n",
    "            #     pred_answer_strs.append(full_pred_text[len(prompt_head_approx):].strip())\n",
    "            # else:\n",
    "            pred_answer_strs.append(full_pred_text) # åå¤‡ï¼šä½¿ç”¨å…¨éƒ¨ï¼Œå¯èƒ½åŒ…å«æç¤º\n",
    "\n",
    "    # è§£ç çœŸå®æ ‡ç­¾ï¼ˆç­”æ¡ˆéƒ¨åˆ†ï¼‰\n",
    "    processed_label_ids = np.where(label_ids_from_input != -100, label_ids_from_input, tokenizer.pad_token_id)\n",
    "    decoded_labels_full_str = tokenizer.batch_decode(processed_label_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    actual_target_strs = []\n",
    "    for full_label_text in decoded_labels_full_str:\n",
    "        if keyword_separator in full_label_text:\n",
    "            actual_target_strs.append(full_label_text.split(keyword_separator, 1)[-1].strip())\n",
    "        else:\n",
    "            # print(f\"è­¦å‘Š: è§£ç çš„æ ‡ç­¾ä¸­æœªæ‰¾åˆ°åˆ†éš”ç¬¦ '{keyword_separator}'. Full label: '{full_label_text[:100]}...'\")\n",
    "            actual_target_strs.append(\"\") \n",
    "\n",
    "    pred_quads_list = [parse_quadruples(p_str) for p_str in pred_answer_strs]\n",
    "    label_quads_list = [parse_quadruples(l_str) for l_str in actual_target_strs] \n",
    "    \n",
    "    results = calculate_f1_metrics(pred_quads_list, label_quads_list)\n",
    "    return results\n",
    "\n",
    "print(\"è¯„ä¼°æŒ‡æ ‡ç›¸å…³å‡½æ•° (parse_quadruples, calculate_f1_metrics, compute_metrics_causal) å·²å®šä¹‰ã€‚\")\n",
    "# æ›´æ–° parse_quadruples_fn_to_use (å¦‚æœå•å…ƒæ ¼3.6åœ¨å•å…ƒæ ¼8ä¹‹å‰è¿è¡Œäº†)\n",
    "if 'parse_quadruples_fn_to_use' in globals() and parse_quadruples_fn_to_use.__name__ == 'parse_quadruples_placeholder':\n",
    "    parse_quadruples_fn_to_use = parse_quadruples\n",
    "    print(\"DEBUG: å·²å°† parse_quadruples_fn_to_use æ›´æ–°ä¸ºå•å…ƒæ ¼8çš„å®Œæ•´å®šä¹‰ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28b1e756d2e5eb8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: æœ€ç»ˆç¡®è®¤çš„ CALCULATED_STEPS_PER_EPOCH: 500\n",
      "è®­ç»ƒå‚æ•° (TrainingArguments) é…ç½®å®Œæˆã€‚è¯„ä¼°å’Œä¿å­˜ç­–ç•¥å‡è®¾ç½®ä¸º 'steps'ï¼Œæ¯ 500 æ­¥æ‰§è¡Œä¸€æ¬¡ã€‚\n",
      "æ•°æ®æ•´ç†å™¨ (DataCollatorForSeq2Seq) åˆå§‹åŒ–å®Œæˆã€‚\n"
     ]
    }
   ],
   "source": [
    "# --- å•å…ƒæ ¼ 9: è®­ç»ƒå‚æ•°é…ç½®ä¸æ•°æ®æ•´ç†å™¨ (æœ€ç»ˆç¡®è®¤ç‰ˆ) ğŸ“‹ ---\n",
    "# (ä¿æŒä¸æ‚¨ç¡®è®¤å¯ç”¨çš„ç‰ˆæœ¬ä¸€è‡´)\n",
    "if 'CALCULATED_STEPS_PER_EPOCH' not in locals(): CALCULATED_STEPS_PER_EPOCH = 500 # å®‰å…¨é»˜è®¤å€¼\n",
    "print(f\"DEBUG: æœ€ç»ˆç¡®è®¤çš„ CALCULATED_STEPS_PER_EPOCH: {CALCULATED_STEPS_PER_EPOCH}\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    do_eval=True,                 \n",
    "    eval_strategy=\"steps\",        \n",
    "    eval_steps=CALCULATED_STEPS_PER_EPOCH, \n",
    "    save_strategy=\"steps\",        \n",
    "    save_steps=CALCULATED_STEPS_PER_EPOCH, \n",
    "    save_total_limit=2, \n",
    "    logging_dir=f\"{OUTPUT_DIR}/logs\", \n",
    "    logging_strategy=\"steps\", \n",
    "    logging_steps=max(1, CALCULATED_STEPS_PER_EPOCH // 10 if CALCULATED_STEPS_PER_EPOCH > 10 else 50),\n",
    "    load_best_model_at_end=True, \n",
    "    metric_for_best_model=\"avg_f1\", \n",
    "    greater_is_better=True,      \n",
    "    fp16=(torch.cuda.is_available() and not USE_QUANTIZATION), # fp16 ä¸ 4-bit/8-bit é‡åŒ–é€šå¸¸ä¸ä¸€èµ·ç”¨\n",
    "    bf16=(torch.cuda.is_bf16_supported() and not USE_QUANTIZATION), # bf16 åŒä¸Š\n",
    "    lr_scheduler_type=LR_SCHEDULER_TYPE, \n",
    "    warmup_ratio=WARMUP_RATIO,           \n",
    "    report_to=[\"tensorboard\"], \n",
    "    seed=SEED,                 \n",
    "    optim=\"paged_adamw_8bit\" if USE_QUANTIZATION else \"adamw_torch\",\n",
    "    remove_unused_columns=True, # æ¨èè®¾ç½®ä¸ºTrue\n",
    "    gradient_checkpointing=True, # ä¸ºèŠ‚çœæ˜¾å­˜å¯ç”¨\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False}, # æ¨èçš„æ¢¯åº¦æ£€æŸ¥ç‚¹è®¾ç½®\n",
    ")\n",
    "print(f\"è®­ç»ƒå‚æ•° (TrainingArguments) é…ç½®å®Œæˆã€‚è¯„ä¼°å’Œä¿å­˜ç­–ç•¥å‡è®¾ç½®ä¸º 'steps'ï¼Œæ¯ {CALCULATED_STEPS_PER_EPOCH} æ­¥æ‰§è¡Œä¸€æ¬¡ã€‚\")\n",
    "\n",
    "if 'training_args' in locals() and training_args is not None:\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer, model=model, label_pad_token_id=-100, \n",
    "        pad_to_multiple_of=8 if (training_args.fp16 or training_args.bf16) else None \n",
    "    )\n",
    "    print(\"æ•°æ®æ•´ç†å™¨ (DataCollatorForSeq2Seq) åˆå§‹åŒ–å®Œæˆã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecd26deb3e217169",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1325/2158355083.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer åˆå§‹åŒ–å®Œæˆã€‚\n"
     ]
    }
   ],
   "source": [
    "# --- å•å…ƒæ ¼ 10: åˆå§‹åŒ– Trainer ğŸ‘¨â€ğŸ« ---\n",
    "# (ä¿æŒä¸å˜)\n",
    "trainer = Trainer(\n",
    "    model=model, args=training_args,                  \n",
    "    train_dataset=tokenized_datasets[\"train\"] if 'tokenized_datasets' in locals() and tokenized_datasets and \"train\" in tokenized_datasets else None, \n",
    "    eval_dataset=tokenized_datasets[\"validation\"] if 'tokenized_datasets' in locals() and tokenized_datasets and \"validation\" in tokenized_datasets else None, \n",
    "    tokenizer=tokenizer, data_collator=data_collator, compute_metrics=compute_metrics_causal, \n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.001)] \n",
    ")\n",
    "print(\"Trainer åˆå§‹åŒ–å®Œæˆã€‚\")\n",
    "if not ('tokenized_datasets' in locals() and tokenized_datasets and \"train\" in tokenized_datasets and tokenized_datasets[\"train\"]):\n",
    "    print(\"è­¦å‘Š: Trainerçš„è®­ç»ƒé›†æœªæ­£ç¡®è®¾ç½®ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a620759a4f617c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å³å°†å¼€å§‹æ¨¡å‹è®­ç»ƒ...\n",
      "æ¨¡å‹è¯„ä¼°æ—¶å°†ä½¿ç”¨ä»¥ä¸‹ç”Ÿæˆé…ç½®: num_beams=3, max_new_tokens=256\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='501' max='1350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 501/1350 3:02:47 < 5:11:00, 0.05 it/s, Epoch 1.11/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  7/200 00:04 < 02:21, 1.36 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: CUDA out of memory. Tried to allocate 6.21 GiB. GPU 0 has a total capacity of 23.55 GiB of which 4.89 GiB is free. Including non-PyTorch memory, this process has 0 bytes memory in use. Of the allocated memory 14.41 GiB is allocated by PyTorch, and 3.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1325/2744667030.py\", line 21, in <module>\n",
      "    train_result = trainer.train()\n",
      "                   ^^^^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/transformers/trainer.py\", line 2240, in train\n",
      "    return inner_training_loop(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/transformers/trainer.py\", line 2622, in _inner_training_loop\n",
      "    self._maybe_log_save_evaluate(\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/transformers/trainer.py\", line 3095, in _maybe_log_save_evaluate\n",
      "    metrics = self._evaluate(trial, ignore_keys_for_eval)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/transformers/trainer.py\", line 3044, in _evaluate\n",
      "    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/transformers/trainer.py\", line 4173, in evaluate\n",
      "    output = eval_loop(\n",
      "             ^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/transformers/trainer.py\", line 4395, in evaluation_loop\n",
      "    all_preds.add(logits)\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/transformers/trainer_pt_utils.py\", line 316, in add\n",
      "    self.tensors = nested_concat(self.tensors, tensors, padding_index=self.padding_index)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/transformers/trainer_pt_utils.py\", line 130, in nested_concat\n",
      "    return torch_pad_and_concatenate(tensors, new_tensors, padding_index=padding_index)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/transformers/trainer_pt_utils.py\", line 94, in torch_pad_and_concatenate\n",
      "    result = tensor1.new_full(new_shape, padding_index)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.21 GiB. GPU 0 has a total capacity of 23.55 GiB of which 4.89 GiB is free. Including non-PyTorch memory, this process has 0 bytes memory in use. Of the allocated memory 14.41 GiB is allocated by PyTorch, and 3.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "# --- å•å…ƒæ ¼ 11: å¼€å§‹æ¨¡å‹è®­ç»ƒ ğŸš€ ---\n",
    "# (ä¿æŒä¸å˜)\n",
    "print(\"å³å°†å¼€å§‹æ¨¡å‹è®­ç»ƒ...\")\n",
    "if trainer.train_dataset is None:\n",
    "    print(\"é”™è¯¯: è®­ç»ƒæ•°æ®é›†æœªè®¾ç½®ï¼Œæ— æ³•å¼€å§‹è®­ç»ƒã€‚\")\n",
    "else:\n",
    "    try:\n",
    "        if model.generation_config is None: \n",
    "            model.generation_config = GenerationConfig.from_model_config(model.config)\n",
    "            print(\"å·²ä¸ºæ¨¡å‹è®¾ç½®é»˜è®¤çš„GenerationConfigã€‚\")\n",
    "        \n",
    "        model.generation_config.max_new_tokens = MAX_TARGET_LENGTH \n",
    "        model.generation_config.num_beams = 3 \n",
    "        model.generation_config.early_stopping = True\n",
    "        if tokenizer.pad_token_id is not None:\n",
    "            model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "        if tokenizer.eos_token_id is not None: # ç¡®ä¿eos_token_idä¹Ÿè®¾ç½®\n",
    "             model.generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "        print(f\"æ¨¡å‹è¯„ä¼°æ—¶å°†ä½¿ç”¨ä»¥ä¸‹ç”Ÿæˆé…ç½®: num_beams={model.generation_config.num_beams}, max_new_tokens={model.generation_config.max_new_tokens}\")\n",
    "        train_result = trainer.train()\n",
    "        print(\"\\næ¨¡å‹è®­ç»ƒå®Œæˆ!\")\n",
    "        print(\"æ­£åœ¨ä¿å­˜æ¨¡å‹ (LoRA adapter)...\")\n",
    "        trainer.save_model(OUTPUT_DIR) \n",
    "        tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "        print(f\"æ¨¡å‹é€‚é…å™¨å’Œtokenizerå·²ä¿å­˜åˆ° '{OUTPUT_DIR}'ã€‚\")\n",
    "        metrics = train_result.metrics\n",
    "        trainer.log_metrics(\"train\", metrics) \n",
    "        trainer.save_metrics(\"train\", metrics) \n",
    "        trainer.save_state() \n",
    "        print(\"\\nè®­ç»ƒæŒ‡æ ‡å·²è®°å½•å’Œä¿å­˜ã€‚\")\n",
    "        print(f\"è®­ç»ƒç»Ÿè®¡æŒ‡æ ‡: {metrics}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\næ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cec30d2c052b7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç”¨äºé¢„æµ‹çš„æ¨¡å‹å·²å‡†å¤‡å¥½ï¼Œå½“å‰è®¾å¤‡: cuda:0\n",
      "é¢„æµ‹/æ¨ç†ç›¸å…³å‡½æ•° (predict_quadruples_causal) å·²å®šä¹‰ã€‚\n"
     ]
    }
   ],
   "source": [
    "# --- å•å…ƒæ ¼ 12: é¢„æµ‹/æ¨ç†å‡½æ•°è®¾ç½® ğŸ”® ---\n",
    "# (ä¿æŒä¸å˜ï¼Œä½†ç¡®ä¿ MAX_INPUT_LENGTH å’Œ MAX_TARGET_LENGTH ä»Cell 2æ­£ç¡®ä¼ é€’)\n",
    "model_to_predict = trainer.model if 'trainer' in locals() and hasattr(trainer, 'model') else None\n",
    "if model_to_predict:\n",
    "    model_to_predict.eval() \n",
    "    print(f\"ç”¨äºé¢„æµ‹çš„æ¨¡å‹å·²å‡†å¤‡å¥½ï¼Œå½“å‰è®¾å¤‡: {model_to_predict.device}\")\n",
    "else:\n",
    "    print(\"è­¦å‘Š: 'trainer.model' æœªæ‰¾åˆ°ï¼Œæ— æ³•è®¾ç½® model_to_predictã€‚ç¤ºä¾‹é¢„æµ‹å’Œæäº¤æ–‡ä»¶ç”Ÿæˆå¯èƒ½å¤±è´¥ã€‚\")\n",
    "\n",
    "\n",
    "def predict_quadruples_causal(text_list, model, tokenizer_pred, max_input_len_pred, max_target_gen_len_pred):\n",
    "    parsed_results_list = []\n",
    "    if model is None or tokenizer_pred is None:\n",
    "        print(\"é”™è¯¯: é¢„æµ‹æ‰€éœ€çš„æ¨¡å‹æˆ–tokenizeræœªæä¾›ã€‚\")\n",
    "        return [{\"original_text\": t, \"extracted_answer_string\": \"ERROR: Model/Tokenizer missing\", \"parsed_quadruples\": []} for t in text_list]\n",
    "\n",
    "    for text_input in text_list:\n",
    "        prompt_for_inference = PROMPT_TEMPLATE.format(input_text=text_input)\n",
    "        \n",
    "        # ç¡®ä¿ max_length å¯¹äºæç¤ºæ˜¯åˆç†çš„\n",
    "        max_prompt_len = max_input_len_pred - max_target_gen_len_pred\n",
    "        if max_prompt_len <= 0 : max_prompt_len = max_input_len_pred // 2 # è‡³å°‘ç»™æç¤ºä¸€åŠç©ºé—´\n",
    "\n",
    "        inputs = tokenizer_pred(\n",
    "            prompt_for_inference, return_tensors=\"pt\", truncation=True, \n",
    "            max_length=max_prompt_len, padding=False \n",
    "        ).to(model.device) \n",
    "\n",
    "        with torch.no_grad():\n",
    "            # å¤åˆ¶å¹¶æ›´æ–°ç”Ÿæˆé…ç½®ï¼Œä»¥é˜²ä¿®æ”¹å…¨å±€é…ç½®\n",
    "            current_gen_config = GenerationConfig(**model.generation_config.to_dict())\n",
    "            current_gen_config.max_new_tokens = max_target_gen_len_pred\n",
    "            # num_beams, early_stopping ç­‰åº”å·²åœ¨ model.generation_config ä¸­è®¾ç½®\n",
    "\n",
    "            outputs = model.generate(**inputs, generation_config=current_gen_config)\n",
    "        \n",
    "        full_generated_text = tokenizer_pred.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        \n",
    "        answer_part_str = \"\"\n",
    "        keyword_separator = \"æ¨¡å‹è¾“å‡ºï¼š\" \n",
    "        split_parts = full_generated_text.split(keyword_separator, 1)\n",
    "        if len(split_parts) > 1:\n",
    "            answer_part_str = split_parts[1].strip()\n",
    "        else: \n",
    "            prompt_head_for_removal = PROMPT_TEMPLATE.format(input_text=text_input).split(keyword_separator)[0] + keyword_separator\n",
    "            decoded_prompt_head = tokenizer_pred.decode(tokenizer_pred.encode(prompt_head_for_removal.split(\"ç”¨æˆ·æä¾›çš„æ–‡æœ¬å¦‚ä¸‹ï¼š\")[0], add_special_tokens=False), skip_special_tokens=True) # å°è¯•è§£ç æç¤ºå¤´éƒ¨åˆ†\n",
    "            \n",
    "            # è¿™æ˜¯ä¸€ä¸ªæ›´å¤æ‚çš„å°è¯•ï¼Œè¯•å›¾ç§»é™¤æç¤ºéƒ¨åˆ†\n",
    "            # å¦‚æœè§£ç åçš„å®Œæ•´æ–‡æœ¬ä»¥è§£ç åçš„æç¤ºå¤´å¼€å§‹ï¼Œåˆ™ç§»é™¤å®ƒ\n",
    "            # è¿™éå¸¸ä¾èµ–äºè§£ç çš„ä¸€è‡´æ€§\n",
    "            # if full_generated_text.startswith(decoded_prompt_head.strip()):\n",
    "            #    answer_part_str = full_generated_text[len(decoded_prompt_head.strip()):].strip()\n",
    "            #    if answer_part_str.startswith(keyword_separator): # å†æ¬¡æ£€æŸ¥\n",
    "            #         answer_part_str = answer_part_str[len(keyword_separator):].strip()\n",
    "            # else:\n",
    "            # print(f\"è­¦å‘Šï¼šæ— æ³•åœ¨ç”Ÿæˆæ–‡æœ¬ä¸­é€šè¿‡å…³é”®è¯ '{keyword_separator}' å®šä½ç­”æ¡ˆï¼Œä¸”æç¤ºå¤´ä¸åŒ¹é…ã€‚Full: '{full_generated_text[:200]}...'\")\n",
    "            answer_part_str = full_generated_text # ä½œä¸ºæœ€åçš„æ‰‹æ®µï¼Œå¯èƒ½åŒ…å«æç¤º\n",
    "\n",
    "        parsed_quads = parse_quadruples(answer_part_str) # ä½¿ç”¨Cell 8å®šä¹‰çš„parse_quadruples\n",
    "        parsed_results_list.append({\n",
    "            \"original_text\": text_input, \"full_generated_text\": full_generated_text, \n",
    "            \"extracted_answer_string\": answer_part_str, \"parsed_quadruples\": parsed_quads\n",
    "        })\n",
    "    return parsed_results_list\n",
    "\n",
    "print(\"é¢„æµ‹/æ¨ç†ç›¸å…³å‡½æ•° (predict_quadruples_causal) å·²å®šä¹‰ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5dd17ab5c4cc693f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "å¼€å§‹è¿è¡Œç¤ºä¾‹é¢„æµ‹...\n",
      "\n",
      "ç¤ºä¾‹é¢„æµ‹ç»“æœ:\n",
      "åŸå§‹æ–‡æœ¬: é‚£äº›åŒæ€§æ‹çœŸæ¶å¿ƒï¼Œè´¥åç¤¾ä¼šé£æ°”ã€‚\n",
      "æå–ç­”æ¡ˆ: åŒæ€§æ‹ | çœŸæ¶å¿ƒï¼Œè´¥åç¤¾ä¼šé£æ°” | LGBTQ | hate [END]\n",
      "è§£æå››å…ƒç»„: [['åŒæ€§æ‹', 'çœŸæ¶å¿ƒï¼Œè´¥åç¤¾ä¼šé£æ°”', 'LGBTQ', 'hate']]\n",
      "------------------------------\n",
      "åŸå§‹æ–‡æœ¬: è¿™é“èœå‘³é“ä¸é”™ï¼Œä¸‹æ¬¡è¿˜æ¥ã€‚\n",
      "æå–ç­”æ¡ˆ: è¿™é“èœ | å‘³é“ä¸é”™ | non-hate | non-hate [END]\n",
      "è§£æå››å…ƒç»„: [['è¿™é“èœ', 'å‘³é“ä¸é”™', 'non-hate', 'non-hate']]\n",
      "------------------------------\n",
      "åŸå§‹æ–‡æœ¬: ä¸Šæµ·äººå°±æ˜¯æ’å¤–ï¼Œçœ‹ä¸èµ·å¤–åœ°äººã€‚\n",
      "æå–ç­”æ¡ˆ: ä¸Šæµ·äºº | æ’å¤–ï¼Œçœ‹ä¸èµ·å¤–åœ°äºº | Region | hate [END]\n",
      "è§£æå››å…ƒç»„: [['ä¸Šæµ·äºº', 'æ’å¤–ï¼Œçœ‹ä¸èµ·å¤–åœ°äºº', 'Region', 'hate']]\n",
      "------------------------------\n",
      "åŸå§‹æ–‡æœ¬: é»‘äººéƒ½æ˜¯ç½ªçŠ¯ï¼Œåº”è¯¥è¢«èµ¶èµ°ã€‚\n",
      "æå–ç­”æ¡ˆ: é»‘äºº | ç½ªçŠ¯ | Racism | hate [END]\n",
      "è§£æå››å…ƒç»„: [['é»‘äºº', 'ç½ªçŠ¯', 'Racism', 'hate']]\n",
      "------------------------------\n",
      "åŸå§‹æ–‡æœ¬: ä½ å¯çœŸæ˜¯å¤´è ¢é©´ï¼Œè¿™éƒ½åšä¸å¥½ã€‚\n",
      "æå–ç­”æ¡ˆ: ä½  | å¤´è ¢é©´ | non-hate | non-hate [END]\n",
      "è§£æå››å…ƒç»„: [['ä½ ', 'å¤´è ¢é©´', 'non-hate', 'non-hate']]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# (ä¿æŒä¸å˜)\n",
    "sample_test_texts_for_prediction = [\n",
    "    \"é‚£äº›åŒæ€§æ‹çœŸæ¶å¿ƒï¼Œè´¥åç¤¾ä¼šé£æ°”ã€‚\", \"è¿™é“èœå‘³é“ä¸é”™ï¼Œä¸‹æ¬¡è¿˜æ¥ã€‚\",\n",
    "    \"ä¸Šæµ·äººå°±æ˜¯æ’å¤–ï¼Œçœ‹ä¸èµ·å¤–åœ°äººã€‚\", \"é»‘äººéƒ½æ˜¯ç½ªçŠ¯ï¼Œåº”è¯¥è¢«èµ¶èµ°ã€‚\",\n",
    "    \"ä½ å¯çœŸæ˜¯å¤´è ¢é©´ï¼Œè¿™éƒ½åšä¸å¥½ã€‚\"\n",
    "]\n",
    "print(\"\\nå¼€å§‹è¿è¡Œç¤ºä¾‹é¢„æµ‹...\")\n",
    "if 'model_to_predict' in locals() and model_to_predict is not None:\n",
    "    predictions = predict_quadruples_causal(\n",
    "        sample_test_texts_for_prediction, model_to_predict, tokenizer,\n",
    "        MAX_INPUT_LENGTH, MAX_TARGET_LENGTH \n",
    "    )\n",
    "    print(\"\\nç¤ºä¾‹é¢„æµ‹ç»“æœ:\")\n",
    "    for item in predictions:\n",
    "        print(f\"åŸå§‹æ–‡æœ¬: {item['original_text']}\")\n",
    "        print(f\"æå–ç­”æ¡ˆ: {item['extracted_answer_string']}\")\n",
    "        print(f\"è§£æå››å…ƒç»„: {item['parsed_quadruples']}\")\n",
    "        print(\"-\" * 30)\n",
    "else:\n",
    "    print(\"é”™è¯¯: 'model_to_predict' æœªå®šä¹‰æˆ–ä¸ºNoneã€‚æ— æ³•è¿è¡Œç¤ºä¾‹é¢„æµ‹ã€‚\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51016ed5040cd5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "å¼€å§‹å¤„ç†å®˜æ–¹æµ‹è¯•æ–‡ä»¶: ./test1.json\n",
      "æ­£åœ¨ä» './test1.json' åŠ è½½å®˜æ–¹æµ‹è¯•æ•°æ®...\n",
      "æˆåŠŸä» './test1.json' åŠ è½½äº† 2000 æ¡æµ‹è¯•æ•°æ®ã€‚\n",
      "å¼€å§‹å¯¹ 2000 æ¡æµ‹è¯•æ•°æ®è¿›è¡Œé¢„æµ‹ (æ‰¹æ¬¡å¤§å°: 3)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å®˜æ–¹æµ‹è¯•é›†é¢„æµ‹: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 667/667 [2:18:28<00:00, 12.46s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "æäº¤æ–‡ä»¶å·²æˆåŠŸç”Ÿæˆ: ./newsubmission.txt\n",
      "è¯¥æ–‡ä»¶åŒ…å« 2000 è¡Œé¢„æµ‹ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- å•å…ƒæ ¼ 14: åŠ è½½å®˜æ–¹æµ‹è¯•æ•°æ®å¹¶ç”Ÿæˆæäº¤æ–‡ä»¶ ğŸ“¤ ---\n",
    "# (ä¿æŒä¸å˜ï¼Œä½†ç¡®ä¿è·¯å¾„å’Œå˜é‡åæ­£ç¡®)\n",
    "# official_test_file_path = \"/kaggle/input/nlptrain/test1.json\" # æ‚¨çš„æµ‹è¯•æ–‡ä»¶è·¯å¾„\n",
    "# official_test_file_path = \"./test1.json\" # å‡è®¾åœ¨å½“å‰ç›®å½•\n",
    "# official_test_file_path = \"./test2.json\" # æˆ–è€… test2.json\n",
    "\n",
    "# ç¡®ä¿ä»¥ä¸‹å˜é‡å·²å®šä¹‰ï¼š\n",
    "# official_test_file_path, model_to_predict, tokenizer, EVAL_BATCH_SIZE, MAX_INPUT_LENGTH, MAX_TARGET_LENGTH\n",
    "\n",
    "# ç¤ºä¾‹è·¯å¾„ï¼Œè¯·æ ¹æ®æ‚¨çš„å®é™…æƒ…å†µä¿®æ”¹\n",
    "# official_test_file_path_to_use = \"./test1.json\" \n",
    "import json # ç¡®ä¿å¯¼å…¥jsonåº“\n",
    "import os   # ç¡®ä¿å¯¼å…¥osåº“\n",
    "\n",
    "# --- å¦‚ä½•åŠ è½½å®˜æ–¹æµ‹è¯•æ•°æ®å¹¶ç”Ÿæˆæäº¤æ–‡ä»¶çš„ç¤ºä¾‹ ---\n",
    "\n",
    "def load_official_test_data(file_path):\n",
    "    \"\"\"\n",
    "    åŠ è½½å®˜æ–¹æµ‹è¯•æ•°æ®ã€‚\n",
    "    å‡è®¾æ–‡ä»¶æ˜¯ä¸€ä¸ªJSONï¼Œå…¶é¡¶çº§ç»“æ„æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œåˆ—è¡¨ä¸­çš„æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªåŒ…å« \"id\" å’Œ \"content\" é”®çš„å­—å…¸ã€‚\n",
    "    \n",
    "    å‚æ•°:\n",
    "    - file_path (str): æµ‹è¯•æ•°æ®JSONæ–‡ä»¶çš„è·¯å¾„ã€‚\n",
    "    \n",
    "    è¿”å›:\n",
    "    - list: åŒ…å«æ‰€æœ‰ \"content\" å­—ç¬¦ä¸²çš„åˆ—è¡¨ã€‚\n",
    "    - list: åŒ…å«æ‰€æœ‰å¯¹åº” \"id\" çš„åˆ—è¡¨ (å¯é€‰, å¦‚æœéœ€è¦idè¿›è¡Œæ˜ å°„æˆ–è°ƒè¯•)ã€‚\n",
    "    \"\"\"\n",
    "    texts_to_predict = []\n",
    "    ids_from_test_data = [] # å¯é€‰ï¼Œç”¨äºè¿½è¸ªID\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"é”™è¯¯: æµ‹è¯•æ–‡ä»¶ '{file_path}' æœªæ‰¾åˆ°ã€‚\")\n",
    "        return texts_to_predict, ids_from_test_data # è¿”å›ç©ºåˆ—è¡¨\n",
    "\n",
    "    print(f\"æ­£åœ¨ä» '{file_path}' åŠ è½½å®˜æ–¹æµ‹è¯•æ•°æ®...\")\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f) # æ•´ä¸ªæ–‡ä»¶æ˜¯ä¸€ä¸ªJSONåˆ—è¡¨\n",
    "            if not isinstance(data, list):\n",
    "                print(f\"é”™è¯¯: æµ‹è¯•æ–‡ä»¶ '{file_path}' çš„é¡¶çº§ç»“æ„ä¸æ˜¯ä¸€ä¸ªåˆ—è¡¨ã€‚è¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼ã€‚\")\n",
    "                return texts_to_predict, ids_from_test_data\n",
    "\n",
    "            for item_num, item in enumerate(data, 1):\n",
    "                if isinstance(item, dict) and \"content\" in item and \"id\" in item:\n",
    "                    texts_to_predict.append(item[\"content\"])\n",
    "                    ids_from_test_data.append(item[\"id\"])\n",
    "                else:\n",
    "                    print(f\"è­¦å‘Š: æµ‹è¯•æ–‡ä»¶ '{file_path}' ä¸­çš„ç¬¬ {item_num} é¡¹æ ¼å¼ä¸æ­£ç¡®æˆ–ç¼ºå°‘ 'id'/'content' é”®ï¼Œå·²è·³è¿‡: {item}\")\n",
    "        \n",
    "        print(f\"æˆåŠŸä» '{file_path}' åŠ è½½äº† {len(texts_to_predict)} æ¡æµ‹è¯•æ•°æ®ã€‚\")\n",
    "\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"é”™è¯¯: è§£ææµ‹è¯•æ–‡ä»¶ '{file_path}' æ—¶å‘ç”ŸJSONè§£ç é”™è¯¯ã€‚è¯·æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸ºæœ‰æ•ˆçš„JSONæ ¼å¼ã€‚\")\n",
    "    except Exception as e:\n",
    "        print(f\"åŠ è½½æµ‹è¯•æ–‡ä»¶ '{file_path}' æ—¶å‘ç”Ÿå…¶ä»–é”™è¯¯: {e}\")\n",
    "        \n",
    "    return texts_to_predict, ids_from_test_data\n",
    "\n",
    "official_test_file_path_to_use = \"./test1.json\" # æˆ–æ‚¨çš„ test2.json è·¯å¾„\n",
    "\n",
    "if 'model_to_predict' not in locals() or model_to_predict is None:\n",
    "    print(\"é”™è¯¯: 'model_to_predict' æœªå®šä¹‰ã€‚æ— æ³•è¿›è¡Œå®˜æ–¹æµ‹è¯•æ•°æ®é¢„æµ‹ã€‚\")\n",
    "elif 'tokenizer' not in locals() or tokenizer is None:\n",
    "    print(\"é”™è¯¯: 'tokenizer' æœªå®šä¹‰ã€‚æ— æ³•è¿›è¡Œå®˜æ–¹æµ‹è¯•æ•°æ®é¢„æµ‹ã€‚\")\n",
    "elif not os.path.exists(official_test_file_path_to_use):\n",
    "    print(f\"é”™è¯¯: æµ‹è¯•æ–‡ä»¶è·¯å¾„ '{official_test_file_path_to_use}' ä¸å­˜åœ¨ã€‚\")\n",
    "else:\n",
    "    print(f\"\\nå¼€å§‹å¤„ç†å®˜æ–¹æµ‹è¯•æ–‡ä»¶: {official_test_file_path_to_use}\")\n",
    "    official_test_texts, official_test_ids = load_official_test_data(official_test_file_path_to_use) # load_official_test_data åœ¨æ‚¨ä¹‹å‰çš„ä»£ç ä¸­å®šä¹‰\n",
    "    \n",
    "    if official_test_texts:\n",
    "        submission_outputs_strings = []\n",
    "        inference_batch_size = EVAL_BATCH_SIZE \n",
    "        print(f\"å¼€å§‹å¯¹ {len(official_test_texts)} æ¡æµ‹è¯•æ•°æ®è¿›è¡Œé¢„æµ‹ (æ‰¹æ¬¡å¤§å°: {inference_batch_size})...\")\n",
    "        for i in tqdm(range(0, len(official_test_texts), inference_batch_size), desc=\"å®˜æ–¹æµ‹è¯•é›†é¢„æµ‹\"):\n",
    "            batch_texts = official_test_texts[i : i + inference_batch_size]\n",
    "            batch_predictions = predict_quadruples_causal(\n",
    "                batch_texts, model_to_predict, tokenizer,\n",
    "                MAX_INPUT_LENGTH, MAX_TARGET_LENGTH\n",
    "            )\n",
    "            for item_prediction in batch_predictions:\n",
    "                submission_outputs_strings.append(item_prediction['extracted_answer_string'])\n",
    "        \n",
    "        #submission_file_path = \"/kaggle/working/submission.txt\" # Kaggle å·¥ä½œç›®å½•\n",
    "        submission_file_path = \"./newsubmission.txt\" # æˆ–è€…æœ¬åœ°è·¯å¾„\n",
    "        try:\n",
    "            with open(submission_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                for line_content in submission_outputs_strings:\n",
    "                    f.write(line_content + \"\\n\")\n",
    "            print(f\"\\næäº¤æ–‡ä»¶å·²æˆåŠŸç”Ÿæˆ: {submission_file_path}\")\n",
    "            print(f\"è¯¥æ–‡ä»¶åŒ…å« {len(submission_outputs_strings)} è¡Œé¢„æµ‹ã€‚\")\n",
    "        except Exception as e:\n",
    "            print(f\"å†™å…¥æäº¤æ–‡ä»¶ '{submission_file_path}' æ—¶å‘ç”Ÿé”™è¯¯: {e}\")\n",
    "    else:\n",
    "        print(f\"æœªèƒ½ä» '{official_test_file_path_to_use}' åŠ è½½ä»»ä½•æµ‹è¯•æ•°æ®è¿›è¡Œé¢„æµ‹ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc782e77-9281-4d18-9add-98bde4227c21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
