#%%
# --- å•å…ƒæ ¼ 1: ç¯å¢ƒè®¾ç½®å’Œåº“å¯¼å…¥ ğŸ› ï¸ ---


# åœ¨Notebookå†…éƒ¨éªŒè¯å’Œæç¤ºbitsandbytesçš„å®‰è£…
try:
    import bitsandbytes as bnb
    print(f"bitsandbytes ç‰ˆæœ¬: {bnb.__version__} å·²æˆåŠŸå¯¼å…¥ã€‚")
except ImportError:
    print("é”™è¯¯: bitsandbytes æœªå®‰è£…æˆ–å¯¼å…¥å¤±è´¥ã€‚")
    print("è¯·å°è¯•åœ¨æ–°çš„å•å…ƒæ ¼ä¸­è¿è¡Œ: !pip install -U bitsandbytes")
    print("æˆ–è€…ï¼Œå¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯ç‰¹å®šCUDAç‰ˆæœ¬ï¼Œå¯èƒ½éœ€è¦æŸ¥æ‰¾ç‰¹å®šçš„bitsandbyteså®‰è£…å‘½ä»¤ã€‚")
    print("å®‰è£…ååŠ¡å¿…é‡å¯Jupyter Kernelï¼")
    raise

import os
import json
import re
from collections import defaultdict
import difflib
import gc 
from tqdm import tqdm 

import torch
from datasets import Dataset, DatasetDict
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForSeq2Seq, 
    EarlyStoppingCallback,
    BitsAndBytesConfig,
    GenerationConfig
)
from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training

from sklearn.model_selection import train_test_split
import numpy as np

# æ£€æŸ¥å¯ç”¨GPU
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"å½“å‰ä½¿ç”¨çš„è®¾å¤‡: {DEVICE}")
if DEVICE.type == 'cuda':
    print(f"GPUåç§°: {torch.cuda.get_device_name(0)}")
    # è®¾ç½® PYTORCH_CUDA_ALLOC_CONF æ¥å‡å°‘æ˜¾å­˜ç¢ç‰‡
    os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
    print("å·²è®¾ç½® PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True")
#%%
# --- å•å…ƒæ ¼ 2: é…ç½®å‚æ•° âš™ï¸ ---
MODEL_NAME = "/root/autodl-tmp/models/Qwen3-1.7B" 
# LoRA é…ç½®
USE_LORA = True 
LORA_R = 16 
LORA_ALPHA = 32 
LORA_DROPOUT = 0.05 
LORA_TARGET_MODULES = ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

# é‡åŒ–é…ç½® 
USE_QUANTIZATION = True 
QUANTIZATION_TYPE = "nf4" 

# è®­ç»ƒç›¸å…³å‚æ•° (å› ä¸ºç»å¸¸OOMæ‰€ä»¥è¿›è¡Œä¿å®ˆè®¾ç½®)
OUTPUT_DIR = "/root/autodl-tmp/qwen_hate_speech_finetuned_llm_aug" # è¾“å‡ºç›®å½•å
TRAIN_FILE_PATH = "./train_formatted_for_llm.jsonl" 

TRAIN_BATCH_SIZE = 2 # éå¸¸å°çš„æ‰¹æ¬¡å¤§å°ä»¥é¿å…OOM
EVAL_BATCH_SIZE = 3  
NUM_TRAIN_EPOCHS = 3 
LEARNING_RATE = 2e-4 
WEIGHT_DECAY = 0.01  
MAX_INPUT_LENGTH = 1024 # 
MAX_TARGET_LENGTH = 256 # ç”Ÿæˆç›®æ ‡ï¼ˆå››å…ƒç»„å­—ç¬¦ä¸²ï¼‰çš„æœ€å¤§tokené•¿åº¦
GRADIENT_ACCUMULATION_STEPS = 8 # å¢å¤§æ¢¯åº¦ç´¯ç§¯ä»¥è¡¥å¿å°æ‰¹æ¬¡å¤§å°
WARMUP_RATIO = 0.03 
LR_SCHEDULER_TYPE = "cosine" 

SEED = 42 

# ç‰¹æ®Šæ ‡è®°å®šä¹‰
END_TOKEN = "[END]" 
SEP_TOKEN = "[SEP]" 
TARGET_GROUPS = ["Region", "Racism", "Sexism", "LGBTQ", "others", "non-hate"] 
HATEFUL_STATUS = ["hate", "non-hate"]

# å®šä¹‰æç¤ºæ¨¡æ¿ç»“æ„ 
PROMPT_TEMPLATE = """<s>[INST] <<SYS>>
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¸­æ–‡ç¤¾äº¤åª’ä½“å†…å®¹åˆ†æåŠ©æ‰‹ï¼Œä¸“é—¨ç”¨äºç»†ç²’åº¦ç‰‡æ®µçº§ä»‡æ¨è¨€è®ºè¯†åˆ«ã€‚è¯·æ ¹æ®ç”¨æˆ·æä¾›çš„æ–‡æœ¬ï¼Œè¯†åˆ«å…¶ä¸­å­˜åœ¨çš„ä»‡æ¨è¨€è®ºæˆ–éä»‡æ¨çš„è¯„è®ºæ€§è¨€è®ºï¼Œå¹¶æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºä¸€ä¸ªæˆ–å¤šä¸ªå››å…ƒç»„ï¼š
è¯„è®ºå¯¹è±¡ (Target) | è®ºç‚¹ (Argument) | ç›®æ ‡ç¾¤ä½“ (Targeted Group) | æ˜¯å¦ä»‡æ¨ (Hateful) [END]
è¯¦ç»†è¯´æ˜ï¼š
1.  **è¯„è®ºå¯¹è±¡ (Target)ï¼š** å¸–å­ä¸­è¢«è¯„è®ºæˆ–æåŠçš„å…·ä½“äººç‰©ã€ç¾¤ä½“ã€äº‹ç‰©æˆ–æ¦‚å¿µã€‚å¦‚æœæ˜¯é’ˆå¯¹æ–‡æœ¬ä¸­éšå«çš„ã€æ²¡æœ‰æ˜ç¡®æŒ‡å‡ºçš„å¯¹è±¡ï¼Œæˆ–è€…è¯„è®ºæ˜¯æ³›æŒ‡ï¼Œåˆ™è®¾ä¸º "NULL"ã€‚
2.  **è®ºç‚¹ (Argument)ï¼š** é’ˆå¯¹â€œè¯„è®ºå¯¹è±¡â€æ‰€å‘è¡¨çš„æ ¸å¿ƒè§‚ç‚¹ã€æè¿°æˆ–è¡Œä¸ºï¼Œåº”ä¸ºæ–‡æœ¬ä¸­çš„ä¸€ä¸ªå…³é”®ä¿¡æ¯ç‰‡æ®µã€‚
3.  **ç›®æ ‡ç¾¤ä½“ (Targeted Group)ï¼š** æŒ‡è¯¥â€œè¯„è®ºå¯¹è±¡-è®ºç‚¹â€æ‰€æ¶‰åŠæˆ–æŒ‡å‘çš„ç¤¾ä¼šç¾¤ä½“ã€‚å…¶ä¸­ï¼Œç›®æ ‡ç¾¤ä½“å¯ä»¥æœ‰å¤šé¡¹ï¼Œä½†å¿…é¡»ä»ä»¥ä¸‹é¢„è®¾ç±»åˆ«ä¸­é€‰æ‹©ï¼š
    * `Region`ï¼šé’ˆå¯¹ç‰¹å®šåœ°åŸŸï¼ˆå›½å®¶ã€çœä»½ã€åŸå¸‚ç­‰ï¼‰äººç¾¤çš„è¯„è®ºã€‚
    * `Racism`ï¼šé’ˆå¯¹ç‰¹å®šç§æ—æˆ–æ°‘æ—äººç¾¤çš„è¯„è®ºã€‚
    * `Sexism`ï¼šé’ˆå¯¹ç‰¹å®šæ€§åˆ«äººç¾¤ï¼ˆç”·æ€§ã€å¥³æ€§ï¼‰çš„è¯„è®ºï¼Œæˆ–æ€§åˆ«æ­§è§†ã€åˆ»æ¿å°è±¡ã€‚
    * `LGBTQ`ï¼šé’ˆå¯¹æ€§å°‘æ•°ç¾¤ä½“çš„è¯„è®ºï¼ˆå¦‚åŒæ€§æ‹ã€è·¨æ€§åˆ«ç­‰ï¼‰ã€‚
    * `others`ï¼šé’ˆå¯¹ä¸Šè¿°å››ç±»ä¹‹å¤–çš„ç‰¹å®šç¾¤ä½“ï¼ˆå¦‚ç‰¹å®šèŒä¸šã€ç–¾ç—…ç¾¤ä½“ã€æ”¿æ²»ç«‹åœºç¾¤ä½“ç­‰ï¼‰æˆ–ä¸æ„æˆå¯¹ç‰¹å®šç¤¾ä¼šç¾¤ä½“çš„æ”»å‡»ï¼Œè€Œæ˜¯ä¸ªäººæ”»å‡»ã€è§‚ç‚¹è¯„è®ºç­‰ã€‚
    * `non-hate`ï¼šä¸å­˜åœ¨æ”»å‡»ç¾¤ä½“ã€‚
4.  **æ˜¯å¦ä»‡æ¨ (Hateful)ï¼š** åˆ¤æ–­è¯¥â€œè¯„è®ºå¯¹è±¡-è®ºç‚¹â€æ˜¯å¦æ„æˆäº†å¯¹â€œç›®æ ‡ç¾¤ä½“â€çš„ä»‡æ¨è¨€è®ºã€‚
    * `hate`ï¼šæ„æˆä»‡æ¨ã€‚
    * `non-hate`ï¼šä¸æ„æˆä»‡æ¨ï¼ˆåŒ…æ‹¬ä¸­æ€§ã€ç§¯æã€æˆ–ä¸€èˆ¬æ€§è´Ÿé¢è¯„è®ºä½†æœªè¾¾åˆ°ä»‡æ¨ç¨‹åº¦ï¼‰ã€‚
æ ¼å¼è¦æ±‚ï¼š
* å››å…ƒç»„å†…å„å…ƒç´ ä¹‹é—´ç”¨ " | "ï¼ˆç©ºæ ¼ç«–æ ç©ºæ ¼ï¼‰åˆ†éš”ã€‚
* æ¯ä¸ªå››å…ƒç»„å¿…é¡»ä»¥ " [END]"ï¼ˆç©ºæ ¼[END]ï¼‰ç»“å°¾ã€‚
* å¦‚æœä¸€æ¡è¯„è®ºä¸­è¯†åˆ«å‡ºå¤šä¸ªç‹¬ç«‹çš„è¯„è®ºå¯¹è±¡å’Œè®ºç‚¹ï¼Œåº”è¾“å‡ºå¤šä¸ªå››å…ƒç»„ï¼Œä¸åŒå››å…ƒç»„ä¹‹é—´ç”¨ " [SEP] "ï¼ˆç©ºæ ¼[SEP]ç©ºæ ¼ï¼‰åˆ†éš”ã€‚

ç°åœ¨ï¼Œè¯·å¤„ç†ä»¥ä¸‹æ–°çš„è¾“å…¥å†…å®¹ï¼š
<</SYS>>

ç”¨æˆ·æä¾›çš„æ–‡æœ¬å¦‚ä¸‹ï¼š
{input_text} [/INST]
æ¨¡å‹è¾“å‡ºï¼š
"""
#%%
# --- å•å…ƒæ ¼ 3: æ•°æ®åŠ è½½ä¸å‡†å¤‡å‡½æ•°  ğŸ“„ ---
def load_and_prepare_data(file_path, test_size=0.1, random_state=SEED):
    input_texts_from_user = []      
    target_quadruples_from_assistant = [] 
    
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"é”™è¯¯: è®­ç»ƒæ–‡ä»¶ '{file_path}' æœªæ‰¾åˆ°ã€‚è¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®ã€‚")
        
    print(f"å¼€å§‹ä» '{file_path}' åŠ è½½æ•°æ® (é€‚é… 'messages' æ ¼å¼)...")
    with open(file_path, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1): 
            try:
                data_item = json.loads(line) 
                
                if "messages" not in data_item or not isinstance(data_item["messages"], list):
                    # print(f"è­¦å‘Š: è·³è¿‡è¡Œ (è¡Œå· {line_num})ï¼Œå› ä¸ºç¼ºå°‘ 'messages' é”®æˆ–å…¶å€¼ä¸æ˜¯åˆ—è¡¨: {line.strip()}")
                    continue

                messages_list = data_item["messages"]
                user_content = None
                assistant_content = None
                
                for message_dict in messages_list:
                    if "role" in message_dict and "content" in message_dict:
                        if message_dict["role"] == "user":
                            user_content = message_dict["content"]
                        elif message_dict["role"] == "assistant":
                            assistant_content = message_dict["content"]
                
                if user_content is not None and assistant_content is not None:
                    input_texts_from_user.append(user_content)
                    target_quadruples_from_assistant.append(assistant_content)
                # else:
                    # print(f"è­¦å‘Š: è·³è¿‡è¡Œ (è¡Œå· {line_num})ï¼Œæœªèƒ½ä» 'messages' ä¸­åŒæ—¶æ‰¾åˆ° 'user' å’Œ 'assistant' çš„æœ‰æ•ˆå†…å®¹ã€‚")

            except json.JSONDecodeError:
                # print(f"è­¦å‘Š: è·³è¿‡æ— æ•ˆçš„JSONè¡Œ (è¡Œå· {line_num}): {line.strip()}")
                pass 
            except Exception: 
                # print(f"è­¦å‘Š: å¤„ç†è¡Œ (è¡Œå· {line_num}) æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯ '{e}': {line.strip()}")
                pass 
    
    if not input_texts_from_user or not target_quadruples_from_assistant:
        raise ValueError(f"é”™è¯¯: æœªèƒ½ä» '{file_path}' åŠ è½½ä»»ä½•æœ‰æ•ˆçš„ 'user'/'assistant' å¯¹è¯æ•°æ®ã€‚")
    
    print(f"æˆåŠŸä» '{file_path}' åŠ è½½äº† {len(input_texts_from_user)} æ¡æœ‰æ•ˆçš„å¯¹è¯è®°å½•ã€‚")

    print(f"æ­£åœ¨å°†æ•°æ®åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›† (éªŒè¯é›†æ¯”ä¾‹: {test_size})...")
    train_texts, val_texts, train_quads, val_quads = train_test_split(
        input_texts_from_user, target_quadruples_from_assistant, 
        test_size=test_size, random_state=random_state
    )
    print(f"åˆ’åˆ†å®Œæˆ: è®­ç»ƒé›† {len(train_texts)} æ¡, éªŒè¯é›† {len(val_texts)} æ¡ã€‚")

    train_dataset = Dataset.from_dict({"text": train_texts, "quadruples_str": train_quads})
    val_dataset = Dataset.from_dict({"text": val_texts, "quadruples_str": val_quads})
    
    return DatasetDict({"train": train_dataset, "validation": val_dataset})
#%%
# --- å•å…ƒæ ¼ 4: åŠ è½½å¹¶æ£€æŸ¥åŸå§‹æ•°æ® ğŸ§ ---
print(f"å‡†å¤‡ä»æ–‡ä»¶ '{TRAIN_FILE_PATH}' åŠ è½½æ•°æ®...")
raw_datasets = None # åˆå§‹åŒ–
try:
    raw_datasets = load_and_prepare_data(TRAIN_FILE_PATH)
    print("\næ•°æ®åŠ è½½å’Œåˆæ­¥åˆ’åˆ†æˆåŠŸ:")
    print(raw_datasets) 
    
    if raw_datasets and 'train' in raw_datasets and len(raw_datasets['train']) > 0:
        print(f"\nè®­ç»ƒé›†ä¸­çš„ç¬¬ä¸€ä¸ªæ ·æœ¬ç¤ºä¾‹:")
        print(f"  è¾“å…¥æ–‡æœ¬ (text): {raw_datasets['train'][0]['text']}")
        print(f"  ç›®æ ‡æ ‡ç­¾ (quadruples_str): {raw_datasets['train'][0]['quadruples_str']}")
    else:
        print("\nè­¦å‘Š: åŠ è½½åçš„ 'raw_datasets' ä¸ºç©ºæˆ– 'train' éƒ¨åˆ†ä¸å®Œæ•´ã€‚è¯·æ£€æŸ¥æ•°æ®åŠ è½½è¿‡ç¨‹ã€‚")
except Exception as e:
    print(f"\næ•°æ®åŠ è½½æˆ–å‡†å¤‡è¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
#%%
# --- å•å…ƒæ ¼ 3.5 (æ–°å¢): åˆ©ç”¨LLMé›¶æ ·æœ¬èƒ½åŠ›ç”Ÿæˆä¼ªæ ‡ç­¾æ•°æ® ğŸ¤–ğŸ·ï¸ ---
# --- é…ç½®ç”¨äºç”Ÿæˆä¼ªæ ‡ç­¾çš„LLM ---
GENERATOR_MODEL_NAME_FOR_PSEUDO = "/root/autodl-tmp/models/Qwen3-8B" # ç¤ºä¾‹ï¼šä½¿ç”¨ä¸å¾®è°ƒç›¸åŒçš„æ¨¡å‹è·¯å¾„ï¼Œæˆ–å¦ä¸€ä¸ªæ›´å¼ºçš„æ¨¡å‹
GENERATOR_USE_QUANTIZATION_FOR_PSEUDO = True 
GENERATOR_QUANTIZATION_TYPE_FOR_PSEUDO = "nf4"

# ç”¨äºç”Ÿæˆä¼ªæ ‡ç­¾çš„æç¤ºæ¨¡æ¿ (ä¸å¾®è°ƒçš„PROMPT_TEMPLATEç±»ä¼¼ï¼Œä½†ä¸åŒ…å« "æ¨¡å‹è¾“å‡ºï¼š" åçš„ç­”æ¡ˆéƒ¨åˆ†)
# æ³¨æ„ï¼šè¿™é‡Œçš„ GENERATOR_PROMPT_TEMPLATE ä¸ä¸» PROMPT_TEMPLATE å‡ ä¹ä¸€è‡´ï¼Œ
# ç¡®ä¿ "æ¨¡å‹è¾“å‡ºï¼š" ä¹‹åæ˜¯ç©ºçš„ï¼Œä»¥ä¾¿LLMå¡«å……ã€‚
GENERATOR_PROMPT_TEMPLATE = """<s>[INST] <<SYS>>
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¸­æ–‡ç¤¾äº¤åª’ä½“å†…å®¹åˆ†æåŠ©æ‰‹ï¼Œä¸“é—¨ç”¨äºç»†ç²’åº¦ç‰‡æ®µçº§ä»‡æ¨è¨€è®ºè¯†åˆ«ã€‚è¯·æ ¹æ®ç”¨æˆ·æä¾›çš„æ–‡æœ¬ï¼Œè¯†åˆ«å…¶ä¸­å­˜åœ¨çš„ä»‡æ¨è¨€è®ºæˆ–éä»‡æ¨çš„è¯„è®ºæ€§è¨€è®ºï¼Œå¹¶æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºä¸€ä¸ªæˆ–å¤šä¸ªå››å…ƒç»„ï¼š
è¯„è®ºå¯¹è±¡ (Target) | è®ºç‚¹ (Argument) | ç›®æ ‡ç¾¤ä½“ (Targeted Group) | æ˜¯å¦ä»‡æ¨ (Hateful) [END]
è¯¦ç»†è¯´æ˜ï¼š
1.  **è¯„è®ºå¯¹è±¡ (Target)ï¼š** å¸–å­ä¸­è¢«è¯„è®ºæˆ–æåŠçš„å…·ä½“äººç‰©ã€ç¾¤ä½“ã€äº‹ç‰©æˆ–æ¦‚å¿µã€‚å¦‚æœæ˜¯é’ˆå¯¹æ–‡æœ¬ä¸­éšå«çš„ã€æ²¡æœ‰æ˜ç¡®æŒ‡å‡ºçš„å¯¹è±¡ï¼Œæˆ–è€…è¯„è®ºæ˜¯æ³›æŒ‡ï¼Œåˆ™è®¾ä¸º "NULL"ã€‚
2.  **è®ºç‚¹ (Argument)ï¼š** é’ˆå¯¹â€œè¯„è®ºå¯¹è±¡â€æ‰€å‘è¡¨çš„æ ¸å¿ƒè§‚ç‚¹ã€æè¿°æˆ–è¡Œä¸ºï¼Œåº”ä¸ºæ–‡æœ¬ä¸­çš„ä¸€ä¸ªå…³é”®ä¿¡æ¯ç‰‡æ®µã€‚
3.  **ç›®æ ‡ç¾¤ä½“ (Targeted Group)ï¼š** æŒ‡è¯¥â€œè¯„è®ºå¯¹è±¡-è®ºç‚¹â€æ‰€æ¶‰åŠæˆ–æŒ‡å‘çš„ç¤¾ä¼šç¾¤ä½“ã€‚å¿…é¡»ä»ä»¥ä¸‹é¢„è®¾ç±»åˆ«ä¸­é€‰æ‹©ï¼š
    * `Region`ï¼šé’ˆå¯¹ç‰¹å®šåœ°åŸŸï¼ˆå›½å®¶ã€çœä»½ã€åŸå¸‚ç­‰ï¼‰äººç¾¤çš„è¯„è®ºã€‚
    * `Racism`ï¼šé’ˆå¯¹ç‰¹å®šç§æ—æˆ–æ°‘æ—äººç¾¤çš„è¯„è®ºã€‚
    * `Sexism`ï¼šé’ˆå¯¹ç‰¹å®šæ€§åˆ«äººç¾¤ï¼ˆç”·æ€§ã€å¥³æ€§ï¼‰çš„è¯„è®ºï¼Œæˆ–æ€§åˆ«æ­§è§†ã€åˆ»æ¿å°è±¡ã€‚
    * `LGBTQ`ï¼šé’ˆå¯¹æ€§å°‘æ•°ç¾¤ä½“çš„è¯„è®ºï¼ˆå¦‚åŒæ€§æ‹ã€è·¨æ€§åˆ«ç­‰ï¼‰ã€‚
    * `others`ï¼šé’ˆå¯¹ä¸Šè¿°å››ç±»ä¹‹å¤–çš„ç‰¹å®šç¾¤ä½“ï¼ˆå¦‚ç‰¹å®šèŒä¸šã€ç–¾ç—…ç¾¤ä½“ã€æ”¿æ²»ç«‹åœºç¾¤ä½“ç­‰ï¼‰æˆ–ä¸æ„æˆå¯¹ç‰¹å®šç¤¾ä¼šç¾¤ä½“çš„æ”»å‡»ï¼Œè€Œæ˜¯ä¸ªäººæ”»å‡»ã€è§‚ç‚¹è¯„è®ºç­‰ã€‚
    * `non-hate`ï¼šä¸å­˜åœ¨æ”»å‡»ç¾¤ä½“ã€‚
4.  **æ˜¯å¦ä»‡æ¨ (Hateful)ï¼š** åˆ¤æ–­è¯¥â€œè¯„è®ºå¯¹è±¡-è®ºç‚¹â€æ˜¯å¦æ„æˆäº†å¯¹â€œç›®æ ‡ç¾¤ä½“â€çš„ä»‡æ¨è¨€è®ºã€‚
    * `hate`ï¼šæ„æˆä»‡æ¨ã€‚
    * `non-hate`ï¼šä¸æ„æˆä»‡æ¨ï¼ˆåŒ…æ‹¬ä¸­æ€§ã€ç§¯æã€æˆ–ä¸€èˆ¬æ€§è´Ÿé¢è¯„è®ºä½†æœªè¾¾åˆ°ä»‡æ¨ç¨‹åº¦ï¼‰ã€‚
æ ¼å¼è¦æ±‚ï¼š
* å››å…ƒç»„å†…å„å…ƒç´ ä¹‹é—´ç”¨ " | "ï¼ˆç©ºæ ¼ç«–æ ç©ºæ ¼ï¼‰åˆ†éš”ã€‚
* æ¯ä¸ªå››å…ƒç»„å¿…é¡»ä»¥ " [END]"ï¼ˆç©ºæ ¼[END]ï¼‰ç»“å°¾ã€‚
* å¦‚æœä¸€æ¡è¯„è®ºä¸­è¯†åˆ«å‡ºå¤šä¸ªç‹¬ç«‹çš„è¯„è®ºå¯¹è±¡å’Œè®ºç‚¹ï¼Œåº”è¾“å‡ºå¤šä¸ªå››å…ƒç»„ï¼Œä¸åŒå››å…ƒç»„ä¹‹é—´ç”¨ " [SEP] "ï¼ˆç©ºæ ¼[SEP]ç©ºæ ¼ï¼‰åˆ†éš”ã€‚

ç°åœ¨ï¼Œè¯·å¤„ç†ä»¥ä¸‹æ–°çš„è¾“å…¥å†…å®¹ï¼š
<</SYS>>

ç”¨æˆ·æä¾›çš„æ–‡æœ¬å¦‚ä¸‹ï¼š
{input_text} [/INST]
æ¨¡å‹è¾“å‡ºï¼š
"""

pseudo_labels_list = []
texts_for_pseudo_generation = []

if raw_datasets and 'train' in raw_datasets and raw_datasets['train'] is not None:
    texts_for_pseudo_generation = list(raw_datasets['train']['text'])
    print(f"å‡†å¤‡ä¸º {len(texts_for_pseudo_generation)} æ¡è®­ç»ƒæ–‡æœ¬ç”Ÿæˆä¼ªæ ‡ç­¾...")

    # --- åŠ è½½ç”Ÿæˆå™¨LLMå’ŒTokenizer ---
    # ä¸ºé¿å…ä¸ä¸»æ¨¡å‹å†²çªï¼Œä½¿ç”¨ä¸åŒçš„å˜é‡å
    generator_model_instance = None
    generator_tokenizer_instance = None
    print(f"æ­£åœ¨ä» '{GENERATOR_MODEL_NAME_FOR_PSEUDO}' åŠ è½½ç”¨äºç”Ÿæˆä¼ªæ ‡ç­¾çš„LLMå’ŒTokenizer...")
    try:
        generator_tokenizer_instance = AutoTokenizer.from_pretrained(GENERATOR_MODEL_NAME_FOR_PSEUDO, trust_remote_code=True)
        
        generator_bnb_config = None
        if GENERATOR_USE_QUANTIZATION_FOR_PSEUDO:
            if GENERATOR_QUANTIZATION_TYPE_FOR_PSEUDO == "nf4" or GENERATOR_QUANTIZATION_TYPE_FOR_PSEUDO == "fp4":
                generator_bnb_config = BitsAndBytesConfig(
                    load_in_4bit=True, bnb_4bit_quant_type=GENERATOR_QUANTIZATION_TYPE_FOR_PSEUDO,
                    bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_use_double_quant=True,
                )
            elif GENERATOR_QUANTIZATION_TYPE_FOR_PSEUDO == "int8":
                generator_bnb_config = BitsAndBytesConfig(load_in_8bit=True)
            print(f"ç”Ÿæˆå™¨LLMå°†ä½¿ç”¨é‡åŒ–: {GENERATOR_QUANTIZATION_TYPE_FOR_PSEUDO if generator_bnb_config else 'æ— '}")

        generator_model_instance = AutoModelForCausalLM.from_pretrained(
            GENERATOR_MODEL_NAME_FOR_PSEUDO,
            quantization_config=generator_bnb_config,
            trust_remote_code=True,
            device_map="auto"
        )
        generator_model_instance.eval() 

        if generator_tokenizer_instance.pad_token is None:
            generator_tokenizer_instance.pad_token = generator_tokenizer_instance.eos_token
            if generator_model_instance.config.pad_token_id is None:
                 generator_model_instance.config.pad_token_id = generator_tokenizer_instance.pad_token_id
            print(f"ç”Ÿæˆå™¨Tokenizerçš„pad_tokenå·²è®¾ç½®ä¸ºeos_token: '{generator_tokenizer_instance.eos_token}'")
        
        print("ç”Ÿæˆå™¨LLMå’ŒTokenizeråŠ è½½æˆåŠŸã€‚")

        GENERATION_BATCH_SIZE = 4 # ä¼ªæ ‡ç­¾ç”Ÿæˆæ‰¹æ¬¡å¤§å°
        
        generation_config_pseudo = GenerationConfig(
            max_new_tokens=MAX_TARGET_LENGTH, 
            num_beams=1, 
            do_sample=False, 
            pad_token_id=generator_tokenizer_instance.pad_token_id if generator_tokenizer_instance.pad_token_id is not None else generator_tokenizer_instance.eos_token_id,
            eos_token_id=generator_tokenizer_instance.eos_token_id
        )

        for i in tqdm(range(0, len(texts_for_pseudo_generation), GENERATION_BATCH_SIZE), desc="ç”Ÿæˆä¼ªæ ‡ç­¾"):
            batch_texts = texts_for_pseudo_generation[i : i + GENERATION_BATCH_SIZE]
            batch_prompts = [GENERATOR_PROMPT_TEMPLATE.format(input_text=text) for text in batch_texts]
            
            inputs = generator_tokenizer_instance(
                batch_prompts, 
                return_tensors="pt", 
                padding=True, 
                truncation=True, 
                max_length=MAX_INPUT_LENGTH - MAX_TARGET_LENGTH 
            ).to(generator_model_instance.device)

            with torch.no_grad():
                outputs = generator_model_instance.generate(**inputs, generation_config=generation_config_pseudo)
            
            # è§£ç å¹¶æå–ç­”æ¡ˆéƒ¨åˆ†
            # outputs åŒ…å«å®Œæ•´çš„åºåˆ— (æç¤º+ç­”æ¡ˆ)ã€‚æˆ‘ä»¬éœ€è¦æå–æ¨¡å‹ç”Ÿæˆçš„éƒ¨åˆ†ã€‚
            # input_len = inputs.input_ids.shape[1]
            # generated_ids_batch = outputs[:, input_len:] # è·å–æ¯ä¸ªæ ·æœ¬æ–°ç”Ÿæˆçš„token
            # decoded_answers = generator_tokenizer_instance.batch_decode(generated_ids_batch, skip_special_tokens=True, clean_up_tokenization_spaces=True)
            
            # æˆ–è€…ï¼Œä½¿ç”¨ä¹‹å‰çš„æ–¹æ³•ä»å®Œæ•´è§£ç æ–‡æœ¬ä¸­åˆ†å‰²
            full_decoded_outputs = generator_tokenizer_instance.batch_decode(outputs, skip_special_tokens=True, clean_up_tokenization_spaces=True)
            keyword_separator_pseudo = "æ¨¡å‹è¾“å‡ºï¼š" # ä¸GENERATOR_PROMPT_TEMPLATEæœ«å°¾ä¸€è‡´
            
            for full_output_text in full_decoded_outputs:
                answer_part_str = ""
                if keyword_separator_pseudo in full_output_text:
                    answer_part_str = full_output_text.split(keyword_separator_pseudo, 1)[-1].strip()
                else: # åå¤‡æ–¹æ¡ˆ
                    original_prompt_text_no_answer = GENERATOR_PROMPT_TEMPLATE.format(input_text="DUMMY").split(keyword_separator_pseudo)[0] # è·å–æç¤ºå¤´
                    # è¿™æ˜¯ä¸€ä¸ªç²—ç•¥çš„ç§»é™¤ï¼Œå¯èƒ½ä¸å®Œç¾
                    if full_output_text.startswith(original_prompt_text_no_answer.split("ç”¨æˆ·æä¾›çš„æ–‡æœ¬å¦‚ä¸‹ï¼š")[0]): # å°è¯•åŒ¹é…ç³»ç»Ÿæç¤ºéƒ¨åˆ†
                         answer_part_str = full_output_text # å¦‚æœæ— æ³•æ¸…æ™°åˆ†å‰²ï¼Œä¿ç•™å®Œæ•´è¾“å‡ºï¼Œåç»­è´¨é‡è¯„ä¼°æ—¶å¤„ç†
                    else:
                         answer_part_str = full_output_text
                pseudo_labels_list.append(answer_part_str)
        
        print(f"æˆåŠŸä¸º {len(pseudo_labels_list)} æ¡æ–‡æœ¬ç”Ÿæˆäº†ä¼ªæ ‡ç­¾ã€‚")

    except Exception as e:
        print(f"åŠ è½½ç”Ÿæˆå™¨LLMæˆ–ç”Ÿæˆä¼ªæ ‡ç­¾è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        print("å°†ä½¿ç”¨ç©ºçš„ä¼ªæ ‡ç­¾åˆ—è¡¨ã€‚")
        pseudo_labels_list = [] 
    finally:
        # æ¸…ç†ç”Ÿæˆå™¨æ¨¡å‹ä»¥é‡Šæ”¾æ˜¾å­˜
        if 'generator_model_instance' in locals() and generator_model_instance is not None:
            del generator_model_instance
        if 'generator_tokenizer_instance' in locals() and generator_tokenizer_instance is not None:
            del generator_tokenizer_instance
        if 'inputs' in locals() and inputs is not None: del inputs
        if 'outputs' in locals() and outputs is not None: del outputs
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
        print("ç”Ÿæˆå™¨LLMåŠç›¸å…³èµ„æºå·²å°è¯•æ¸…ç†ã€‚")
else:
    print("è­¦å‘Š: åŸå§‹æ•°æ®é›† 'raw_datasets' æœªåŠ è½½ï¼Œæ— æ³•ç”Ÿæˆä¼ªæ ‡ç­¾ã€‚")
    pseudo_labels_list = []

if pseudo_labels_list:
    print("\nç”Ÿæˆçš„ä¸€äº›ä¼ªæ ‡ç­¾æ ·æœ¬:")
    for i in range(min(3, len(pseudo_labels_list))):
        print(f"  åŸå§‹æ–‡æœ¬ (éƒ¨åˆ†): {texts_for_pseudo_generation[i][:50]}...")
        print(f"  ç”Ÿæˆä¼ªæ ‡ç­¾: {pseudo_labels_list[i]}")
else:
    print("\næœªèƒ½ç”Ÿæˆæˆ–åŠ è½½ä»»ä½•ä¼ªæ ‡ç­¾ã€‚")
#%%
# --- å•å…ƒæ ¼ 3.6: åˆ©ç”¨LLMç”Ÿæˆçš„è´Ÿä¾‹è¿›è¡Œå¯¹æ¯”å¢å¼ºSFTæ•°æ® ---

# æ˜¯å¦å¯ç”¨åŸºäºLLMè´Ÿä¾‹çš„å¯¹æ¯”å¢å¼º
ENABLE_CONTRASTIVE_AUGMENTATION_WITH_NEGATIVES = True

# ç¡®ä¿ parse_quadruples å‡½æ•°å·²å®šä¹‰ (é€šå¸¸åœ¨å•å…ƒæ ¼8)
# å¦‚æœå•å…ƒæ ¼8çš„è¿˜æœªæ‰§è¡Œ, ä½¿ç”¨ä¸´æ—¶å ä½ç¬¦ (ä¸»è¦ç”¨äºç»“æ„å®Œæ•´æ€§, å®é™…è¿è¡Œå‰åº”ç¡®ä¿å·²å®šä¹‰)
if 'parse_quadruples' not in globals(): 
    def parse_quadruples_placeholder(text_str_dummy): 
        if not text_str_dummy: return []
        quads = []
        parts = text_str_dummy.split(SEP_TOKEN if 'SEP_TOKEN' in globals() else "[SEP]") # ä½¿ç”¨å…¨å±€å˜é‡æˆ–é»˜è®¤å€¼
        for part in parts:
            part_c = part.strip()
            if part_c.endswith(END_TOKEN if 'END_TOKEN' in globals() else "[END]"):
                part_c = part_c[:-len(END_TOKEN if 'END_TOKEN' in globals() else "[END]")].strip()
            if not part_c: continue
            elements = [e.strip() for e in part_c.split(" | ")]
            if len(elements) == 4:
                quads.append(elements)
        return quads
    parse_quadruples_fn_to_use = parse_quadruples_placeholder
    print("è­¦å‘Šï¼šå•å…ƒæ ¼8çš„ 'parse_quadruples' å‡½æ•°å®šä¹‰å…ˆäºæ­¤å•å…ƒæ ¼æ‰§è¡Œã€‚å°†ä½¿ç”¨ä¸´æ—¶å ä½ç¬¦ã€‚")
else:
    parse_quadruples_fn_to_use = parse_quadruples


if ENABLE_CONTRASTIVE_AUGMENTATION_WITH_NEGATIVES and \
   'raw_datasets' in locals() and raw_datasets and \
   'pseudo_labels_list' in locals() and \
   len(pseudo_labels_list) == len(raw_datasets['train']): # ç¡®ä¿ä¼ªæ ‡ç­¾åˆ—è¡¨ä¸è®­ç»ƒæ•°æ®å¯¹é½

    print(f"å¼€å§‹åŸºäºLLMç”Ÿæˆçš„ä¼ªæ ‡ç­¾ï¼ˆä½œä¸ºè´Ÿä¾‹ï¼‰è¿›è¡Œå¯¹æ¯”æ•°æ®å¢å¼º...")
    
    original_train_texts = list(raw_datasets['train']['text'])
    original_train_quads = list(raw_datasets['train']['quadruples_str']) # çœŸå®æ ‡ç­¾ (æ­£ä¾‹)
    
    # pseudo_labels_list åŒ…å«çš„æ˜¯LLMç”Ÿæˆçš„ä¼ªæ ‡ç­¾ (å°†è¢«è§†ä¸ºè´Ÿä¾‹æˆ–å¹²æ‰°é¡¹)
    
    contrastive_augmented_texts = []
    contrastive_augmented_quads = [] # ç›®æ ‡è¾“å‡ºå§‹ç»ˆæ˜¯çœŸå®çš„å››å…ƒç»„
    
    num_augmented_samples_created = 0

    for i in tqdm(range(len(original_train_texts)), desc="åˆ›å»ºå¯¹æ¯”å¢å¼ºSFTæ•°æ®"):
        original_text_content = original_train_texts[i]
        true_quad_str = original_train_quads[i]         # æ­£ä¾‹è¾“å‡º
        negative_pseudo_quad_str = pseudo_labels_list[i] # LLMç”Ÿæˆçš„ï¼Œä½œä¸ºè´Ÿä¾‹/å¹²æ‰°é¡¹

        # 1. æ·»åŠ æ ‡å‡†çš„SFTæ ·æœ¬ï¼š (åŸå§‹æç¤º -> çœŸå®å››å…ƒç»„)
        #    PROMPT_TEMPLATE ä¸­çš„ {input_text} å°†ç›´æ¥ä½¿ç”¨ original_text_content
        contrastive_augmented_texts.append(original_text_content) 
        contrastive_augmented_quads.append(true_quad_str)
        
        # 2. åˆ›å»ºå¯¹æ¯”å¢å¼ºçš„SFTæ ·æœ¬ï¼š
        #    (åŒ…å«è´Ÿä¾‹çš„å¤æ‚æç¤º -> çœŸå®å››å…ƒç»„)
        #    åªæœ‰å½“ä¼ªæ ‡ç­¾ä¸çœŸå®æ ‡ç­¾ç¡®å®ä¸åŒæ—¶ï¼Œè¿™ç§å¢å¼ºæ‰æœ‰æ„ä¹‰
        if negative_pseudo_quad_str and negative_pseudo_quad_str.strip() and \
           negative_pseudo_quad_str.strip() != true_quad_str.strip():
            
            # æ„å»ºåŒ…å«åŸå§‹æ–‡æœ¬å’Œâ€œé”™è¯¯ææ¡ˆâ€ï¼ˆè´Ÿä¾‹ï¼‰çš„æ–°è¾“å…¥æ–‡æœ¬
            # è¿™ä¸ª new_input_for_prompt ä¼šè¢«å¡«å…¥ä¸» PROMPT_TEMPLATE çš„ {input_text} å ä½ç¬¦
            new_input_for_prompt = (
                f"åŸå§‹æ–‡æœ¬å†…å®¹ï¼š\n\"{original_text_content}\"\n\n"
                f"ä¸€ä¸ªAIåŠ©æ‰‹é’ˆå¯¹ä»¥ä¸Šæ–‡æœ¬ç»™å‡ºäº†å¦‚ä¸‹å¯èƒ½æ˜¯é”™è¯¯æˆ–ä¸å®Œå–„çš„å››å…ƒç»„æå–ç»“æœï¼š\n"
                f"\"{negative_pseudo_quad_str}\"\n\n"
                f"è¯·ä½ å¿½ç•¥ä¸Šè¿°AIåŠ©æ‰‹çš„æå–ç»“æœï¼ˆå®ƒå¯èƒ½åŒ…å«é”™è¯¯ï¼‰ï¼Œå¹¶ä¸¥æ ¼æŒ‰ç…§æŒ‡ä»¤ï¼Œæ ¹æ®â€œåŸå§‹æ–‡æœ¬å†…å®¹â€é‡æ–°åˆ†æå¹¶ç»™å‡ºæ­£ç¡®çš„å››å…ƒç»„ã€‚"
            )
            
            contrastive_augmented_texts.append(new_input_for_prompt)
            contrastive_augmented_quads.append(true_quad_str) # ç›®æ ‡ä»ç„¶æ˜¯çœŸå®çš„å››å…ƒç»„
            num_augmented_samples_created += 1

    print(f"å¯¹æ¯”æ•°æ®å¢å¼ºå®Œæˆã€‚")
    print(f"åŸå§‹è®­ç»ƒæ ·æœ¬æ•°: {len(original_train_texts)}")
    print(f"é¢å¤–åˆ›å»ºäº† {num_augmented_samples_created} ä¸ªå¯¹æ¯”å¢å¼ºæ ·æœ¬ã€‚")
    
    if num_augmented_samples_created > 0 or len(contrastive_augmented_texts) != len(original_train_texts) :
        contrastive_augmented_train_dataset = Dataset.from_dict({
            "text": contrastive_augmented_texts, # "text" å­—æ®µç°åœ¨åŒ…å«åŸå§‹æ–‡æœ¬æˆ–å¢å¼ºåçš„å¤æ‚æç¤º
            "quadruples_str": contrastive_augmented_quads # ç›®æ ‡å§‹ç»ˆæ˜¯çœŸå®çš„å››å…ƒç»„
        })
        
        # æ›´æ–° raw_datasets ä¸­çš„è®­ç»ƒé›†
        # éªŒè¯é›†ä¿æŒä¸å˜ï¼Œç”¨äºè¯„ä¼°åŸå§‹ä»»åŠ¡æ€§èƒ½
        raw_datasets['train'] = contrastive_augmented_train_dataset
        print(f"å¯¹æ¯”å¢å¼ºSFTæ•°æ®å‡†å¤‡å®Œæˆã€‚è®­ç»ƒé›†ç°åœ¨åŒ…å« {len(raw_datasets['train'])} æ¡æ ·æœ¬ã€‚")
        if len(raw_datasets['train']) > 0:
            print(f"å¢å¼ºåè®­ç»ƒé›†çš„ç¬¬ä¸€ä¸ªæ ·æœ¬ text (å¯èƒ½ä¸ºåŸå§‹): {raw_datasets['train'][0]['text'][:150]}...") 
            print(f"å¢å¼ºåè®­ç»ƒé›†çš„ç¬¬ä¸€ä¸ªæ ·æœ¬ quadruple: {raw_datasets['train'][0]['quadruples_str']}")
            if len(raw_datasets['train']) > len(original_train_texts): # å¦‚æœç¡®å®æ·»åŠ äº†å¢å¼ºæ ·æœ¬
                 print(f"ä¸€ä¸ªå¯¹æ¯”å¢å¼ºæ ·æœ¬çš„ text (éƒ¨åˆ†): {raw_datasets['train'][-1]['text'][:250]}...") # æ‰“å°æœ€åä¸€ä¸ªï¼ˆå¯èƒ½æ˜¯å¢å¼ºçš„ï¼‰
                 print(f"è¯¥å¢å¼ºæ ·æœ¬çš„ç›®æ ‡ quadruple: {raw_datasets['train'][-1]['quadruples_str']}")
    else:
        print("æ²¡æœ‰æ–°çš„å¯¹æ¯”å¢å¼ºæ ·æœ¬è¢«æ·»åŠ åˆ°è®­ç»ƒé›†ï¼ˆå¯èƒ½å› ä¸ºæ‰€æœ‰ä¼ªæ ‡ç­¾éƒ½ä¸çœŸå®æ ‡ç­¾ç›¸åŒï¼Œæˆ–è€…ä¼ªæ ‡ç­¾ä¸ºç©ºï¼‰ã€‚")

else:
    if not ENABLE_CONTRASTIVE_AUGMENTATION_WITH_NEGATIVES:
        print("åŸºäºLLMè´Ÿä¾‹çš„å¯¹æ¯”æ•°æ®å¢å¼ºè¢«ç¦ç”¨ã€‚")
    else:
        print("è­¦å‘Š: æœªæ‰§è¡ŒåŸºäºLLMè´Ÿä¾‹çš„å¯¹æ¯”æ•°æ®å¢å¼ºï¼Œå› ä¸º 'raw_datasets' æˆ– 'pseudo_labels_list' æœªæ­£ç¡®å‡†å¤‡æˆ–æ•°é‡ä¸åŒ¹é…ã€‚")
#%%
# --- å•å…ƒæ ¼ 5: Tokenizer åˆå§‹åŒ–å’Œæ•°æ®é¢„å¤„ç†å‡½æ•° ğŸ“ ---
print(f"æ­£åœ¨ä» '{MODEL_NAME}' åŠ è½½ç”¨äºå¾®è°ƒçš„Tokenizer...") # ä¸»å¾®è°ƒæ¨¡å‹çš„Tokenizer
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
print("å¾®è°ƒTokenizeråŠ è½½å®Œæˆã€‚")

if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token 
    print(f"å¾®è°ƒTokenizerçš„pad_tokenæœªè®¾ç½®ï¼Œå·²å°†å…¶è®¾ç½®ä¸ºeos_token: '{tokenizer.eos_token}' (ID: {tokenizer.eos_token_id})")
else:
    print(f"å¾®è°ƒTokenizerçš„pad_tokenå·²è®¾ç½®ä¸º: '{tokenizer.pad_token}' (ID: {tokenizer.pad_token_id})")

def preprocess_function_causal(examples):
    full_prompts = []
    input_texts_for_prompt = examples["text"]
    target_outputs = examples["quadruples_str"]

    for input_text, target_output in zip(input_texts_for_prompt, target_outputs):
        input_text_str = str(input_text) if input_text is not None else ""
        target_output_str = str(target_output) if target_output is not None else ""
        
        prompt_part = PROMPT_TEMPLATE.format(input_text=input_text_str)
        full_text = prompt_part + target_output_str + tokenizer.eos_token
        full_prompts.append(full_text)

    model_inputs = tokenizer(
        full_prompts,
        max_length=MAX_INPUT_LENGTH,
        truncation=True,
        padding=False, 
        return_attention_mask=True 
    )

    labels = [list(ids) for ids in model_inputs["input_ids"]] 

    for i in range(len(examples["text"])):
        # input_text_str = str(examples["text"][i]) if examples["text"][i] is not None else ""
        # prompt_part_only = PROMPT_TEMPLATE.format(input_text=input_text_str)
        current_input_ids = model_inputs["input_ids"][i]
        current_labels = labels[i]
        
        answer_part_str = str(examples["quadruples_str"][i]) if examples["quadruples_str"][i] is not None else ""
        # Tokenizeç­”æ¡ˆéƒ¨åˆ†ï¼ˆä¸åŠ ç‰¹æ®Štokenï¼Œå› ä¸ºå®ƒä»¬å·²åœ¨full_textä¸­å¤„ç†ï¼‰
        answer_tokens = tokenizer(answer_part_str + tokenizer.eos_token, add_special_tokens=False)["input_ids"]
        
        len_to_mask = len(current_input_ids) - len(answer_tokens)
        
        if len_to_mask < 0: 
            # print(f"è­¦å‘Š: æ ·æœ¬ {i} çš„è®¡ç®—å±è”½é•¿åº¦ä¸ºè´Ÿ ({len_to_mask})ã€‚Input: '{str(examples['text'][i])[:50]}...', Target: '{answer_part_str}'")
            # print(f"  Input IDs len: {len(current_input_ids)}, Answer tokens len: {len(answer_tokens)}")
            # å¦‚æœç­”æ¡ˆæ¯”æ•´ä¸ªåºåˆ—è¿˜é•¿ï¼ˆæˆ–å› æˆªæ–­å¯¼è‡´ä¸åŒ¹é…ï¼‰ï¼Œåˆ™ä¸å±è”½ä»»ä½•å†…å®¹ï¼Œæˆ–ä»…å±è”½BOS
            if current_input_ids and current_input_ids[0] == tokenizer.bos_token_id:
                 len_to_mask = 1 # åªå±è”½BOS
            else:
                 len_to_mask = 0 # ä¸å±è”½
        
        for j in range(min(len_to_mask, len(current_labels))): 
            current_labels[j] = -100
        
        if answer_part_str and all(l == -100 for l in current_labels):
            # print(f"è­¦å‘Š: æ ·æœ¬ {i} çš„æ‰€æœ‰æ ‡ç­¾éƒ½è¢«å±è”½ï¼Œä½†ç›®æ ‡è¾“å‡ºä¸ä¸ºç©º ('{answer_part_str}')ã€‚å±è”½é•¿åº¦: {len_to_mask}")
            if current_labels:
                 current_labels[-1] = current_input_ids[-1]

    model_inputs["labels"] = labels
    return model_inputs
#%%
# --- å•å…ƒæ ¼ 6: å¯¹æ•°æ®é›†è¿›è¡ŒTokenize ğŸ§© ---
print("å¼€å§‹å¯¹æ•°æ®é›†è¿›è¡Œtokenizeå’Œé¢„å¤„ç† (é€‚é…Causal LM)...")
if 'raw_datasets' in locals() and raw_datasets and 'train' in raw_datasets and raw_datasets['train'] is not None:
    tokenized_datasets = raw_datasets.map(
        preprocess_function_causal, 
        batched=True, 
        remove_columns=raw_datasets["train"].column_names 
    )
    print("\næ•°æ®tokenizeå’Œé¢„å¤„ç†å®Œæˆ:")
    print(tokenized_datasets) 

    if tokenized_datasets and 'train' in tokenized_datasets and len(tokenized_datasets['train']) > 0:
        print(f"\nTokenizeåçš„è®­ç»ƒé›†æ ·æœ¬ (æ£€æŸ¥input_idså’Œlabelsçš„å±è”½æƒ…å†µ):")
        sample_idx = 0 
        if sample_idx < len(tokenized_datasets['train']) and sample_idx < len(raw_datasets['train']):
            print(f"  åŸå§‹/å¢å¼ºåè¾“å…¥æ–‡æœ¬ (text): {raw_datasets['train'][sample_idx]['text']}") 
            print(f"  åŸå§‹ç›®æ ‡è¾“å‡º (quadruples_str): {raw_datasets['train'][sample_idx]['quadruples_str']}")
            
            tokenized_sample = tokenized_datasets['train'][sample_idx]
            print(f"\n  Tokenized input_ids (å‰60): {tokenized_sample['input_ids'][:60]}")
            print(f"  Decoded input_ids (å‰60): {tokenizer.decode(tokenized_sample['input_ids'][:60])}")
            
            print(f"\n  Tokenized labels (å‰60, -100è¡¨ç¤ºå·²å±è”½): {tokenized_sample['labels'][:60]}")
            
            first_label_idx = -1
            for idx, lbl_id in enumerate(tokenized_sample['labels']):
                if lbl_id != -100:
                    first_label_idx = idx
                    break
            
            if first_label_idx != -1:
                decoded_label_part = tokenizer.decode([l for l in tokenized_sample['labels'][first_label_idx:] if l != -100])
                print(f"  Decoded labels from first non-masked token (éƒ¨åˆ†): {decoded_label_part}")
            else:
                print("  æ³¨æ„ï¼šè¯¥æ ·æœ¬çš„æ‰€æœ‰æ ‡ç­¾éƒ½è¢«å±è”½äº†ã€‚")
                if raw_datasets['train'][sample_idx]['quadruples_str']: 
                     print(f"  åŸå§‹ç›®æ ‡è¾“å‡ºéç©º ('{raw_datasets['train'][sample_idx]['quadruples_str']}'), ä½†æ‰€æœ‰æ ‡ç­¾è¢«å±è”½ï¼Œè¯·ä»”ç»†æ£€æŸ¥preprocess_function_causalä¸­çš„å±è”½é€»è¾‘ã€‚")
        else:
            print(f"è­¦å‘Šï¼šé€‰æ‹©çš„æ ·æœ¬ç´¢å¼• {sample_idx} è¶…å‡ºè®­ç»ƒé›†èŒƒå›´ã€‚")
    else:
        print("\nè­¦å‘Š: Tokenizeåçš„æ•°æ®é›†ä¸ºç©ºæˆ–ä¸å®Œæ•´ã€‚")
else:
    print("é”™è¯¯: 'raw_datasets' æˆ–å…¶è®­ç»ƒé›†æœªå®šä¹‰/ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œtokenizeã€‚è¯·å…ˆæˆåŠŸæ‰§è¡Œæ•°æ®åŠ è½½å’Œï¼ˆå¯é€‰çš„ï¼‰å¢å¼ºå•å…ƒæ ¼ã€‚")

#%%
# --- å•å…ƒæ ¼ 7: æ¨¡å‹åŠ è½½ä¸PEFT (LoRA) é…ç½® ğŸ§± ---
# ä¸»å¾®è°ƒæ¨¡å‹ä½¿ç”¨ MODEL_NAME
print(f"å‡†å¤‡ä» '{MODEL_NAME}' åŠ è½½ç”¨äºå¾®è°ƒçš„Causal LM...")

bnb_config_finetune = None # ä¸ç”Ÿæˆå™¨LLMçš„bnb_configåŒºåˆ†å¼€
if USE_QUANTIZATION: # ä½¿ç”¨ä¸»é…ç½®ä¸­çš„é‡åŒ–è®¾ç½®
    if QUANTIZATION_TYPE == "nf4" or QUANTIZATION_TYPE == "fp4":
        bnb_config_finetune = BitsAndBytesConfig(
            load_in_4bit=True, bnb_4bit_quant_type=QUANTIZATION_TYPE, 
            bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_use_double_quant=True, 
        )
        print(f"å¾®è°ƒæ¨¡å‹å°†ä½¿ç”¨4-bité‡åŒ– ({QUANTIZATION_TYPE}) åŠ è½½ã€‚")
    elif QUANTIZATION_TYPE == "int8":
        bnb_config_finetune = BitsAndBytesConfig(load_in_8bit=True)
        print("å¾®è°ƒæ¨¡å‹å°†ä½¿ç”¨8-bité‡åŒ–åŠ è½½ã€‚")

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    quantization_config=bnb_config_finetune,
    trust_remote_code=True,
    device_map="auto" 
)
print(f"ç”¨äºå¾®è°ƒçš„æ¨¡å‹ '{MODEL_NAME}' åŠ è½½å®Œæˆã€‚")

if tokenizer.pad_token_id is not None and model.config.pad_token_id is None:
    model.config.pad_token_id = tokenizer.pad_token_id
    print(f"å¾®è°ƒæ¨¡å‹é…ç½®çš„pad_token_idå·²è®¾ç½®ä¸ºtokenizerçš„pad_token_id: {tokenizer.pad_token_id}")

if hasattr(model, 'config') and model.config.model_type and "qwen2" in model.config.model_type.lower() and hasattr(model, 'enable_input_require_grads'):
    try:
        model.enable_input_require_grads()
        print("å·²ä¸ºQwen2å¾®è°ƒæ¨¡å‹è°ƒç”¨ enable_input_require_grads()")
    except Exception as e_grad:
        print(f"ä¸ºQwen2å¾®è°ƒæ¨¡å‹è°ƒç”¨ enable_input_require_grads() æ—¶å‘ç”Ÿé”™è¯¯ (å¯èƒ½ä¸éœ€è¦æˆ–ä¸é€‚ç”¨): {e_grad}")

if USE_LORA:
    print("\nä¸ºå¾®è°ƒæ¨¡å‹å¯ç”¨LoRAã€‚")
    # training_args ç°åœ¨åº”è¯¥åœ¨Cell 9ä¸­å®šä¹‰ï¼Œè¿™é‡Œæˆ‘ä»¬å‡è®¾å®ƒä¼šè¢«å®šä¹‰
    # ä¸ºäº†æ›´å®‰å…¨ï¼Œå¯ä»¥åœ¨ prepare_model_for_kbit_training è°ƒç”¨æ—¶ç›´æ¥ä¼ é€’å¸ƒå°”å€¼
    use_grad_ckpt_for_lora = True # é»˜è®¤å¯ç”¨ï¼Œé™¤éåœ¨TrainingArgumentsä¸­æ˜¾å¼å…³é—­
    if 'training_args' in locals() and hasattr(training_args, 'gradient_checkpointing'):
        use_grad_ckpt_for_lora = training_args.gradient_checkpointing

    if hasattr(model, "is_loaded_in_8bit") or hasattr(model, "is_loaded_in_4bit") or (USE_QUANTIZATION and bnb_config_finetune is not None):
        print("æ£€æµ‹åˆ°å¾®è°ƒæ¨¡å‹å·²é‡åŒ–åŠ è½½ï¼Œå‡†å¤‡k-bitè®­ç»ƒ...")
        model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=use_grad_ckpt_for_lora) 
        print(f"å¾®è°ƒæ¨¡å‹å·²ä¸ºk-bitè®­ç»ƒå‡†å¤‡å°±ç»ªã€‚æ¢¯åº¦æ£€æŸ¥ç‚¹å°†{'å¯ç”¨' if use_grad_ckpt_for_lora else 'ç¦ç”¨'}ã€‚")
    
    lora_config = LoraConfig(
        r=LORA_R, lora_alpha=LORA_ALPHA, target_modules=LORA_TARGET_MODULES, 
        lora_dropout=LORA_DROPOUT, bias="none", task_type=TaskType.CAUSAL_LM 
    )
    print("LoRAé…ç½®å·²åˆ›å»º:")
    print(lora_config)
    
    model = get_peft_model(model, lora_config) 
    print("\nLoRAé€‚é…å™¨å·²åº”ç”¨åˆ°å¾®è°ƒæ¨¡å‹ã€‚")
    model.print_trainable_parameters() 
else:
    print("\næœªå¯ç”¨LoRAï¼Œå°†è¿›è¡Œå…¨å‚æ•°å¾®è°ƒã€‚")

print(f"å½“å‰å¾®è°ƒæ¨¡å‹æ‰€åœ¨è®¾å¤‡: {model.device}")
if hasattr(model, 'hf_device_map'):
    print(f"å¾®è°ƒæ¨¡å‹å±‚è®¾å¤‡åˆ†å¸ƒ: {model.hf_device_map}")
#%%
# --- å•å…ƒæ ¼ 8: è¯„ä¼°æŒ‡æ ‡è®¡ç®—å‡½æ•° ğŸ“Š ---
# (ä¿æŒä¸å˜ï¼Œä½†ç¡®ä¿ parse_quadruples åœ¨æ­¤å®šä¹‰æˆ–ä¹‹å‰å·²å®šä¹‰)
def parse_quadruples(text_str): # ç¡®ä¿è¿™æ˜¯å®Œæ•´çš„å®šä¹‰
    quadruples = [] 
    if not isinstance(text_str, str) or not text_str.strip(): 
        return [] 
        
    parts = text_str.split(SEP_TOKEN) 
    for part_idx, part in enumerate(parts):
        part_cleaned = part.strip() 
        
        if part_cleaned.endswith(END_TOKEN):
            part_cleaned = part_cleaned[:-len(END_TOKEN)].strip() 
        elif not part_cleaned and part_idx == len(parts) -1 : 
             continue

        if not part_cleaned: 
            continue
            
        elements = [e.strip() for e in part_cleaned.split(" | ")] # æ³¨æ„åˆ†éš”ç¬¦ä¸­çš„ç©ºæ ¼
        
        if len(elements) == 4:
            quadruples.append(elements)
    return quadruples

def calculate_f1_metrics(preds_quads_list, labels_quads_list):
    true_positives_hard = 0
    predicted_positives_hard = 0 
    actual_positives_hard = 0    
    true_positives_soft = 0
    predicted_positives_soft = 0
    actual_positives_soft = 0

    for pred_quads_for_sample, gold_quads_for_sample in zip(preds_quads_list, labels_quads_list):
        predicted_positives_hard += len(pred_quads_for_sample)
        actual_positives_hard += len(gold_quads_for_sample)
        predicted_positives_soft += len(pred_quads_for_sample)
        actual_positives_soft += len(gold_quads_for_sample)

        matched_gold_indices_hard = set()
        for p_quad in pred_quads_for_sample:
            for i, g_quad in enumerate(gold_quads_for_sample):
                if i in matched_gold_indices_hard: continue
                if p_quad == g_quad: 
                    true_positives_hard += 1
                    matched_gold_indices_hard.add(i)
                    break 
        
        matched_gold_indices_soft = set()
        for p_quad in pred_quads_for_sample:
            if len(p_quad) != 4: continue
            for i, g_quad in enumerate(gold_quads_for_sample):
                if len(g_quad) != 4: continue 
                if i in matched_gold_indices_soft: continue
                if p_quad[2].strip().lower() == g_quad[2].strip().lower() and \
                   p_quad[3].strip().lower().startswith(g_quad[3].strip().lower().split(" ")[0]): # æ¯”è¾ƒä¸»è¦éƒ¨åˆ†
                    sim_target = difflib.SequenceMatcher(None, p_quad[0], g_quad[0]).ratio()
                    sim_argument = difflib.SequenceMatcher(None, p_quad[1], g_quad[1]).ratio()
                    if sim_target > 0.5 and sim_argument > 0.5: 
                        true_positives_soft += 1
                        matched_gold_indices_soft.add(i)
                        break 

    precision_hard = true_positives_hard / predicted_positives_hard if predicted_positives_hard > 0 else 0
    recall_hard = true_positives_hard / actual_positives_hard if actual_positives_hard > 0 else 0
    f1_hard = 2 * (precision_hard * recall_hard) / (precision_hard + recall_hard) if (precision_hard + recall_hard) > 0 else 0
    precision_soft = true_positives_soft / predicted_positives_soft if predicted_positives_soft > 0 else 0
    recall_soft = true_positives_soft / actual_positives_soft if actual_positives_soft > 0 else 0
    f1_soft = 2 * (precision_soft * recall_soft) / (precision_soft + recall_soft) if (precision_soft + recall_soft) > 0 else 0
    avg_f1 = (f1_hard + f1_soft) / 2
    return {
        "f1_hard": f1_hard, "precision_hard": precision_hard, "recall_hard": recall_hard,
        "f1_soft": f1_soft, "precision_soft": precision_soft, "recall_soft": recall_soft,
        "avg_f1": avg_f1
    }

def compute_metrics_causal(eval_preds):
    generated_token_ids, label_ids_from_input = eval_preds 
    # label_ids_from_input åŒ…å«äº† -100 ç”¨äºå±è”½æç¤ºéƒ¨åˆ†
    # generated_token_ids æ˜¯æ¨¡å‹ç”Ÿæˆçš„åºåˆ—ï¼Œå¯èƒ½ä¹ŸåŒ…å«æç¤ºéƒ¨åˆ†ï¼ˆå¦‚æœgenerateæœªæ­£ç¡®é…ç½®åªè¾“å‡ºæ–°tokenï¼‰
    # é€šå¸¸ï¼ŒTrainerçš„generateä¼šå¤„ç†å¥½ï¼Œåªè¿”å›æ–°ç”Ÿæˆçš„tokensï¼Œæˆ–è€…æˆ‘ä»¬éœ€è¦ä»å®Œæ•´åºåˆ—ä¸­æå–

    # å‡è®¾ generated_token_ids æ˜¯æ¨¡å‹æ–°ç”Ÿæˆçš„token (ä¸å«æç¤º)
    # å¦‚æœå®ƒåŒ…å«äº†æç¤ºï¼Œæˆ‘ä»¬éœ€è¦ä»æ¨¡å‹è¾“å‡ºä¸­ç§»é™¤æç¤ºéƒ¨åˆ†
    # decoded_preds_str = tokenizer.batch_decode(generated_token_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)
    
    # ä¸€ä¸ªæ›´é²æ£’çš„æ–¹æ³•æ˜¯ï¼Œæˆ‘ä»¬çŸ¥é“label_ids_from_inputæ˜¯å®Œæ•´çš„ï¼ŒåŒ…å«-100
    # è€Œ generated_token_ids æ˜¯æ¨¡å‹é’ˆå¯¹è¿™äº›è¾“å…¥ç”Ÿæˆçš„å®Œæ•´åºåˆ—ï¼ˆæç¤º+ç­”æ¡ˆï¼‰
    # æˆ‘ä»¬éœ€è¦ä» generated_token_ids ä¸­æå–ç­”æ¡ˆéƒ¨åˆ†ï¼Œæˆ–è€…ä» decoded_preds_str ä¸­æå–

    # æ–¹æ¡ˆ1: å‡è®¾ generated_token_ids æ˜¯å®Œæ•´çš„ï¼ˆæç¤º+ç­”æ¡ˆï¼‰
    decoded_preds_full_str = tokenizer.batch_decode(generated_token_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)
    
    # ä»è§£ç çš„å®Œæ•´é¢„æµ‹ä¸­æå–çœŸå®çš„ç­”æ¡ˆéƒ¨åˆ†
    pred_answer_strs = []
    keyword_separator = "æ¨¡å‹è¾“å‡ºï¼š" # ä¸PROMPT_TEMPLATEä¸€è‡´
    for full_pred_text in decoded_preds_full_str:
        if keyword_separator in full_pred_text:
            pred_answer_strs.append(full_pred_text.split(keyword_separator, 1)[-1].strip())
        else: # å¦‚æœæ¨¡å‹æ²¡æŒ‰å¥—è·¯å‡ºç‰Œ
            # print(f"è­¦å‘Š: é¢„æµ‹ç»“æœä¸­æœªæ‰¾åˆ°åˆ†éš”ç¬¦ '{keyword_separator}'. Full pred: '{full_pred_text[:100]}...'")
            # å°è¯•ç§»é™¤å·²çŸ¥çš„æç¤ºå¤´ï¼ˆè¿™æ¯”è¾ƒè„†å¼±ï¼‰
            # prompt_head_approx = PROMPT_TEMPLATE.split("{input_text}")[0].split("<<SYS>>")[-1].strip() # å–ç³»ç»Ÿæç¤ºä¹‹åçš„éƒ¨åˆ†
            # if full_pred_text.startswith(prompt_head_approx):
            #     pred_answer_strs.append(full_pred_text[len(prompt_head_approx):].strip())
            # else:
            pred_answer_strs.append(full_pred_text) # åå¤‡ï¼šä½¿ç”¨å…¨éƒ¨ï¼Œå¯èƒ½åŒ…å«æç¤º

    # è§£ç çœŸå®æ ‡ç­¾ï¼ˆç­”æ¡ˆéƒ¨åˆ†ï¼‰
    processed_label_ids = np.where(label_ids_from_input != -100, label_ids_from_input, tokenizer.pad_token_id)
    decoded_labels_full_str = tokenizer.batch_decode(processed_label_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)
    actual_target_strs = []
    for full_label_text in decoded_labels_full_str:
        if keyword_separator in full_label_text:
            actual_target_strs.append(full_label_text.split(keyword_separator, 1)[-1].strip())
        else:
            # print(f"è­¦å‘Š: è§£ç çš„æ ‡ç­¾ä¸­æœªæ‰¾åˆ°åˆ†éš”ç¬¦ '{keyword_separator}'. Full label: '{full_label_text[:100]}...'")
            actual_target_strs.append("") 

    pred_quads_list = [parse_quadruples(p_str) for p_str in pred_answer_strs]
    label_quads_list = [parse_quadruples(l_str) for l_str in actual_target_strs] 
    
    results = calculate_f1_metrics(pred_quads_list, label_quads_list)
    return results

print("è¯„ä¼°æŒ‡æ ‡ç›¸å…³å‡½æ•° (parse_quadruples, calculate_f1_metrics, compute_metrics_causal) å·²å®šä¹‰ã€‚")
# æ›´æ–° parse_quadruples_fn_to_use (å¦‚æœå•å…ƒæ ¼3.6åœ¨å•å…ƒæ ¼8ä¹‹å‰è¿è¡Œäº†)
if 'parse_quadruples_fn_to_use' in globals() and parse_quadruples_fn_to_use.__name__ == 'parse_quadruples_placeholder':
    parse_quadruples_fn_to_use = parse_quadruples
    print("DEBUG: å·²å°† parse_quadruples_fn_to_use æ›´æ–°ä¸ºå•å…ƒæ ¼8çš„å®Œæ•´å®šä¹‰ã€‚")
#%%
# --- å•å…ƒæ ¼ 9: è®­ç»ƒå‚æ•°é…ç½®ä¸æ•°æ®æ•´ç†å™¨ (æœ€ç»ˆç¡®è®¤ç‰ˆ) ğŸ“‹ ---
# (ä¿æŒä¸æ‚¨ç¡®è®¤å¯ç”¨çš„ç‰ˆæœ¬ä¸€è‡´)
if 'CALCULATED_STEPS_PER_EPOCH' not in locals(): CALCULATED_STEPS_PER_EPOCH = 500 # å®‰å…¨é»˜è®¤å€¼
print(f"DEBUG: æœ€ç»ˆç¡®è®¤çš„ CALCULATED_STEPS_PER_EPOCH: {CALCULATED_STEPS_PER_EPOCH}")

training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    num_train_epochs=NUM_TRAIN_EPOCHS,
    per_device_train_batch_size=TRAIN_BATCH_SIZE,
    per_device_eval_batch_size=EVAL_BATCH_SIZE,
    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
    learning_rate=LEARNING_RATE,
    weight_decay=WEIGHT_DECAY,
    do_eval=True,                 
    eval_strategy="steps",        
    eval_steps=CALCULATED_STEPS_PER_EPOCH, 
    save_strategy="steps",        
    save_steps=CALCULATED_STEPS_PER_EPOCH, 
    save_total_limit=2, 
    logging_dir=f"{OUTPUT_DIR}/logs", 
    logging_strategy="steps", 
    logging_steps=max(1, CALCULATED_STEPS_PER_EPOCH // 10 if CALCULATED_STEPS_PER_EPOCH > 10 else 50),
    load_best_model_at_end=True, 
    metric_for_best_model="avg_f1", 
    greater_is_better=True,      
    fp16=(torch.cuda.is_available() and not USE_QUANTIZATION), # fp16 ä¸ 4-bit/8-bit é‡åŒ–é€šå¸¸ä¸ä¸€èµ·ç”¨
    bf16=(torch.cuda.is_bf16_supported() and not USE_QUANTIZATION), # bf16 åŒä¸Š
    lr_scheduler_type=LR_SCHEDULER_TYPE, 
    warmup_ratio=WARMUP_RATIO,           
    report_to=["tensorboard"], 
    seed=SEED,                 
    optim="paged_adamw_8bit" if USE_QUANTIZATION else "adamw_torch",
    remove_unused_columns=True, # æ¨èè®¾ç½®ä¸ºTrue
    gradient_checkpointing=True, # ä¸ºèŠ‚çœæ˜¾å­˜å¯ç”¨
    gradient_checkpointing_kwargs={"use_reentrant": False}, # æ¨èçš„æ¢¯åº¦æ£€æŸ¥ç‚¹è®¾ç½®
)
print(f"è®­ç»ƒå‚æ•° (TrainingArguments) é…ç½®å®Œæˆã€‚è¯„ä¼°å’Œä¿å­˜ç­–ç•¥å‡è®¾ç½®ä¸º 'steps'ï¼Œæ¯ {CALCULATED_STEPS_PER_EPOCH} æ­¥æ‰§è¡Œä¸€æ¬¡ã€‚")

if 'training_args' in locals() and training_args is not None:
    data_collator = DataCollatorForSeq2Seq(
        tokenizer, model=model, label_pad_token_id=-100, 
        pad_to_multiple_of=8 if (training_args.fp16 or training_args.bf16) else None 
    )
    print("æ•°æ®æ•´ç†å™¨ (DataCollatorForSeq2Seq) åˆå§‹åŒ–å®Œæˆã€‚")
#%%
# --- å•å…ƒæ ¼ 10: åˆå§‹åŒ– Trainer ğŸ‘¨â€ğŸ« ---
# (ä¿æŒä¸å˜)
trainer = Trainer(
    model=model, args=training_args,                  
    train_dataset=tokenized_datasets["train"] if 'tokenized_datasets' in locals() and tokenized_datasets and "train" in tokenized_datasets else None, 
    eval_dataset=tokenized_datasets["validation"] if 'tokenized_datasets' in locals() and tokenized_datasets and "validation" in tokenized_datasets else None, 
    tokenizer=tokenizer, data_collator=data_collator, compute_metrics=compute_metrics_causal, 
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.001)] 
)
print("Trainer åˆå§‹åŒ–å®Œæˆã€‚")
if not ('tokenized_datasets' in locals() and tokenized_datasets and "train" in tokenized_datasets and tokenized_datasets["train"]):
    print("è­¦å‘Š: Trainerçš„è®­ç»ƒé›†æœªæ­£ç¡®è®¾ç½®ã€‚")
#%%
# --- å•å…ƒæ ¼ 11: å¼€å§‹æ¨¡å‹è®­ç»ƒ ğŸš€ ---
# (ä¿æŒä¸å˜)
print("å³å°†å¼€å§‹æ¨¡å‹è®­ç»ƒ...")
if trainer.train_dataset is None:
    print("é”™è¯¯: è®­ç»ƒæ•°æ®é›†æœªè®¾ç½®ï¼Œæ— æ³•å¼€å§‹è®­ç»ƒã€‚")
else:
    try:
        if model.generation_config is None: 
            model.generation_config = GenerationConfig.from_model_config(model.config)
            print("å·²ä¸ºæ¨¡å‹è®¾ç½®é»˜è®¤çš„GenerationConfigã€‚")
        
        model.generation_config.max_new_tokens = MAX_TARGET_LENGTH 
        model.generation_config.num_beams = 3 
        model.generation_config.early_stopping = True
        if tokenizer.pad_token_id is not None:
            model.generation_config.pad_token_id = tokenizer.pad_token_id
        if tokenizer.eos_token_id is not None: # ç¡®ä¿eos_token_idä¹Ÿè®¾ç½®
             model.generation_config.eos_token_id = tokenizer.eos_token_id

        print(f"æ¨¡å‹è¯„ä¼°æ—¶å°†ä½¿ç”¨ä»¥ä¸‹ç”Ÿæˆé…ç½®: num_beams={model.generation_config.num_beams}, max_new_tokens={model.generation_config.max_new_tokens}")
        train_result = trainer.train()
        print("\næ¨¡å‹è®­ç»ƒå®Œæˆ!")
        print("æ­£åœ¨ä¿å­˜æ¨¡å‹ (LoRA adapter)...")
        trainer.save_model(OUTPUT_DIR) 
        tokenizer.save_pretrained(OUTPUT_DIR)
        print(f"æ¨¡å‹é€‚é…å™¨å’Œtokenizerå·²ä¿å­˜åˆ° '{OUTPUT_DIR}'ã€‚")
        metrics = train_result.metrics
        trainer.log_metrics("train", metrics) 
        trainer.save_metrics("train", metrics) 
        trainer.save_state() 
        print("\nè®­ç»ƒæŒ‡æ ‡å·²è®°å½•å’Œä¿å­˜ã€‚")
        print(f"è®­ç»ƒç»Ÿè®¡æŒ‡æ ‡: {metrics}")
    except Exception as e:
        print(f"\næ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
        import traceback
        traceback.print_exc() 
#%%
# --- å•å…ƒæ ¼ 12: é¢„æµ‹/æ¨ç†å‡½æ•°è®¾ç½® ğŸ”® ---
# (ä¿æŒä¸å˜ï¼Œä½†ç¡®ä¿ MAX_INPUT_LENGTH å’Œ MAX_TARGET_LENGTH ä»Cell 2æ­£ç¡®ä¼ é€’)
model_to_predict = trainer.model if 'trainer' in locals() and hasattr(trainer, 'model') else None
if model_to_predict:
    model_to_predict.eval() 
    print(f"ç”¨äºé¢„æµ‹çš„æ¨¡å‹å·²å‡†å¤‡å¥½ï¼Œå½“å‰è®¾å¤‡: {model_to_predict.device}")
else:
    print("è­¦å‘Š: 'trainer.model' æœªæ‰¾åˆ°ï¼Œæ— æ³•è®¾ç½® model_to_predictã€‚ç¤ºä¾‹é¢„æµ‹å’Œæäº¤æ–‡ä»¶ç”Ÿæˆå¯èƒ½å¤±è´¥ã€‚")


def predict_quadruples_causal(text_list, model, tokenizer_pred, max_input_len_pred, max_target_gen_len_pred):
    parsed_results_list = []
    if model is None or tokenizer_pred is None:
        print("é”™è¯¯: é¢„æµ‹æ‰€éœ€çš„æ¨¡å‹æˆ–tokenizeræœªæä¾›ã€‚")
        return [{"original_text": t, "extracted_answer_string": "ERROR: Model/Tokenizer missing", "parsed_quadruples": []} for t in text_list]

    for text_input in text_list:
        prompt_for_inference = PROMPT_TEMPLATE.format(input_text=text_input)
        
        # ç¡®ä¿ max_length å¯¹äºæç¤ºæ˜¯åˆç†çš„
        max_prompt_len = max_input_len_pred - max_target_gen_len_pred
        if max_prompt_len <= 0 : max_prompt_len = max_input_len_pred // 2 # è‡³å°‘ç»™æç¤ºä¸€åŠç©ºé—´

        inputs = tokenizer_pred(
            prompt_for_inference, return_tensors="pt", truncation=True, 
            max_length=max_prompt_len, padding=False 
        ).to(model.device) 

        with torch.no_grad():
            # å¤åˆ¶å¹¶æ›´æ–°ç”Ÿæˆé…ç½®ï¼Œä»¥é˜²ä¿®æ”¹å…¨å±€é…ç½®
            current_gen_config = GenerationConfig(**model.generation_config.to_dict())
            current_gen_config.max_new_tokens = max_target_gen_len_pred
            # num_beams, early_stopping ç­‰åº”å·²åœ¨ model.generation_config ä¸­è®¾ç½®

            outputs = model.generate(**inputs, generation_config=current_gen_config)
        
        full_generated_text = tokenizer_pred.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)
        
        answer_part_str = ""
        keyword_separator = "æ¨¡å‹è¾“å‡ºï¼š" 
        split_parts = full_generated_text.split(keyword_separator, 1)
        if len(split_parts) > 1:
            answer_part_str = split_parts[1].strip()
        else: 
            prompt_head_for_removal = PROMPT_TEMPLATE.format(input_text=text_input).split(keyword_separator)[0] + keyword_separator
            decoded_prompt_head = tokenizer_pred.decode(tokenizer_pred.encode(prompt_head_for_removal.split("ç”¨æˆ·æä¾›çš„æ–‡æœ¬å¦‚ä¸‹ï¼š")[0], add_special_tokens=False), skip_special_tokens=True) # å°è¯•è§£ç æç¤ºå¤´éƒ¨åˆ†
            
            # è¿™æ˜¯ä¸€ä¸ªæ›´å¤æ‚çš„å°è¯•ï¼Œè¯•å›¾ç§»é™¤æç¤ºéƒ¨åˆ†
            # å¦‚æœè§£ç åçš„å®Œæ•´æ–‡æœ¬ä»¥è§£ç åçš„æç¤ºå¤´å¼€å§‹ï¼Œåˆ™ç§»é™¤å®ƒ
            # è¿™éå¸¸ä¾èµ–äºè§£ç çš„ä¸€è‡´æ€§
            # if full_generated_text.startswith(decoded_prompt_head.strip()):
            #    answer_part_str = full_generated_text[len(decoded_prompt_head.strip()):].strip()
            #    if answer_part_str.startswith(keyword_separator): # å†æ¬¡æ£€æŸ¥
            #         answer_part_str = answer_part_str[len(keyword_separator):].strip()
            # else:
            # print(f"è­¦å‘Šï¼šæ— æ³•åœ¨ç”Ÿæˆæ–‡æœ¬ä¸­é€šè¿‡å…³é”®è¯ '{keyword_separator}' å®šä½ç­”æ¡ˆï¼Œä¸”æç¤ºå¤´ä¸åŒ¹é…ã€‚Full: '{full_generated_text[:200]}...'")
            answer_part_str = full_generated_text # ä½œä¸ºæœ€åçš„æ‰‹æ®µï¼Œå¯èƒ½åŒ…å«æç¤º

        parsed_quads = parse_quadruples(answer_part_str) # ä½¿ç”¨Cell 8å®šä¹‰çš„parse_quadruples
        parsed_results_list.append({
            "original_text": text_input, "full_generated_text": full_generated_text, 
            "extracted_answer_string": answer_part_str, "parsed_quadruples": parsed_quads
        })
    return parsed_results_list

print("é¢„æµ‹/æ¨ç†ç›¸å…³å‡½æ•° (predict_quadruples_causal) å·²å®šä¹‰ã€‚")
#%%
# (ä¿æŒä¸å˜)
sample_test_texts_for_prediction = [
    "é‚£äº›åŒæ€§æ‹çœŸæ¶å¿ƒï¼Œè´¥åç¤¾ä¼šé£æ°”ã€‚", "è¿™é“èœå‘³é“ä¸é”™ï¼Œä¸‹æ¬¡è¿˜æ¥ã€‚",
    "ä¸Šæµ·äººå°±æ˜¯æ’å¤–ï¼Œçœ‹ä¸èµ·å¤–åœ°äººã€‚", "é»‘äººéƒ½æ˜¯ç½ªçŠ¯ï¼Œåº”è¯¥è¢«èµ¶èµ°ã€‚",
    "ä½ å¯çœŸæ˜¯å¤´è ¢é©´ï¼Œè¿™éƒ½åšä¸å¥½ã€‚"
]
print("\nå¼€å§‹è¿è¡Œç¤ºä¾‹é¢„æµ‹...")
if 'model_to_predict' in locals() and model_to_predict is not None:
    predictions = predict_quadruples_causal(
        sample_test_texts_for_prediction, model_to_predict, tokenizer,
        MAX_INPUT_LENGTH, MAX_TARGET_LENGTH 
    )
    print("\nç¤ºä¾‹é¢„æµ‹ç»“æœ:")
    for item in predictions:
        print(f"åŸå§‹æ–‡æœ¬: {item['original_text']}")
        print(f"æå–ç­”æ¡ˆ: {item['extracted_answer_string']}")
        print(f"è§£æå››å…ƒç»„: {item['parsed_quadruples']}")
        print("-" * 30)
else:
    print("é”™è¯¯: 'model_to_predict' æœªå®šä¹‰æˆ–ä¸ºNoneã€‚æ— æ³•è¿è¡Œç¤ºä¾‹é¢„æµ‹ã€‚")

#%%
# --- å•å…ƒæ ¼ 14: åŠ è½½å®˜æ–¹æµ‹è¯•æ•°æ®å¹¶ç”Ÿæˆæäº¤æ–‡ä»¶ ğŸ“¤ ---
# (ä¿æŒä¸å˜ï¼Œä½†ç¡®ä¿è·¯å¾„å’Œå˜é‡åæ­£ç¡®)
# official_test_file_path = "/kaggle/input/nlptrain/test1.json" # æ‚¨çš„æµ‹è¯•æ–‡ä»¶è·¯å¾„
# official_test_file_path = "./test1.json" # å‡è®¾åœ¨å½“å‰ç›®å½•
# official_test_file_path = "./test2.json" # æˆ–è€… test2.json

# ç¡®ä¿ä»¥ä¸‹å˜é‡å·²å®šä¹‰ï¼š
# official_test_file_path, model_to_predict, tokenizer, EVAL_BATCH_SIZE, MAX_INPUT_LENGTH, MAX_TARGET_LENGTH

# ç¤ºä¾‹è·¯å¾„ï¼Œè¯·æ ¹æ®æ‚¨çš„å®é™…æƒ…å†µä¿®æ”¹
# official_test_file_path_to_use = "./test1.json" 
import json # ç¡®ä¿å¯¼å…¥jsonåº“
import os   # ç¡®ä¿å¯¼å…¥osåº“

# --- å¦‚ä½•åŠ è½½å®˜æ–¹æµ‹è¯•æ•°æ®å¹¶ç”Ÿæˆæäº¤æ–‡ä»¶çš„ç¤ºä¾‹ ---

def load_official_test_data(file_path):
    """
    åŠ è½½å®˜æ–¹æµ‹è¯•æ•°æ®ã€‚
    å‡è®¾æ–‡ä»¶æ˜¯ä¸€ä¸ªJSONï¼Œå…¶é¡¶çº§ç»“æ„æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œåˆ—è¡¨ä¸­çš„æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªåŒ…å« "id" å’Œ "content" é”®çš„å­—å…¸ã€‚
    
    å‚æ•°:
    - file_path (str): æµ‹è¯•æ•°æ®JSONæ–‡ä»¶çš„è·¯å¾„ã€‚
    
    è¿”å›:
    - list: åŒ…å«æ‰€æœ‰ "content" å­—ç¬¦ä¸²çš„åˆ—è¡¨ã€‚
    - list: åŒ…å«æ‰€æœ‰å¯¹åº” "id" çš„åˆ—è¡¨ (å¯é€‰, å¦‚æœéœ€è¦idè¿›è¡Œæ˜ å°„æˆ–è°ƒè¯•)ã€‚
    """
    texts_to_predict = []
    ids_from_test_data = [] # å¯é€‰ï¼Œç”¨äºè¿½è¸ªID

    if not os.path.exists(file_path):
        print(f"é”™è¯¯: æµ‹è¯•æ–‡ä»¶ '{file_path}' æœªæ‰¾åˆ°ã€‚")
        return texts_to_predict, ids_from_test_data # è¿”å›ç©ºåˆ—è¡¨

    print(f"æ­£åœ¨ä» '{file_path}' åŠ è½½å®˜æ–¹æµ‹è¯•æ•°æ®...")
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f) # æ•´ä¸ªæ–‡ä»¶æ˜¯ä¸€ä¸ªJSONåˆ—è¡¨
            if not isinstance(data, list):
                print(f"é”™è¯¯: æµ‹è¯•æ–‡ä»¶ '{file_path}' çš„é¡¶çº§ç»“æ„ä¸æ˜¯ä¸€ä¸ªåˆ—è¡¨ã€‚è¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼ã€‚")
                return texts_to_predict, ids_from_test_data

            for item_num, item in enumerate(data, 1):
                if isinstance(item, dict) and "content" in item and "id" in item:
                    texts_to_predict.append(item["content"])
                    ids_from_test_data.append(item["id"])
                else:
                    print(f"è­¦å‘Š: æµ‹è¯•æ–‡ä»¶ '{file_path}' ä¸­çš„ç¬¬ {item_num} é¡¹æ ¼å¼ä¸æ­£ç¡®æˆ–ç¼ºå°‘ 'id'/'content' é”®ï¼Œå·²è·³è¿‡: {item}")
        
        print(f"æˆåŠŸä» '{file_path}' åŠ è½½äº† {len(texts_to_predict)} æ¡æµ‹è¯•æ•°æ®ã€‚")

    except json.JSONDecodeError:
        print(f"é”™è¯¯: è§£ææµ‹è¯•æ–‡ä»¶ '{file_path}' æ—¶å‘ç”ŸJSONè§£ç é”™è¯¯ã€‚è¯·æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸ºæœ‰æ•ˆçš„JSONæ ¼å¼ã€‚")
    except Exception as e:
        print(f"åŠ è½½æµ‹è¯•æ–‡ä»¶ '{file_path}' æ—¶å‘ç”Ÿå…¶ä»–é”™è¯¯: {e}")
        
    return texts_to_predict, ids_from_test_data

official_test_file_path_to_use = "./test1.json" # æˆ–æ‚¨çš„ test2.json è·¯å¾„

if 'model_to_predict' not in locals() or model_to_predict is None:
    print("é”™è¯¯: 'model_to_predict' æœªå®šä¹‰ã€‚æ— æ³•è¿›è¡Œå®˜æ–¹æµ‹è¯•æ•°æ®é¢„æµ‹ã€‚")
elif 'tokenizer' not in locals() or tokenizer is None:
    print("é”™è¯¯: 'tokenizer' æœªå®šä¹‰ã€‚æ— æ³•è¿›è¡Œå®˜æ–¹æµ‹è¯•æ•°æ®é¢„æµ‹ã€‚")
elif not os.path.exists(official_test_file_path_to_use):
    print(f"é”™è¯¯: æµ‹è¯•æ–‡ä»¶è·¯å¾„ '{official_test_file_path_to_use}' ä¸å­˜åœ¨ã€‚")
else:
    print(f"\nå¼€å§‹å¤„ç†å®˜æ–¹æµ‹è¯•æ–‡ä»¶: {official_test_file_path_to_use}")
    official_test_texts, official_test_ids = load_official_test_data(official_test_file_path_to_use) # load_official_test_data åœ¨æ‚¨ä¹‹å‰çš„ä»£ç ä¸­å®šä¹‰
    
    if official_test_texts:
        submission_outputs_strings = []
        inference_batch_size = EVAL_BATCH_SIZE 
        print(f"å¼€å§‹å¯¹ {len(official_test_texts)} æ¡æµ‹è¯•æ•°æ®è¿›è¡Œé¢„æµ‹ (æ‰¹æ¬¡å¤§å°: {inference_batch_size})...")
        for i in tqdm(range(0, len(official_test_texts), inference_batch_size), desc="å®˜æ–¹æµ‹è¯•é›†é¢„æµ‹"):
            batch_texts = official_test_texts[i : i + inference_batch_size]
            batch_predictions = predict_quadruples_causal(
                batch_texts, model_to_predict, tokenizer,
                MAX_INPUT_LENGTH, MAX_TARGET_LENGTH
            )
            for item_prediction in batch_predictions:
                submission_outputs_strings.append(item_prediction['extracted_answer_string'])
        
        #submission_file_path = "/kaggle/working/submission.txt" # Kaggle å·¥ä½œç›®å½•
        submission_file_path = "./newsubmission.txt" # æˆ–è€…æœ¬åœ°è·¯å¾„
        try:
            with open(submission_file_path, "w", encoding="utf-8") as f:
                for line_content in submission_outputs_strings:
                    f.write(line_content + "\n")
            print(f"\næäº¤æ–‡ä»¶å·²æˆåŠŸç”Ÿæˆ: {submission_file_path}")
            print(f"è¯¥æ–‡ä»¶åŒ…å« {len(submission_outputs_strings)} è¡Œé¢„æµ‹ã€‚")
        except Exception as e:
            print(f"å†™å…¥æäº¤æ–‡ä»¶ '{submission_file_path}' æ—¶å‘ç”Ÿé”™è¯¯: {e}")
    else:
        print(f"æœªèƒ½ä» '{official_test_file_path_to_use}' åŠ è½½ä»»ä½•æµ‹è¯•æ•°æ®è¿›è¡Œé¢„æµ‹ã€‚")
#%%
